./SPELLCHECK.md
./Diffusion/Cross-frame attention for consistent video generation.md
./Diffusion/Diffusion steps with low-resolution images.md
./Diffusion/Problem with using DiT for autonomous driving simulation and scalable alternative.md
./Cuda and GPU/Under utilization of tensor cores.md
./Cuda and GPU/The strided access trap.md
./Cuda and GPU/Warp divergence in GPU threads.md
./Cuda and GPU/Should we write cuda kernels?.md
./Cuda and GPU/Atomic contention using serialization while computing histogram of image.md
./Cuda and GPU/Optimizing FlashAttention kernel for Blackwell.md
./Cuda and GPU/Optimizing custom CUDA kernel for matrix multiplication.md
./Reinforcement Learning(RL)/Choosing between PPO and DPO.md
./Neural Network/Why transformers win over RNN and LSTM for sequence modelling?.md
./Neural Network/How to fix Internal Covariate Shift in Neural Network.md
./Neural Network/How Adam performs better over SGD.md
./Neural Network/Training run budget of Neural Network.md
./Neural Network/Problem of some hidden layer neurons outputting zero.md
./Neural Network/DL Layer Summary.md
./Neural Network/Implementation of softmax regression.md
./Neural Network/What is problem with linear scaling rule for LR while using SGD.md
./Neural Network/How to use Dropout layer during inference?.md
./Neural Network/Should we reduce dimension size when model is overfitting on training set?.md
./Neural Network/Weight initialization techniques in NN.md
./Data/Risk in fetching features from Redis Cache.md
./README.md
./Traditional ML/Reservoir Sampling when value of sample is large.md
./Traditional ML/Metric for fraud detection model.md
./Traditional ML/What to use in bagging and boosting for production credit risk model.md
./Traditional ML/good feature engineering.md
./Traditional ML/Problems with hinge loss for training binary text classifier in production.md
./Traditional ML/Imbalanced toxicity annotation dataset.md
./Traditional ML/Sigmoid and Logit.md
./Traditional ML/Interpreting Coefficients in Logistic Regression.md
./Traditional ML/Handling Missing Values.md
./Traditional ML/Train a model to predict next month's purchases.md
./Traditional ML/Class Imbalance for fraud detection.md
./Traditional ML/Convex vs Non-Convex Loss Function.md
./Traditional ML/Balancing conflicting optimization.md
./Traditional ML/How L1 and L2 metric change if feature space is rotated by 45 degrees.md
./Traditional ML/Diagnosis of Training and Validation Accuracy.md
./Computer Vision/Tracking across frames of video.md
./Computer Vision/class imbalance in computer vision.md
./Computer Vision/How to use transformation in case of data sparsity.md
./Computer Vision/Why Zero-padding is dangerous for models statistical distribution.md
./Computer Vision/Segmentation/Deciding whether segmentation model is ready for production based on test set accuracy.md
./Computer Vision/Segmentation/Background bias in semantic segmentation.md
./Computer Vision/CNN its types and Training/Why we replace a single convolution with stack of convolutions in VGGNet?.md
./Computer Vision/CNN its types and Training/How CNN is different that Fully-connected network in analyzing images.md
./Computer Vision/CNN its types and Training/Infering softmax Loss values while training on CIFAR-10 dataset.md
./Computer Vision/Classification/How to fix stability and boost ImageNet accuracy while feeding raw class names.md
./Computer Vision/Classification/Contextual Bias in image classification.md
./Computer Vision/Object Detection/K-Means Anchor for detection of custom shapes.md
./Computer Vision/Object Detection/Detection of 5-pixel cracks in 4k image.md
./Computer Vision/Object Detection/Why RPN helps with small objects.md
./Computer Vision/Object Detection/How to decide threshold for NMS?.md
./Computer Vision/Data Augmentation/Data Labelling to train resnet model.md
./Computer Vision/Data Augmentation/Why data augmentation is applied to only for train and not on validation data?.md
./Computer Vision/What to use between Early and Slow fusion for video understanding model.md
./website/index.md
./MultiModal/Running VLM on phone and extracting clauses from it.md
./MultiModal/Why do multimodal models sometimes underperform unimodal ones?.md
./RAG/Why RAG Pipeline hallucinates even with good retrieval.md
./RAG/Why production RAG with 200k context window fails on complex queries.md
./RAG/RAG vs CAG.md
./RAG/Designing retrieval layer with Hybrid Search and RRF for legal database RAG system.md
./RAG/RAG for financial reports with different format of content.md
./RAG/Is distillation best architecture if we want latency of 1 model and intelligence of other?.md
./RAG/What to do when RAG pipeline is slow?.md
./RAG/Why semantic search is not enough and we need rerankers in RAG?.md
./RAG/How to evaluate search retrieval accuracy in RAG system for compliance document retrieval.md
./RAG/How to decide what fails when RAG Hallucinates-retriever or generator?.md
./RAG/Vector DB for RAG.md
./Recommender System(RecSys)/How often should we retrain recommendation model?.md
./Recommender System(RecSys)/Real-time model to recommend new connections in a social graph.md
./Vision Language Models(VLMs)/Training CLIP Model for medical imaging.md
./Vision Language Models(VLMs)/Vision Token Explosion Trap in VLMs.md
./Vision Language Models(VLMs)/Training VLMs for reasoning from vision Encoder and LLM.md
./Vision Language Models(VLMs)/Searchable semantic index for videos.md
./Vision Language Models(VLMs)/Extracting data from invoices.md
./Vision Language Models(VLMs)/What to use in ğ˜¾ğ™¤ğ™£ğ™©ğ™§ğ™–ğ™¨ğ™©ğ™ğ™«ğ™š ğ™¡ğ™šğ™–ğ™§ğ™£ğ™ğ™£ğ™œ ğ™«ğ™¨ ğ™ˆğ™–ğ™¨ğ™ ğ™šğ™™ ğ˜¼ğ™ªğ™©ğ™¤ğ™€ğ™£ğ™˜ğ™¤ğ™™ğ™šğ™§ for FSD using unlabeled video.md
./Vision Language Models(VLMs)/Loss function to train VLM from ViT and Text Encoder.md
./Vision Language Models(VLMs)/Visual Instruction Tuning of VLMs to maximize image-text alignment.md
./Statistics/Hypothesis Testing/How to decide which tests to use when?.md
./System Design/Tradeoff to make in KPIs while building real-time Assistant for customer service.md
./System Design/Why does latency become such a hard problem in GenAI systems?.md
./Agents/Context Engineering and its components.md
./Agents/How to ensure Multi-Agent system and its tool calls are secure?.md
./Agents/MCP/Why MCP is needed?.md
./Agents/How to create HR agent using azure without rewriting everything.md
./Agents/Multi-Agent System/Designing Data Analyst Copilot.md
./Agents/Architecture to build autonomous Agents for enterprise customers.md
./Agents/How Agents are different from chatbot?.md
./Agents/How do you evaluate LLM agents in production?.md
./Agents/Hierarchical routing for deciding in agentic system to decide tool call for user request.md
./Agents/Why and how to use Agentic Context Engineering.md
./Agents/Memory/Memory in Agents.md
./Agents/Memory/Long-term vs. Short-term memory in agentic trading assistant.md
./Agents/Why do tool-using agents often hallucinate function calls even when APIs are available?.md
./Agents/Which framework should use to build autonomous customer support agent.md
./Agents/Termination Problem in Agents.md
./Agents/Patterns in Agentic workflow.md
./Vector Database/Scaling DB to handle more requests.md
./Vector Database/When to use inbuilt Vector DB or build your Vector DB.md
./LLM/Translation/Data generation and training for Japanese-to-English translation model.md
./LLM/Tokenization/Adjusting tokenizer of 1 language to other language.md
./LLM/Tokenization/Tokenizer_for_training_domain_specialized_LLMs.md
./LLM/Tokenization/Why BPE tokenizer over traditional byte-based tokenizer?.md
./LLM/LLM Alignment/Aligning LLM output.md
./LLM/LLM Alignment/Explain how solution is architecturally trustworthy.md
./LLM/Post-training/Adjusting post-training pipeline to boost ELO score.md
./LLM/Post-training/Training a student model through distillation.md
./LLM/Finetuning/SFT vs GRPO for complex legal reasoning model.md
./LLM/Finetuning/LORA/Initializing matrix weights in LORA.md
./LLM/Finetuning/LORA/Effect of increasing rank in lora.md
./LLM/Finetuning/LORA/LORA.md
./LLM/Finetuning/LORA/Some detail on using LORA.md
./LLM/Finetuning/LORA/LORA alternative for finetuning model in technical domain with GPU constraint.md
./LLM/Finetuning/Why didn't reducing precision by 4x from FP16 to 4-bit result in a 4x speedup?.md
./LLM/Finetuning/Finetuning linear head on a pre-trained Transformer.md
./LLM/Finetuning/Finetune model directly on user prompts.md
./LLM/Hallucination/Why do LLM Hallucinates?.md
./LLM/Hallucination/Hallucination in LLMs.md
./LLM/Positional Embedding/ROPE Positional Embedding.md
./LLM/Positional Embedding/How ROPE is different from sin-cos positional embeddings.md
./LLM/Positional Embedding/Advantage of ROPE for inference on longer sequence.md
./LLM/Evals/Benchmarks and Evaluations.md
./LLM/Evals/Analyzing which model is better based on perplexity score.md
./LLM/Evals/How evals are done.md
./LLM/Mixture of Experts(MOE)/Routing in MOE.md
./LLM/Mixture of Experts(MOE)/Validation loss flattened in MOE training.md
./LLM/Mixture of Experts(MOE)/Should we use dense or MOE when we have to scale LLM to 100B+ while optimizing on inference cost.md
./LLM/Embeddings/How do you prove your word embeddings aren't biased?.md
./LLM/Data/Data for LLM-training.md
./LLM/Inference/Chinchilla law actual meaning and how to do inference on scaling in budget.md
./LLM/Inference/How do you optimize LLM inference?.md
./LLM/Inference/Paged Attention/ğ™†ğ™‘ ğ˜¾ğ™–ğ™˜ğ™ğ™š ğ™ğ™§ğ™–ğ™œğ™¢ğ™šğ™£ğ™©ğ™–ğ™©ğ™ğ™¤ğ™£ and its solution.md
./LLM/Inference/Paged Attention/Paged Attention1.md
./LLM/Inference/Query Routing/Problem with routing queries to different LLMs based on difficulty.md
./LLM/Inference/Query Routing/How to use query routing over knowledge distillation.md
./LLM/Inference/How you can build a production-grade multi-tenant LLM platform?.md
./LLM/Inference/Speculative Decoding/How exactly speculative decoding speed up LLM Inference.md
./LLM/Inference/Speculative Decoding/Speculative Decoding1.md
./LLM/Inference/Speculative Decoding/Should we use Speculative Decoding to optimize inference for 128 batch size?.md
./LLM/Inference/Implementing GRPO and reward hacking trap.md
./LLM/Inference/Serving/Continuous batching for serving chatbot.md
./LLM/Inference/Serving/Cuda OOM error in serving Multimodal on vLLM.md
./LLM/Inference/Serving/Sequence packing instead of zero-padding for variable length prompts.md
./LLM/Inference/Serving/Metric for production grade LLM systems.md
./LLM/Inference/Serving/Serving LLM and optimize on TTFT metric.md
./LLM/Inference/Serving/LLM Serving Frameworks.md
./LLM/Inference/How increasing batch size affects costs, latency and throughput?.md
./LLM/Inference/Inference speed of MHA vs. GQA.md
./LLM/Inference/Fusion of Thinking and Non-thinking Model.md
./LLM/Inference/ğ—¦ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—²-ğ—•ğ—®ğ˜€ğ—²ğ—± ğ—£ğ—¿ğ—²-ğ—ªğ—®ğ—¿ğ—ºğ—¶ğ—»ğ—´ & ğ——ğ—²ğ—´ğ—¿ğ—®ğ—±ğ—®ğ˜ğ—¶ğ—¼ğ—» for predictable event spike.md
./LLM/Inference/What actually happens when we run a LLM model locally.md
./LLM/Inference/How to fix GPU running out of memory for long conversations in LLM Inference.md
./LLM/Inference/Dynamic shape recompilation issue in optimized deployed Transformer model.md
./LLM/Prompt Engineering/Why COT improves LLM performance but sometimes fails?.md
./LLM/Prompt Engineering/Distribution shift caused by tokenization artifacts.md
./LLM/Prompt Engineering/Prompt Drift Problem.md
./LLM/Prompt Engineering/Why do we need prompt caching?.md
./LLM/Why we use Position-wise ML layers when we have self-attention.md
./LLM/Training/Predicting transformer training steps using Adam.md
./LLM/Training/Finding optimal LR while scaling 1B model to 70B model.md
./LLM/Training/Grokking in reasoning tasks.md
./LLM/Training/Effect of changing training run from SGD to Adam.md
./LLM/Training/Training_Instability_Exploding_Gradients_Deeper_LLM.md
./LLM/Training/Why to use weight decay while pretraining 500B parameter model.md
./LLM/Training/Fixing gradient oscillation and loss spikes during LLM training.md
./LLM/Training/Change in LR as LLM model size changes.md
./LLM/Training/To maximize model performance on benchmark, do we spend compute on pretraining or gathering more data?.md
./LLM/Training/Data feed order during pre-training of Math Reasoner.md
./LLM/Training/Fixed Compute Budget- Best Model Training.md
./LLM/Training/Backprop gradient in QAT.md
./LLM/Training/Training_stability_in_Shallow_LLMs.md
./LLM/Training/How performance affects after adding more languages in training.md
./LLM/Training/Why SwiGLU outperforms standard ReLU.md
./LLM/Training/Warmup+Decay phase in LLM training.md
./LLM/Training/How to decide retraining.md
./LLM/Training/Debugging transformer training when loss curve is flat.md
./LLM/GPU Utilization/Optimization of cuda kernel.md
./LLM/GPU Utilization/Tradeoff between TTFT and batch summarization in Chatbot.md
./LLM/GPU Utilization/Choice between 4x A100 40GB or 1x H100 80GB to deploy 70B LLM.md
./LLM/Attention/How do we use the Attention mechanism's weights to measure the model's uncertainty?.md
./LLM/Attention/Attention Mechanism choice for serving 70B LLM.md
./LLM/Attention/Linear vs. Sparse Attention.md
./LLM/Attention/Why do we divide by âˆšdâ‚– instead of âˆšd? .md
./LLM/Attention/d_model in positional embeddings vs. d_k in attention.md
./LLM/Other General Questions/Transformer-based regression model for home sales predictions.md
./LLM/Other General Questions/LLM Parameters and Memory Estimation.md
./LLM/Other General Questions/Should we use Greedy Search for factual questions?.md
./LLM/Other General Questions/LLMs can call tools. Why do they still make simple mistakes?.md
./LLM/Other General Questions/Difference between Layer normalization and batch normalization & why transformer uses layer and not batchnorm?.md
./LLM/Other General Questions/Catstophic forgetting/Catstophic forgetting and how to solve it?.md
./LLM/Other General Questions/Catstophic forgetting/Teaching live model new data and catastropic forgetting issue.md
./LLM/Context Window/Context caching for chatting with history.md
./LLM/Context Window/Implementing 1M context window using Ring Attention (Context Parallelism).md
./LLM/Context Window/Does large(1 M) context window means always better?.md
./LLM/Context Window/Lost in the middle problem in LLM.md
./LLM/Context Window/Context_Window_issue.md
./LLM/Quantization/QAT vs. MPT.md
./LLM/Quantization/Risk with standard Post-training Quantization.md
./LLM/Quantization/Deciding which component to convert to bfloat16 and which to FP32?.md
./LLM/Quantization/Which one is to use? 4-bit vs 8-bit for production trading signals.md
./LLM/Quantization/Quantization from FP16 to INT8.md
./LLM/Quantization/FP16 vs BF16, Which one to use?.md
./LLM/Quantization/Why decreasing numerical precision result in almost zero loss in intelligence?.md
./LLM/Quantization/Should we avoid quantization (keeping Float32) to preserve model stability when we have strict latency limits?.md
./LLM/Quantization/temp.md
