ğ™ğ™ğ™š ğ™ğ™šğ™¨ğ™¤ğ™¡ğ™ªğ™©ğ™ğ™¤ğ™£-ğ™ğ™¤ğ™ ğ™šğ™£ ğ˜¿ğ™ğ™¡ğ™šğ™¢ğ™¢ğ™– ğŸ¤³

You're in a Multimodal Systems interview at Apple. The interviewer presents a problem:

"ğ˜ğ˜¦ ğ˜¸ğ˜¢ğ˜¯ğ˜µ ğ˜µğ˜° ğ˜³ğ˜¶ğ˜¯ ğ˜¢ ğ˜ğ˜“ğ˜” ğ˜°ğ˜¯-ğ˜¥ğ˜¦ğ˜·ğ˜ªğ˜¤ğ˜¦ (ğ˜ªğ˜—ğ˜©ğ˜°ğ˜¯ğ˜¦). ğ˜›ğ˜©ğ˜¦ ğ˜¶ğ˜´ğ˜¦ğ˜³ ğ˜µğ˜¢ğ˜¬ğ˜¦ğ˜´ ğ˜¢ ğ˜±ğ˜©ğ˜°ğ˜µğ˜° ğ˜°ğ˜§ ğ˜¢ ğ˜¥ğ˜¦ğ˜¯ğ˜´ğ˜¦ ğ˜­ğ˜¦ğ˜¨ğ˜¢ğ˜­ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜³ğ˜¢ğ˜¤ğ˜µ (4ğ˜’ ğ˜³ğ˜¦ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯). ğ˜ğ˜¦ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ ğ˜µğ˜° ğ˜¦ğ˜¹ğ˜µğ˜³ğ˜¢ğ˜¤ğ˜µ ğ˜´ğ˜±ğ˜¦ğ˜¤ğ˜ªğ˜§ğ˜ªğ˜¤ ğ˜¤ğ˜­ğ˜¢ğ˜¶ğ˜´ğ˜¦ğ˜´. ğ˜šğ˜µğ˜¢ğ˜¯ğ˜¥ğ˜¢ğ˜³ğ˜¥ ğ˜Šğ˜“ğ˜ğ˜— ğ˜¦ğ˜¯ğ˜¤ğ˜°ğ˜¥ğ˜¦ğ˜³ğ˜´ ğ˜³ğ˜¦ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜¦ğ˜·ğ˜¦ğ˜³ğ˜ºğ˜µğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜° 336ğ˜¹336. ğ˜ğ˜©ğ˜¢ğ˜µ ğ˜©ğ˜¢ğ˜±ğ˜±ğ˜¦ğ˜¯ğ˜´ ğ˜ªğ˜§ ğ˜¸ğ˜¦ ğ˜¶ğ˜´ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜´ğ˜µğ˜¢ğ˜¯ğ˜¥ğ˜¢ğ˜³ğ˜¥ ğ˜¢ğ˜³ğ˜¤ğ˜©ğ˜ªğ˜µğ˜¦ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦?"

The candidate shrugs: 
"ğ˜™ğ˜¦ğ˜´ğ˜ªğ˜»ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜° 336ğ˜¹336 ğ˜®ğ˜¢ğ˜¬ğ˜¦ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜¶ğ˜¯ğ˜³ğ˜¦ğ˜¢ğ˜¥ğ˜¢ğ˜£ğ˜­ğ˜¦. ğ˜šğ˜°, ğ˜¸ğ˜¦ ğ˜´ğ˜©ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜´ğ˜­ğ˜ªğ˜¤ğ˜¦ ğ˜µğ˜©ğ˜¦ 4ğ˜’ ğ˜ªğ˜®ğ˜¢ğ˜¨ğ˜¦ ğ˜ªğ˜¯ğ˜µğ˜° ğ˜°ğ˜·ğ˜¦ğ˜³ğ˜­ğ˜¢ğ˜±ğ˜±ğ˜ªğ˜¯ğ˜¨ 336ğ˜¹336 ğ˜±ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜¦ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜¦ğ˜¯ğ˜¤ğ˜°ğ˜¥ğ˜¦ ğ˜¦ğ˜¢ğ˜¤ğ˜© ğ˜°ğ˜¯ğ˜¦."

ğ—ªğ—¿ğ—¼ğ—»ğ—´. You just killed the battery and the inference budget.ğŸ’€

ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜†: 
Slicing a 4K image into patches results in roughly 144 patches. 
If each patch becomes 576 tokens (standard ViT), you are feeding ğŸ´ğŸ®,ğŸ¬ğŸ¬ğŸ¬+ ğ˜ƒğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ ğ˜ğ—¼ğ—¸ğ—²ğ—»ğ˜€ into your LLM. 

The LLM's attention cost is quadratic O(NÂ²). You just turned a simple extraction task into a heavy-duty context processing job that will drain the battery in minutes and take 10 seconds to respond.

You have a ğ—§ğ—¼ğ—¸ğ—²ğ—» ğ—•ğ—¹ğ—¼ğ—®ğ˜ problem. ğŸ’¥

ğ—§ğ—µğ—² ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ——ğ—²ğ—°ğ—¼ğ˜‚ğ—½ğ—¹ğ—²ğ—± ğ—©ğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ ğ—§ğ—¼ğ—¸ğ—²ğ—»ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» (ğ—™ğ—®ğ˜€ğ˜ğ—©ğ—Ÿğ—  / ğ—Ÿğ—Ÿğ—®ğ—©ğ—”-ğ—¨ğ—›ğ——)
You need to separate ğ˜ğ˜®ğ˜¢ğ˜¨ğ˜¦ ğ˜™ğ˜¦ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ from ğ˜›ğ˜°ğ˜¬ğ˜¦ğ˜¯ ğ˜Šğ˜°ğ˜¶ğ˜¯ğ˜µ. 
High resolution is needed for "ğ˜šğ˜¦ğ˜¦ğ˜ªğ˜¯ğ˜¨" (Encoder), but not all those pixels are needed for "ğ˜™ğ˜¦ğ˜¢ğ˜´ğ˜°ğ˜¯ğ˜ªğ˜¯ğ˜¨" (LLM).

You use a query-based summarizer (like a Perceiver Resampler from Google DeepMind or C-Abstractor from KakaoBrain) or a specialized architecture like ğ—™ğ—®ğ˜€ğ˜ğ—©ğ—Ÿğ— .

1. ğ—›ğ—¶ğ—´ğ—µ-ğ—¥ğ—²ğ˜€ ğ—˜ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿: The ConvNet/ViT sees the full 4K image cheaply (linear complexity).
 
2. ğ—§ğ—¼ğ—¸ğ—²ğ—» ğ—–ğ—¼ğ—ºğ—½ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»: instead of flattening the 2D feature map 1:1, you learn a set of "Latent Queries" that extract only the semantic essence.
 
3. ğ—¥ğ—²ğ˜€ğ˜‚ğ—¹ğ˜: You process 4K pixels but only send 64 or 128 highly compressed tokens to the LLM.
 

ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±: 
"ğ˜—ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜¤ğ˜³ğ˜¦ğ˜¢ğ˜µğ˜¦ğ˜´ ğ˜¢ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ ğ˜¦ğ˜¹ğ˜±ğ˜­ğ˜°ğ˜´ğ˜ªğ˜°ğ˜¯ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¥ğ˜¦ğ˜´ğ˜µğ˜³ğ˜°ğ˜ºğ˜´ ğ˜®ğ˜°ğ˜£ğ˜ªğ˜­ğ˜¦ ğ˜ªğ˜¯ğ˜§ğ˜¦ğ˜³ğ˜¦ğ˜¯ğ˜¤ğ˜¦. ğ˜›ğ˜©ğ˜¦ ğ˜±ğ˜³ğ˜°ğ˜£ğ˜­ğ˜¦ğ˜® ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¦ 1:1 ğ˜®ğ˜¢ğ˜±ğ˜±ğ˜ªğ˜¯ğ˜¨ ğ˜°ğ˜§ ğ˜±ğ˜¢ğ˜µğ˜¤ğ˜©-ğ˜µğ˜°-ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜¶ğ˜´ğ˜¦ ğ˜¢ ğ™ƒğ™®ğ™—ğ™§ğ™ğ™™ ğ˜¼ğ™§ğ™˜ğ™ğ™ğ™©ğ™šğ™˜ğ™©ğ™ªğ™§ğ™š ğ™¡ğ™ğ™ ğ™š ğ™ğ™–ğ™¨ğ™©ğ™‘ğ™‡ğ™ˆ, ğ˜¸ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜¢ ğ˜©ğ˜ªğ˜¨ğ˜©-ğ˜³ğ˜¦ğ˜´ ğ˜Šğ˜•ğ˜• ğ˜¦ğ˜¯ğ˜¤ğ˜°ğ˜¥ğ˜¦ğ˜³ ğ˜±ğ˜³ğ˜°ğ˜¤ğ˜¦ğ˜´ğ˜´ğ˜¦ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜¥ğ˜¦ğ˜µğ˜¢ğ˜ªğ˜­ğ˜´ ğ˜¦ğ˜§ğ˜§ğ˜ªğ˜¤ğ˜ªğ˜¦ğ˜¯ğ˜µğ˜­ğ˜º, ğ˜§ğ˜°ğ˜­ğ˜­ğ˜°ğ˜¸ğ˜¦ğ˜¥ ğ˜£ğ˜º ğ˜¢ ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜²ğ˜¶ğ˜¦ğ˜³ğ˜º ğ˜³ğ˜¦ğ˜´ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦ğ˜³ ğ˜µğ˜° ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜´ğ˜±ğ˜¢ğ˜µğ˜ªğ˜¢ğ˜­ ğ˜§ğ˜¦ğ˜¢ğ˜µğ˜¶ğ˜³ğ˜¦ğ˜´ ğ˜ªğ˜¯ğ˜µğ˜° ğ˜¢ ğ˜§ğ˜ªğ˜¹ğ˜¦ğ˜¥ ğ˜£ğ˜¶ğ˜¥ğ˜¨ğ˜¦ğ˜µ ğ˜°ğ˜§ ~256 ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ğ˜´ ğ˜£ğ˜¦ğ˜§ğ˜°ğ˜³ğ˜¦ ğ˜µğ˜©ğ˜¦ğ˜º ğ˜©ğ˜ªğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜“ğ˜“ğ˜”."
