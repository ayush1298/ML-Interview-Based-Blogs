ğ—ªğ—² ğ—µğ—®ğ˜ƒğ—² ğŸ­ğŸ¬,ğŸ¬ğŸ¬ğŸ¬ ğ—µğ—¼ğ˜‚ğ—¿ğ˜€ ğ—¼ğ—³ ğ˜‚ğ˜€ğ—²ğ—¿-ğ˜‚ğ—½ğ—¹ğ—¼ğ—®ğ—±ğ—²ğ—± ğ˜ƒğ—¶ğ—±ğ—²ğ—¼. ğ—ªğ—² ğ—»ğ—²ğ—²ğ—± ğ—® ğ˜€ğ—²ğ—®ğ—¿ğ—°ğ—µğ—®ğ—¯ğ—¹ğ—² ğ˜€ğ—²ğ—ºğ—®ğ—»ğ˜ğ—¶ğ—° ğ—¶ğ—»ğ—±ğ—²ğ˜… ğ—¯ğ˜† ğ˜ğ—¼ğ—ºğ—¼ğ—¿ğ—¿ğ—¼ğ˜„." ğŸ¤¯

The CEO wants to search for "ğ˜ˆ ğ˜¥ğ˜°ğ˜¨ ğ˜¤ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜§ğ˜³ğ˜ªğ˜´ğ˜£ğ˜¦ğ˜¦". The Engineer knows that video is just a 4D data tensor nightmare.

ğŸ…°ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—”: ğ—§ğ—µğ—² ğ—™ğ—¿ğ—®ğ—ºğ—²-ğ—¯ğ˜†-ğ—™ğ—¿ğ—®ğ—ºğ—² ğ—©ğ—Ÿğ—  Extract 1 frame per second. Send 3,600 images per hour to GPT-4o. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: You will hit the rate limit in 5 minutes and run out of budget in 10.

ğŸ…±ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—•: ğ—§ğ—µğ—² ğ—˜ğ—ºğ—¯ğ—²ğ—±ğ—±ğ—¶ğ—»ğ—´ ğ—¦ğ—µğ—¼ğ—¿ğ˜ğ—°ğ˜‚ğ˜ (ğ—–ğ—Ÿğ—œğ—£) Just average the CLIP embeddings of the frames. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: You lose temporal causality. The model can't tell the difference between "ğ˜ˆ ğ˜¥ğ˜°ğ˜¨ ğ˜¤ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜§ğ˜³ğ˜ªğ˜´ğ˜£ğ˜¦ğ˜¦" and "ğ˜ˆ ğ˜¥ğ˜°ğ˜¨ ğ˜¥ğ˜³ğ˜°ğ˜±ğ˜±ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜§ğ˜³ğ˜ªğ˜´ğ˜£ğ˜¦ğ˜¦".

ğŸ”‘ ğ—§ğ—µğ—² "ğ—§ğ—µğ—¶ğ—¿ğ—± ğ——ğ—¼ğ—¼ğ—¿" ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—ğ—²ğ˜†ğ—³ğ—¿ğ—®ğ—ºğ—² ğ—¦ğ—²ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ˜„ğ—¶ğ˜ğ—µ ğ—©ğ—Ÿğ—  ğ—–ğ—®ğ—½ğ˜ğ—¶ğ—¼ğ—»ğ—¶ğ—»ğ—´ We stop treating video as a stream of images. We treat it as a stream of ğ˜¦ğ˜·ğ˜¦ğ˜¯ğ˜µğ˜´.

1. Use a lightweight boundary detection algorithm (ğš™ğš¢ğšœğšŒğšğš—ğšğšğšğšğšğšŒğš) to find scene cuts.
 
2. Extract only ğ—¼ğ—»ğ—² ğ—¿ğ—²ğ—½ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ˜ƒğ—² ğ—³ğ—¿ğ—®ğ—ºğ—² per scene.
 
3. Send that single frame to a LLaVA/VLM to generate a dense text caption.
 
4. Embed the ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜¤ğ˜¢ğ˜±ğ˜µğ˜ªğ˜°ğ˜¯ into the vector DB, not the image.
 
ğ—§ğ—µğ—² ğ—¢ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—²: We reduce the compute load by 95% (from 1 FPS to 1 frame per scene) while actually ğ˜ªğ˜¯ğ˜¤ğ˜³ğ˜¦ğ˜¢ğ˜´ğ˜ªğ˜¯ğ˜¨ search accuracy because we search against rich text descriptions, not noisy pixel vectors.

ğŸ“– ğ—§ğ—µğ—² ğ—Ÿğ—²ğ˜€ğ˜€ğ—¼ğ—»: The best compression algorithm for video isn't H.264. It's ğ—§ğ—²ğ˜…ğ˜.
