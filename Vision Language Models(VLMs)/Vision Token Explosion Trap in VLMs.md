ğ™ğ™ğ™š ğ™‘ğ™ğ™¨ğ™ğ™¤ğ™£ ğ™ğ™¤ğ™ ğ™šğ™£ ğ™€ğ™­ğ™¥ğ™¡ğ™¤ğ™¨ğ™ğ™¤ğ™£ ğ™ğ™§ğ™–ğ™¥ ğ™ğ™£ ğ™‘ğ™‡ğ™ˆğ™¨ ğŸ’¥

Youâ€™re in a Senior ML Engineer interview at Meta. The interviewer shows a 2880Ã—2160 scanned document and says:

â€œğ˜–ğ˜¶ğ˜³ ğ˜ğ˜ªğ˜´ğ˜ªğ˜°ğ˜¯-ğ˜“ğ˜¢ğ˜¯ğ˜¨ğ˜¶ğ˜¢ğ˜¨ğ˜¦ ğ˜”ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜ªğ˜´ ğ˜¨ğ˜³ğ˜¦ğ˜¢ğ˜µ ğ˜°ğ˜¯ ğ˜£ğ˜¦ğ˜¯ğ˜¤ğ˜©ğ˜®ğ˜¢ğ˜³ğ˜¬ğ˜´, ğ˜£ğ˜¶ğ˜µ ğ˜›ğ˜ªğ˜®ğ˜¦-ğ˜µğ˜°-ğ˜ğ˜ªğ˜³ğ˜´ğ˜µ-ğ˜›ğ˜°ğ˜¬ğ˜¦ğ˜¯ (ğ˜›ğ˜›ğ˜ğ˜›) ğ˜«ğ˜¶ğ˜®ğ˜±ğ˜´ ğ˜µğ˜° 8+ ğ˜´ğ˜¦ğ˜¤ğ˜°ğ˜¯ğ˜¥ğ˜´ ğ˜°ğ˜¯ ğ˜µğ˜©ğ˜¦ğ˜´ğ˜¦ ğ˜©ğ˜ªğ˜¨ğ˜©-ğ˜³ğ˜¦ğ˜´ ğ˜ªğ˜®ğ˜¢ğ˜¨ğ˜¦ğ˜´. ğ˜–ğ˜¯ğ˜¦ ğ˜¦ğ˜¯ğ˜¨ğ˜ªğ˜¯ğ˜¦ğ˜¦ğ˜³ ğ˜´ğ˜¶ğ˜¨ğ˜¨ğ˜¦ğ˜´ğ˜µğ˜´ ğ˜¸ğ˜¦ ğ˜«ğ˜¶ğ˜´ğ˜µ ğ˜³ğ˜¦ğ˜¥ğ˜¶ğ˜¤ğ˜¦ ğ˜³ğ˜¦ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯. ğ˜ğ˜©ğ˜¢ğ˜µâ€™ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜³ğ˜¦ğ˜¢ğ˜­ ğ˜¢ğ˜³ğ˜¤ğ˜©ğ˜ªğ˜µğ˜¦ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¢ğ˜­ ğ˜£ğ˜°ğ˜µğ˜µğ˜­ğ˜¦ğ˜¯ğ˜¦ğ˜¤ğ˜¬, ğ˜¢ğ˜¯ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜±ğ˜³ğ˜°ğ˜¥ğ˜¶ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯-ğ˜¨ğ˜³ğ˜¢ğ˜¥ğ˜¦ ğ˜§ğ˜ªğ˜¹?â€

ğŸµğŸ¬% ğ—¼ğ—³ ğ—°ğ—®ğ—»ğ—±ğ—¶ğ—±ğ—®ğ˜ğ—²ğ˜€ ğ˜€ğ—®ğ˜†:

â€œğ˜›ğ˜°ğ˜° ğ˜®ğ˜¢ğ˜¯ğ˜º ğ˜·ğ˜ªğ˜´ğ˜¶ğ˜¢ğ˜­ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ğ˜´ ğ˜´ğ˜­ğ˜°ğ˜¸ ğ˜¥ğ˜°ğ˜¸ğ˜¯ ğ˜“ğ˜“ğ˜”'ğ˜´ ğ˜¢ğ˜µğ˜µğ˜¦ğ˜¯ğ˜µğ˜ªğ˜°ğ˜¯. ğ˜™ğ˜¦ğ˜¥ğ˜¶ğ˜¤ğ˜¦ ğ˜³ğ˜¦ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜°ğ˜³ ğ˜¥ğ˜° ğ˜¥ğ˜ºğ˜¯ğ˜¢ğ˜®ğ˜ªğ˜¤ ğ˜±ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜° ğ˜£ğ˜¢ğ˜­ğ˜¢ğ˜¯ğ˜¤ğ˜¦ ğ˜´ğ˜±ğ˜¦ğ˜¦ğ˜¥ ğ˜¢ğ˜¯ğ˜¥ ğ˜²ğ˜¶ğ˜¢ğ˜­ğ˜ªğ˜µğ˜º.â€

Sounds plausible. Itâ€™s ğ˜„ğ—¿ğ—¼ğ—»ğ—´.

ğŸ§  The Reality: Vision Encoder, Not LLM, Is The Bottleneck

At 2880Ã—2160 with 14Ã—14 patches, the ViT-style encoder creates ~2,880 tokens and does full self-attention on all patches before the LLM sees anything. That attention is quadratic in tokens = massive latency.â€‹

LLM prefill is mostly parallel; vision encoding is a sequential choke point.
Resizing kills latency by destroying critical fine details, bad for document OCR.

ğŸš€ ğ—£ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—»-ğ—šğ—¿ğ—®ğ—±ğ—² ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—›ğ˜†ğ—¯ğ—¿ğ—¶ğ—± ğ—©ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ—˜ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿ğ˜€ (ğ—™ğ—®ğ˜€ğ˜ğ—©ğ—¶ğ—§, ğ—™ğ—®ğ˜€ğ˜ğ—©ğ—Ÿğ— )

Why this works:
â€¢ Conv Front-End: Early layers with depthwise convolutions cheaply downsample spatial info (linear cost).
â€¢ Sparse Transformer Back-End: Attention only on compressed tokens; drastically fewer tokens to attend.

Visual tokens are redundant, neighbors have near-1 cosine similarity, wasting compute on repeated info.â€‹

âš¡ ğ—œğ—ºğ—½ğ—®ğ—°ğ˜:

â€¢ TTFT from ~8 seconds to ~800ms
â€¢ No accuracy loss on OCR tasks
â€¢ Scales gracefully to 4K+ images without exploding latency

ğŸ¯ ğ—§ğ—µğ—² ğ—›ğ—¶ğ—¿ğ—¶ğ—»ğ—´ ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿:

â€œğ˜™ğ˜¦ğ˜¥ğ˜¶ğ˜¤ğ˜ªğ˜¯ğ˜¨ ğ˜³ğ˜¦ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ªğ˜´ ğ˜¢ ğ˜§ğ˜¢ğ˜­ğ˜´ğ˜¦ ğ˜¦ğ˜¤ğ˜°ğ˜¯ğ˜°ğ˜®ğ˜ºâ€”ğ˜´ğ˜¢ğ˜¤ğ˜³ğ˜ªğ˜§ğ˜ªğ˜¤ğ˜ªğ˜¯ğ˜¨ ğ˜´ğ˜ªğ˜¨ğ˜¯ğ˜¢ğ˜­ ğ˜²ğ˜¶ğ˜¢ğ˜­ğ˜ªğ˜µğ˜º ğ˜µğ˜° ğ˜³ğ˜¦ğ˜¥ğ˜¶ğ˜¤ğ˜¦ ğ˜­ğ˜¢ğ˜µğ˜¦ğ˜¯ğ˜¤ğ˜º. ğ˜›ğ˜©ğ˜¦ ğ˜£ğ˜°ğ˜µğ˜µğ˜­ğ˜¦ğ˜¯ğ˜¦ğ˜¤ğ˜¬ ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜ğ˜ªğ˜› ğ˜¦ğ˜¯ğ˜¤ğ˜°ğ˜¥ğ˜¦ğ˜³â€™ğ˜´ ğ˜²ğ˜¶ğ˜¢ğ˜¥ğ˜³ğ˜¢ğ˜µğ˜ªğ˜¤ ğ˜´ğ˜¦ğ˜­ğ˜§-ğ˜¢ğ˜µğ˜µğ˜¦ğ˜¯ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜°ğ˜¯ ğ˜¢ ğ˜®ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜·ğ˜¦ ğ˜±ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜¨ğ˜³ğ˜ªğ˜¥. ğ˜â€™ğ˜¥ ğ˜³ğ˜¦ğ˜±ğ˜­ğ˜¢ğ˜¤ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜ğ˜ªğ˜› ğ˜¦ğ˜¯ğ˜¤ğ˜°ğ˜¥ğ˜¦ğ˜³ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜¢ ğ˜©ğ˜ºğ˜£ğ˜³ğ˜ªğ˜¥ ğ˜°ğ˜¯ğ˜¦â€”ğ˜¤ğ˜°ğ˜¯ğ˜· ğ˜­ğ˜¢ğ˜ºğ˜¦ğ˜³ğ˜´ ğ˜§ğ˜ªğ˜³ğ˜´ğ˜µ ğ˜µğ˜° ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜¥ğ˜°ğ˜¸ğ˜¯ğ˜´ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦, ğ˜µğ˜©ğ˜¦ğ˜¯ ğ˜¢ğ˜µğ˜µğ˜¦ğ˜¯ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜°ğ˜¯ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜´ğ˜¦ğ˜¥ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ğ˜´. ğ˜›ğ˜©ğ˜ªğ˜´ ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜¦ğ˜³ğ˜·ğ˜¦ğ˜´ ğ˜³ğ˜¦ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¯ğ˜¥ ğ˜¥ğ˜¦ğ˜µğ˜¢ğ˜ªğ˜­ ğ˜£ğ˜¶ğ˜µ ğ˜´ğ˜­ğ˜¢ğ˜´ğ˜©ğ˜¦ğ˜´ ğ˜¦ğ˜¯ğ˜¤ğ˜°ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜­ğ˜¢ğ˜µğ˜¦ğ˜¯ğ˜¤ğ˜º ğ˜£ğ˜º ~10ğ˜¹.â€
