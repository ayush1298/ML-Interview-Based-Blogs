"ğ—ªğ—² ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—²ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ˜ğ—²ğ—ºğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğŸ±ğŸ¬,ğŸ¬ğŸ¬ğŸ¬ ğ˜€ğ—°ğ—®ğ—»ğ—»ğ—²ğ—±, ğ—ºğ—²ğ˜€ğ˜€ğ˜† ğ—¶ğ—»ğ˜ƒğ—¼ğ—¶ğ—°ğ—²ğ˜€. ğ—”ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† ğ—ºğ˜‚ğ˜€ğ˜ ğ—¯ğ—² >ğŸµğŸµ%." ğŸ“„

The PM thinks "GPT-4V can read anything." The Engineer knows VLMs struggle with spatial coordinate precision.

ğŸ…°ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—”: ğ—£ğ˜‚ğ—¿ğ—² ğ—©ğ—Ÿğ—  (ğ—˜ğ—»ğ—±-ğ˜ğ—¼-ğ—˜ğ—»ğ—±) Feed the image of the invoice to the VLM. Ask for JSON output. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: ğ—¡ğ˜‚ğ—ºğ—²ğ—¿ğ—¶ğ—° ğ—›ğ—®ğ—¹ğ—¹ğ˜‚ğ—°ğ—¶ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—». The VLM reads "$100.00" as "$1000.00" because a speck of dust looked like a zero. It misses the "Total" row because it was at the bottom right (attention sink).

ğŸ…±ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—•: ğ—¢ğ—–ğ—¥ + ğ—Ÿğ—Ÿğ—  (ğ—§ğ—²ğ˜…ğ˜-ğ—¢ğ—»ğ—¹ğ˜†) Run Tesseract/PaddleOCR, flatten the text, send to LLM. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: ğ—Ÿğ—¼ğ˜€ğ˜€ ğ—¼ğ—³ ğ—Ÿğ—®ğ˜†ğ—¼ğ˜‚ğ˜. The "Price" column gets mixed with the "Quantity" column because the whitespace was lost. The LLM can't figure out which number belongs to which header.

ğŸ”‘ ğ—§ğ—µğ—² "ğ—§ğ—µğ—¶ğ—¿ğ—± ğ——ğ—¼ğ—¼ğ—¿" ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—§ğ—µğ—² ğ—¦ğ—²ğ—»ğ˜ğ—¶ğ—»ğ—²ğ—¹-ğ—šğ˜‚ğ—¶ğ—±ğ—²ğ—± ğ—”ğ—»ğ—°ğ—µğ—¼ğ—¿ We use a hybrid approach.

1. Run a cheap OCR model to get text + bounding box coordinates.
 
2. We draw ğ—©ğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ ğ— ğ—®ğ—¿ğ—¸ğ—²ğ—¿ğ˜€ (e.g., red boxes with ID numbers) on the image around the text fields.
 
3. We feed the ğ˜®ğ˜¢ğ˜³ğ˜¬ğ˜¦ğ˜¥ image to the VLM and say: "What value is in box #12?"
 
4. We validate the VLM's answer against the raw OCR text.
 

ğ—§ğ—µğ—² ğ—¢ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—²: The VLM handles the semantic understanding ("This is a Tax field"), while the OCR provides the character-level ground truth.

ğŸ“– ğ—§ğ—µğ—² ğ—Ÿğ—²ğ˜€ğ˜€ğ—¼ğ—»: VLMs are for ğ—¥ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´. OCR is for ğ—¥ğ—²ğ—®ğ—±ğ—¶ğ—»ğ—´. Don't ask a philosopher to do a typist's job.
