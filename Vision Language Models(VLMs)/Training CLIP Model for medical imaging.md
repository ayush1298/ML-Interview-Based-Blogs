You're in a Machine Learning System Design interview at OpenAI. The interviewer sets a trap:

"We need to train a specialized CLIP model for medical imaging from scratch. You have a node of 8 A100s. What batch size do you configure?"

95% of candidates walk right into the trap.

The candidates pull out a calculator and optimizes for VRAM.

"Well, with a ViT-L backbone and high-res X-rays, a batch size of 64 per GPU is the safe limit to avoid OOM errors. So, a global batch size of 512."

It sounds like a competent engineering answer. They respected the hardware constraints. They avoided the crash.

They also just guaranteed the model will be useless.

In Supervised Learning (e.g., ResNet classifier), batch size is just a gradient estimation tool. Smaller batches are often fine (or even better for regularization).

But in ğ‚ğ¨ğ§ğ­ğ«ğšğ¬ğ­ğ¢ğ¯ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ  (ğ‚ğ‹ğˆğ), your batch size is your dataset.
- With a batch of N, you have N correct pairs (positives).
- But more importantly, you have N^2 - N incorrect pairs (negatives).

If N=64, your model only has to distinguish the correct X-ray from 63 random other X-rays. That is too easy. The model learns trivial features (e.g., "this image is bright") and converges instantly. It never learns the semantic nuance.

You need to explain that for CLIP, Batch Size â‰  Speed. Batch Size = Intelligence.

To force the model to learn robust features, it needs to distinguish the correct image from thousands of hard negatives in a single pass. The original CLIP paper didn't use massive batches just for speed; they did it because the loss function breaks down mathematically at small scales.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
1ï¸âƒ£ ğ˜ğ˜¨ğ˜¯ğ˜°ğ˜³ğ˜¦ ğ˜ğ˜™ğ˜ˆğ˜” ğ˜­ğ˜ªğ˜®ğ˜ªğ˜µğ˜´: You cannot train effective CLIP on a single GPU's memory limit.
2ï¸âƒ£ ğ˜‹ğ˜ªğ˜´ğ˜µğ˜³ğ˜ªğ˜£ğ˜¶ğ˜µğ˜¦ğ˜¥ ğ˜•ğ˜¦ğ˜¨ğ˜¢ğ˜µğ˜ªğ˜·ğ˜¦ğ˜´: Use torch.distributed.all_gather to collect embeddings from all GPUs before calculating the loss matrix.
3ï¸âƒ£ ğ˜›ğ˜¢ğ˜³ğ˜¨ğ˜¦ğ˜µ ğ˜ğ˜ªğ˜¨ğ˜© ğ˜•ğ˜¶ğ˜®ğ˜£ğ˜¦ğ˜³ğ˜´: You need a global effective batch size of 4,096 minimum. If you can't fit it, use techniques like Gradient Checkpointing or Gradient Caching to simulate the larger matrix.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ
"In Contrastive Learning, the batch size is a hyperparameter for model quality, not just hardware efficiency. If we can't hit 4k+ samples per step, we shouldn't start training."
