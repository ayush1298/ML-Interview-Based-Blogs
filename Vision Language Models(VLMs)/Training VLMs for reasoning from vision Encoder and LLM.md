ğ™ğ™š ğ™ğ™ğ™–ğ™¡ğ™¡ğ™¤ğ™¬ ğ˜¼ğ™¡ğ™ğ™œğ™£ğ™¢ğ™šğ™£ğ™© ğ™ğ™§ğ™–ğ™¥ ğŸ”—

You're in a Research Scientist interview at Google Google DeepMind. The interviewer asks:

"ğ˜ğ˜¦ ğ˜©ğ˜¢ğ˜·ğ˜¦ ğ˜¢ ğ˜±ğ˜³ğ˜¦-ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜¦ğ˜¥ ğ˜ğ˜ªğ˜´ğ˜ªğ˜°ğ˜¯ ğ˜Œğ˜¯ğ˜¤ğ˜°ğ˜¥ğ˜¦ğ˜³ (ğ˜ğ˜ªğ˜›-ğ˜“) ğ˜¢ğ˜¯ğ˜¥ ğ˜¢ ğ˜§ğ˜³ğ˜°ğ˜»ğ˜¦ğ˜¯ ğ˜“ğ˜“ğ˜” (ğ˜ğ˜­ğ˜¢ğ˜¯-ğ˜›5). ğ˜ğ˜¦ ğ˜¸ğ˜¢ğ˜¯ğ˜µ ğ˜µğ˜° ğ˜£ğ˜¶ğ˜ªğ˜­ğ˜¥ ğ˜¢ ğ˜ğ˜ªğ˜´ğ˜¶ğ˜¢ğ˜­ ğ˜“ğ˜¢ğ˜¯ğ˜¨ğ˜¶ğ˜¢ğ˜¨ğ˜¦ ğ˜”ğ˜°ğ˜¥ğ˜¦ğ˜­ (ğ˜ğ˜“ğ˜”) ğ˜¤ğ˜¢ğ˜±ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜°ğ˜§ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜­ğ˜¦ğ˜¹ ğ˜³ğ˜¦ğ˜¢ğ˜´ğ˜°ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¢ğ˜£ğ˜°ğ˜¶ğ˜µ ğ˜ªğ˜®ğ˜¢ğ˜¨ğ˜¦ğ˜´. ğ˜ğ˜°ğ˜¸ ğ˜¥ğ˜° ğ˜ºğ˜°ğ˜¶ ğ˜¤ğ˜°ğ˜¯ğ˜¯ğ˜¦ğ˜¤ğ˜µ ğ˜µğ˜©ğ˜¦ğ˜®?"

ğŸ•¸ï¸ 90% of candidates walk right into the trap.

They say: "ğ˜ğ˜¦ ğ˜´ğ˜©ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ ğ˜¢ ğ˜´ğ˜ªğ˜®ğ˜±ğ˜­ğ˜¦ ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜¢ğ˜³ ğ˜±ğ˜³ğ˜°ğ˜«ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜­ğ˜¢ğ˜ºğ˜¦ğ˜³ ğ˜µğ˜° ğ˜®ğ˜¢ğ˜± ğ˜µğ˜©ğ˜¦ ğ˜ğ˜ªğ˜› ğ˜¦ğ˜®ğ˜£ğ˜¦ğ˜¥ğ˜¥ğ˜ªğ˜¯ğ˜¨ğ˜´ ğ˜µğ˜° ğ˜µğ˜©ğ˜¦ ğ˜“ğ˜“ğ˜”'ğ˜´ ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ ğ˜¥ğ˜ªğ˜®ğ˜¦ğ˜¯ğ˜´ğ˜ªğ˜°ğ˜¯."

This works for simple captioning. It fails for reasoning.

ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜†: ğ—§ğ—µğ—² ğ— ğ—¼ğ—±ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—šğ—®ğ—½ ğ—¶ğ˜€ ğ—±ğ—²ğ—²ğ—½ğ—²ğ—¿ ğ˜ğ—µğ—®ğ—» ğ—® ğ—¹ğ—¶ğ—»ğ—²ğ—®ğ—¿ ğ—¹ğ—®ğ˜†ğ—²ğ—¿.

A Vision Transformer outputs spatial features (patches). An LLM expects semantic sequences.

- ğ—œğ—»ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—•ğ—¼ğ˜ğ˜ğ—¹ğ—²ğ—»ğ—²ğ—°ğ—¸: A linear layer forces a 1:1 mapping between visual patches and language tokens. It lacks the capacity to "digest" or "select" the relevant visual information for a specific text query.
 
- ğ—¡ğ—¼ğ—¶ğ˜€ğ—²: The LLM gets flooded with irrelevant visual details from the background patches, diluting its reasoning ability.
 

âœ… ğ—§ğ—µğ—² ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—¬ğ—¼ğ˜‚ ğ—»ğ—²ğ—²ğ—± ğ—® ğ—¤ğ˜‚ğ—²ğ—¿ğ˜†ğ—¶ğ—»ğ—´ ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—²ğ—¿ (ğ—¤-ğ—™ğ—¼ğ—¿ğ—ºğ—²ğ—¿).

Instead of a passive projection, you need an active "Bridge" module (like in BLIP-2 or Flamingo).

1. ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—®ğ—¯ğ—¹ğ—² ğ—¤ğ˜‚ğ—²ğ—¿ğ—¶ğ—²ğ˜€: You create a set of static "Query Tokens" (e.g., 32 tokens).
 
2. ğ—–ğ—¿ğ—¼ğ˜€ğ˜€-ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—»: These queries attend to the image features through a small Transformer block.
 
3. ğ—˜ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»: The queries extract ğ˜°ğ˜¯ğ˜­ğ˜º the visual information most relevant to the text condition, compressing the image into a dense, semantic summary before it ever hits the LLM.
 

âœï¸ ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±:

"ğ˜ˆ ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜¢ğ˜³ ğ˜±ğ˜³ğ˜°ğ˜«ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¶ğ˜¯ğ˜¥ğ˜¦ğ˜³ğ˜§ğ˜ªğ˜µğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¢ğ˜­ğ˜ªğ˜µğ˜º ğ˜¨ğ˜¢ğ˜±. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜ªğ˜®ğ˜±ğ˜­ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µ ğ˜¢ ğ˜˜-ğ˜ğ˜°ğ˜³ğ˜®ğ˜¦ğ˜³ (ğ˜°ğ˜³ ğ˜™ğ˜¦ğ˜´ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦ğ˜³) ğ˜¢ğ˜³ğ˜¤ğ˜©ğ˜ªğ˜µğ˜¦ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦. ğ˜‰ğ˜º ğ˜¶ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜²ğ˜¶ğ˜¦ğ˜³ğ˜º ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜¤ğ˜³ğ˜°ğ˜´ğ˜´-ğ˜¢ğ˜µğ˜µğ˜¦ğ˜¯ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜­ğ˜¢ğ˜ºğ˜¦ğ˜³ğ˜´, ğ˜¸ğ˜¦ ğ˜¥ğ˜ºğ˜¯ğ˜¢ğ˜®ğ˜ªğ˜¤ğ˜¢ğ˜­ğ˜­ğ˜º ğ˜¦ğ˜¹ğ˜µğ˜³ğ˜¢ğ˜¤ğ˜µ ğ˜¢ğ˜¯ğ˜¥ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜·ğ˜ªğ˜´ğ˜¶ğ˜¢ğ˜­ ğ˜§ğ˜¦ğ˜¢ğ˜µğ˜¶ğ˜³ğ˜¦ğ˜´ ğ˜ªğ˜¯ğ˜µğ˜° ğ˜¢ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜¢ğ˜¤ğ˜µ ğ˜´ğ˜¦ğ˜®ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜¤ ğ˜³ğ˜¦ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜¦ğ˜¯ğ˜µğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜­ğ˜ªğ˜¨ğ˜¯ğ˜¦ğ˜¥ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜µğ˜©ğ˜¦ ğ˜“ğ˜“ğ˜”'ğ˜´ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ ğ˜´ğ˜±ğ˜¢ğ˜¤ğ˜¦, ğ˜¦ğ˜¯ğ˜¢ğ˜£ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜¥ğ˜¦ğ˜¦ğ˜±ğ˜¦ğ˜³ ğ˜³ğ˜¦ğ˜¢ğ˜´ğ˜°ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¤ğ˜¢ğ˜±ğ˜¢ğ˜£ğ˜ªğ˜­ğ˜ªğ˜µğ˜ªğ˜¦ğ˜´."
