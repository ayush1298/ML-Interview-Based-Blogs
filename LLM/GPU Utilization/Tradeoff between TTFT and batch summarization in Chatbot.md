You're in an ML Engineer interview at Google and the interviewer asks:

"We need to serve our model for two different use cases: a low-latency chatbot that needs a fast ğ“ğ¢ğ¦ğ-ğ­ğ¨-ğ…ğ¢ğ«ğ¬ğ­-ğ“ğ¨ğ¤ğğ§ (ğ“ğ“ğ…ğ“), and a high-throughput batch summarization job. How do these two workloads stress the GPU differently, and what fundamental tradeoff are you managing?"

Don't say: "For the chatbot, I'd use a small batch size (like 1) for low latency. For the batch job, I'd use a large batch size for high throughput."

This is the classic answer. It's the ğ˜¸ğ˜©ğ˜¢ğ˜µ, not the ğ˜¸ğ˜©ğ˜º. It completely misses the underlying hardware bottleneck.

The reality is that LLM inference isn't one workload. It's two distinct phases with opposite performance profiles.

1. ğ“ğ¡ğ ğğ«ğğŸğ¢ğ¥ğ¥ ğğ¡ğšğ¬ğ (ğ˜ğ¨ğ®ğ« ğ‚ğ¡ğšğ­ğ›ğ¨ğ­'ğ¬ ğ“ğ“ğ…ğ“)
- This is processing the entire input prompt (e.g., 2,000 tokens) all at once.
- It's a massively parallel operation, full of large matrix multiplications.
- This phase is ğ‚ğ¨ğ¦ğ©ğ®ğ­ğ-ğğ¨ğ®ğ§ğ. It can actually saturate the GPU's tensor cores. Your chatbot's TTFT is almost ğ˜¦ğ˜¯ğ˜µğ˜ªğ˜³ğ˜¦ğ˜­ğ˜º dominated by this prefill speed.

2. ğ“ğ¡ğ ğ†ğğ§ğğ«ğšğ­ğ¢ğ¨ğ§ ğğ¡ğšğ¬ğ (ğ˜ğ¨ğ®ğ« ğğšğ­ğœğ¡ ğ‰ğ¨ğ›'ğ¬ ğ“ğ¡ğ«ğ¨ğ®ğ ğ¡ğ©ğ®ğ­)
- This is generating one token at a time, auto-regressively.
- For every single token, the GPU must read the entire, massive KV cache from high-bandwidth memory (HBM).
- This phase is Memory-Bandwidth-Bound. The GPU's compute units are starved, just sitting idle waiting for data.

Think of it this way: Prefill is a drag race (all compute, right now). Generation is a factory assembly line (your speed is limited by how fast you can move parts from the warehouse).

Your chatbot's latency is a ğœğ¨ğ¦ğ©ğ®ğ­ğ ğ©ğ«ğ¨ğ›ğ¥ğğ¦.
Your batch job's throughput is a ğ¦ğğ¦ğ¨ğ«ğ² ğ›ğšğ§ğğ°ğ¢ğğ­ğ¡ ğ©ğ«ğ¨ğ›ğ¥ğğ¦.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:

"The core tradeoff is managing for two different bottlenecks. The chatbot's ğ“ğ¢ğ¦ğ-ğ­ğ¨-ğ…ğ¢ğ«ğ¬ğ­-ğ“ğ¨ğ¤ğğ§ is dominated by the ğœğ¨ğ¦ğ©ğ®ğ­ğ-ğ›ğ¨ğ®ğ§ğ ğ©ğ«ğğŸğ¢ğ¥ğ¥ phase. The batch job's throughput is limited by the ğ¦ğğ¦ğ¨ğ«ğ²-ğ›ğšğ§ğğ°ğ¢ğğ­ğ¡-ğ›ğ¨ğ®ğ§ğ ğ ğğ§ğğ«ğšğ­ğ¢ğ¨ğ§ phase. I'd optimize the prefill for the chatbot, but for the batch job, I'd focus on saturating memory bandwidth, likely with larger batches, to maximize token throughput."
