You're in a GenAI Engineer interview at NVIDIA, and the interviewer asks:
"We need to deploy a 70B parameter LLM for production inference. Should we use 4x A100 40GB or 1x H100 80GB? Justify your choice."
Here's how you can answer:
A. Most candidates fumble here because they only know "more GPUs = more power." Incomplete answer.
B. There are 5 critical factors every GenAI engineer should understand cold.

ğŸ­. ğ—§ğ—µğ—² ğ—–ğ—¼ğ—ºğ—ºğ˜‚ğ—»ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¢ğ˜ƒğ—²ğ—¿ğ—µğ—²ğ—®ğ—± - ğ—§ğ—µğ—² ğ—µğ—¶ğ—±ğ—±ğ—²ğ—» ğ—½ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² ğ—¸ğ—¶ğ—¹ğ—¹ğ—²ğ—¿
Single GPU: Zero communication overhead
Pure compute speed
No synchronization barriers
Multi-GPU: Inter-GPU communication becomes bottleneck
All-reduce operations on EVERY layer
The brutal truth? Multi-GPU adds 20-40% latency overhead for autoregressive generation.

ğŸ®. ğ—§ğ—µğ—² ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¦ğ˜ğ—¿ğ—®ğ˜ğ—²ğ—´ğ˜† - ğ—ªğ—µğ—²ğ—¿ğ—² ğŸ´ğŸ±% ğ—¼ğ—³ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ˜€ ğ—´ğ—¼ ğ˜„ğ—¿ğ—¼ğ—»ğ—´
Most people think "split model = split work equally."
Wrong move.
Tensor Parallelism (TP):
Splits individual layers across GPUs
Requires communication EVERY forward pass
Best for: Ultra-large models that don't fit on single GPU
Pipeline Parallelism (PP):
Splits layers sequentially across GPUs
Bubble overhead 10-30%
Best for: Training, not inference
The counterintuitive reality? For inference, you want MAXIMUM model parallelism, MINIMUM communication.

ğŸ¯. ğ—§ğ—µğ—² ğ—•ğ—®ğ˜ğ—°ğ—µ ğ—¦ğ—¶ğ˜‡ğ—² ğ—§ğ—¿ğ—®ğ—±ğ—²ğ—¼ğ—³ğ—³ - ğ—§ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ ğ—²ğ—°ğ—¼ğ—»ğ—¼ğ—ºğ—¶ğ—°ğ˜€
Single GPU (H100 80GB):
Batch size: 4-8 for 70B model
Throughput: ~200 tokens/sec total
Latency: 50-80ms per token
Multi-GPU (4x A100 40GB):
Batch size: 16-32 theoretically
Throughput: ~280 tokens/sec total
Latency: 70-120ms per token
But here's the catch - higher batch size only helps if you HAVE that many concurrent requests.
Real-world API serving? Avg concurrent requests = 2-6, not 32.

ğŸ°. ğ—§ğ—µğ—² ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ—•ğ—®ğ—»ğ—±ğ˜„ğ—¶ğ—±ğ˜ğ—µ ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜† - ğ—ªğ—µğ˜† ğ—»ğ—²ğ˜„ğ—²ğ—¿ â‰  ğ—¯ğ—²ğ˜ğ˜ğ—²ğ—¿ ğ—®ğ—¹ğ˜„ğ—®ğ˜†ğ˜€
H100 80GB:
Memory bandwidth: 3.35 TB/s (HBM3)
Single point of failure
4x A100 40GB:
Memory bandwidth: 6.4 TB/s combined (4 Ã— 1.6 TB/s)
Redundancy built-in
Multi-GPU bandwidth is NOT additive for sequential operations.
LLM inference is memory-bound, NOT compute-bound. You're reading weights, not multiplying.

ğŸ±. ğ—§ğ—µğ—² ğ—™ğ—®ğ—¶ğ—¹ğ˜‚ğ—¿ğ—² ğ— ğ—¼ğ—±ğ—² ğ—§ğ—¿ğ—®ğ—±ğ—²ğ—¼ğ—³ğ—³ - ğ—§ğ—µğ—² ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¿ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—»ğ—¼ğ—¯ğ—¼ğ—±ğ˜† ğ˜ğ—®ğ—¹ğ—¸ğ˜€ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜
Single GPU: 1 GPU fails = 100% downtime
Multi-GPU: 1 GPU fails = 100% downtime (model split across all GPUs)
Wait, what?
Unless you implement replica groups, multi-GPU gives you ZERO fault tolerance.

ğ—ªğ—µğ—²ğ—» ğ—¦ğ—¶ğ—»ğ—´ğ—¹ğ—² ğ—šğ—£ğ—¨ ğ˜„ğ—¶ğ—»ğ˜€:
âœ… Low latency critical (real-time applications) 
âœ… Low concurrent request volume (<10) 
âœ… Simplicity > raw throughput 
âœ… Newer GPU architecture available

ğ—ªğ—µğ—²ğ—» ğ— ğ˜‚ğ—¹ğ˜ğ—¶-ğ—šğ—£ğ—¨ ğ˜„ğ—¶ğ—»ğ˜€:
âœ… High throughput workloads (batch processing) 
âœ… Model too large for single GPU 
âœ… High concurrent request volume (>20) 
âœ… Cost per token > latency optimization
