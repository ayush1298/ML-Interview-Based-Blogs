ğ™ğ™ğ™š ğ™ğ™£ğ™™ğ™šğ™§-ğ™ğ™©ğ™ğ™¡ğ™ğ™¯ğ™ğ™£ğ™œ ğ™ğ™šğ™£ğ™¨ğ™¤ğ™§ ğ˜¾ğ™¤ğ™§ğ™šğ™¨ ğ™ğ™§ğ™–ğ™¥ ğŸ–¥ï¸

You're in a Senior ML Interview at NVIDIA. The interviewer sets a trap:

"ğ˜ ğ˜°ğ˜¶ ğ˜¢ğ˜³ğ˜¦ ğ˜³ğ˜¶ğ˜¯ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜®ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜·ğ˜¦ ğ˜ğ˜—16 ğ˜ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜­ ğ˜”ğ˜¢ğ˜µğ˜³ğ˜ªğ˜¹ ğ˜”ğ˜¶ğ˜­ğ˜µğ˜ªğ˜±ğ˜­ğ˜º (ğ˜ğ˜Œğ˜”ğ˜”) ğ˜°ğ˜¯ ğ˜¢ğ˜¯ ğ˜ˆ100 ğ˜ğ˜—ğ˜œ ğ˜¶ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜©ğ˜¢ğ˜¯ğ˜¥-ğ˜¤ğ˜°ğ˜¥ğ˜¦ğ˜¥ ğ˜Šğ˜œğ˜‹ğ˜ˆ ğ˜¬ğ˜¦ğ˜³ğ˜¯ğ˜¦ğ˜­. ğ˜ ğ˜°ğ˜¶'ğ˜³ğ˜¦ ğ˜©ğ˜ªğ˜µğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜°ğ˜¯ğ˜­ğ˜º 10-15% ğ˜°ğ˜§ ğ˜µğ˜©ğ˜¦ ğ˜µğ˜©ğ˜¦ğ˜°ğ˜³ğ˜¦ğ˜µğ˜ªğ˜¤ğ˜¢ğ˜­ ğ˜±ğ˜¦ğ˜¢ğ˜¬ ğ˜ğ˜“ğ˜–ğ˜—ğ˜š. ğ˜ ğ˜°ğ˜¶ğ˜³ ğ˜®ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º ğ˜¢ğ˜¤ğ˜¤ğ˜¦ğ˜´ğ˜´ğ˜¦ğ˜´ ğ˜¢ğ˜³ğ˜¦ ğ˜¤ğ˜°ğ˜¢ğ˜­ğ˜¦ğ˜´ğ˜¤ğ˜¦ğ˜¥. ğ˜ğ˜©ğ˜¢ğ˜µ ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜´ğ˜µ ğ˜­ğ˜ªğ˜¬ğ˜¦ğ˜­ğ˜º ğ˜£ğ˜°ğ˜µğ˜µğ˜­ğ˜¦ğ˜¯ğ˜¦ğ˜¤ğ˜¬?"

ğŸ•¸ï¸ 90% of candidates walk right into the trap.

Their answer is: "ğ˜ğ˜µ ğ˜®ğ˜¶ğ˜´ğ˜µ ğ˜£ğ˜¦ ğ˜®ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º ğ˜£ğ˜¢ğ˜¯ğ˜¥ğ˜¸ğ˜ªğ˜¥ğ˜µğ˜©. ğ˜ğ˜¦ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ ğ˜µğ˜° ğ˜°ğ˜±ğ˜µğ˜ªğ˜®ğ˜ªğ˜»ğ˜¦ ğ˜¥ğ˜¢ğ˜µğ˜¢ ğ˜³ğ˜¦ğ˜¶ğ˜´ğ˜¦ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜®ğ˜°ğ˜³ğ˜¦ ğ˜šğ˜©ğ˜¢ğ˜³ğ˜¦ğ˜¥ ğ˜”ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º ğ˜µğ˜ªğ˜­ğ˜ªğ˜¯ğ˜¨."

It sounds like classical CUDA optimization. It's obsolete.

The Reality: They aren't using the dedicated hardware.

For modern architectures (Volta, Ampere, Hopper), the core bottleneck isn't the memory bandwidth or the standard CUDA cores, it's the failure to use the ğ—§ğ—²ğ—»ğ˜€ğ—¼ğ—¿ ğ—–ğ—¼ğ—¿ğ—²ğ˜€.

â€¢ Tensor Cores perform matrix multiply-accumulate operations (e.g., ğ™³ = ğ™° â‹… ğ™± + ğ™²) dramatically faster than standard FP32/FP16 CUDA Cores.

â€¢ If your kernel falls back to standard FP16 CUDA cores (e.g., due to misaligned data or non-standard dimensions), you are performing operations ğ˜´ğ˜¦ğ˜·ğ˜¦ğ˜¯ ğ˜µğ˜ªğ˜®ğ˜¦ğ˜´ slower than the Tensor Core units.
 

âœ… The Solution: You must align your operations to the Tensor Core architecture.

The senior-level solution is to use the correct programming model and enforce dimension alignment:

â€¢ ğ—”ğ—£ğ—œ ğ—¨ğ˜€ğ—®ğ—´ğ—²: The kernel must use the ğ—ªğ— ğ— ğ—” (ğ—ªğ—®ğ—¿ğ—½ ğ— ğ—®ğ˜ğ—¿ğ—¶ğ˜… ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—½ğ—¹ğ˜†-ğ—”ğ—°ğ—°ğ˜‚ğ—ºğ˜‚ğ—¹ğ—®ğ˜ğ—²) ğ—”ğ—£ğ—œ or be compiled via a framework (like cuBLAS or CUTLASS) that generates the underlying ğ—›ğ— ğ— ğ—” (ğ—›ğ—®ğ—¿ğ—±ğ˜„ğ—®ğ—¿ğ—² ğ— ğ—®ğ˜ğ—¿ğ—¶ğ˜… ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—½ğ—¹ğ˜†-ğ—”ğ—°ğ—°ğ˜‚ğ—ºğ˜‚ğ—¹ğ—®ğ˜ğ—²) instructions.
 
â€¢ ğ——ğ—¶ğ—ºğ—²ğ—»ğ˜€ğ—¶ğ—¼ğ—»ğ—¶ğ—»ğ—´: Crucially, your input matrix dimensions (M, N, K) must be tiled or padded to be multiples of the Tensor Core block sizes (e.g., 16Ã—16 or 8Ã—8 depending on architecture/precision).
 

âœï¸ ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±

"The bottleneck is inefficient hardware utilization: the kernel is failing to engage the ğ—§ğ—²ğ—»ğ˜€ğ—¼ğ—¿ ğ—–ğ—¼ğ—¿ğ—²ğ˜€. We are likely performing operations on standard CUDA cores. To reach peak performance, the GEMM must be implemented using the ğ—ªğ— ğ— ğ—” ğ—”ğ—£ğ—œ (or CUTLASS), ensuring the data is correctly laid out and tiled such that the dimensions (M, N, K) are multiples of the Tensor Core processing size."
