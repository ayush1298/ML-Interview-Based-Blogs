Youâ€™re in an AI Engineer interview at OpenAI. The interviewer asks:

â€œThe product team wants a 2x speedup on our Llama 3 70B endpoint, but theyâ€™ve forbidden any lossy techniques like quantization or pruning. How can you losslessly accelerate inference, and what core asymmetry in the Transformer are you exploiting?â€

Most candidates say: â€œWell, we could improve our batching strategy with vLLMâ€™s PagedAttention...â€.

Wrong approach. That improves overall ğ˜µğ˜©ğ˜³ğ˜°ğ˜¶ğ˜¨ğ˜©ğ˜±ğ˜¶ğ˜µ, but it doesnâ€™t make a ğ˜´ğ˜ªğ˜¯ğ˜¨ğ˜­ğ˜¦ userâ€™s generation 2x faster. Theyâ€™re stumped by the â€œlosslessâ€ constraint.

The reality is that autoregressive generation is painfully slow. Itâ€™s ğ¦ğğ¦ğ¨ğ«ğ²-ğ›ğšğ§ğğ°ğ¢ğğ­ğ¡-ğ›ğ¨ğ®ğ§ğ. For every single token, we have to read the ğ˜¦ğ˜¯ğ˜µğ˜ªğ˜³ğ˜¦ massive KV cache from HBM.

But inference isnâ€™t one process. Itâ€™s two:
- ğğ«ğğŸğ¢ğ¥ğ¥ (ğ•ğğ«ğ¢ğŸğ¢ğœğšğ­ğ¢ğ¨ğ§): Processing the prompt. This is fast, parallel, and compute-bound.
- ğ†ğğ§ğğ«ğšğ­ğ¢ğ¨ğ§ (ğƒğğœğ¨ğğ¢ğ§ğ ): Generating one token at a time. This is slow, sequential, and memory-bound.

The core asymmetry is this: ğ‚ğ¡ğğœğ¤ğ¢ğ§ğ  ğš ğ­ğ¨ğ¤ğğ§ ğ¢ğ¬ 10ğ± ğŸğšğ¬ğ­ğğ« ğ­ğ¡ğšğ§ ğ ğğ§ğğ«ğšğ­ğ¢ğ§ğ  ğš ğ­ğ¨ğ¤ğğ§.

So, we exploit this with ğ’ğ©ğğœğ®ğ¥ğšğ­ğ¢ğ¯ğ ğƒğğœğ¨ğğ¢ğ§ğ .

Hereâ€™s the plan:
- We use a small, lightning-fast â€œğğ«ğšğŸğ­ ğ¦ğ¨ğğğ¥â€ (e.g., a 1.5B model) to â€œguessâ€ the next 5-7 tokens. This is cheap.
- We then feed that entire 5-token chunk to our 70B â€œğ­ğšğ«ğ ğğ­ ğ¦ğ¨ğğğ¥â€ in a single forward pass.
- This single pass verifies all 5 tokens in parallel (using the fast prefill mode), which is dramatically faster than generating them 5 times sequentially.
- Using a clever rejection sampling algorithm, the 70B model accepts the tokens it would have generated anyway (say, the first 4) and only pays the slow generation cost for the one token it disagrees on.

We just generated 4-5 tokens for the price of ~1.5. Thatâ€™s your 2x lossless speedup.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:

â€œWe exploit the asymmetry between fast, compute-bound verification and slow, memory-bound generation. Weâ€™ll implement Speculative Decoding, using a small draft model to rapidly propose tokens and our 70B target model to losslessly verify them in a single, parallel batch.â€
