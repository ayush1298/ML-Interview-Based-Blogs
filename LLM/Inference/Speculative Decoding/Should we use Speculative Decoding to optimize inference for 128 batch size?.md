You're in a Machine Learning Engineer interview at OpenAI. The interviewer sets a trap:

"We need to optimize inference for batch size 128. Should we use Speculative Decoding?"

90% of candidates walk right into the trap.

They answer: "No. At batch size 128, the GPU is fully saturated (compute-bound). Running a draft model just adds overhead and kills throughput."

It sounds logical. It is standard textbook advice.
It is also completely wrong for modern workloads.

Here is the blind spot.
The candidates are assuming standard RAG (2k tokens). But in Long Context Inference (100k+ tokens), the bottleneck shifts violently.

The candidates aren't stalled by matrix multiplication, they are choking on memory bandwidth.

-----
ğ“ğ¡ğ ğ‘ğğšğ¥ğ¢ğ­ğ²:
- Model Weights are shared across the batch (loaded once).
- KV Cache is unique to every single request (loaded 128 times).

At 128k context, moving that massive KV cache from VRAM to the compute unit takes longer than the actual math. Your A100 isn't calculating; it's waiting for data.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: The Senior Engineer knows the fix: ğ’ğğ¥ğŸ-ğ’ğ©ğğœğ®ğ¥ğšğ­ğ¢ğ¯ğ ğŠğ• ğ‚ğ¨ğ¦ğ©ğ«ğğ¬ğ¬ğ¢ğ¨ğ§.

Instead of using a separate draft model, you use the same 70B model to speculate, but you only load the Top-1% of the KV cache.

Because you are memory-bound, fetching a tiny cache is instant. You speculate the next token cheaply, then verify with the full cache. Since the bottleneck was IO, you get the speedup without the compute penalty.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"For Long Context, high batch sizes remain memory-bound due to unique KV Caches. We enable self-speculation to bypass the I/O bottleneck, trading cheap compute for expensive memory bandwidth."
