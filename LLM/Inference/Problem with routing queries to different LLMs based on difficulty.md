You're in a Senior AI Engineer interview at Anthropic. The interviewer leans in and asks:

"We're bleeding money on inference. We want to build a ğŒğ¨ğğğ¥ ğ‚ğšğ¬ğœğšğğ (ğ…ğ«ğ®ğ ğšğ¥ğ†ğğ“) system, route easy queries to Llama-7B, and only send the hard stuff to GPT-4. What is the actual engineering bottleneck that makes this unreliable in production?"

Don't say: "The latency overhead of calling multiple models." (ğ˜ğ˜©ğ˜ªğ˜­ğ˜¦ ğ˜µğ˜³ğ˜¶ğ˜¦, ğ˜µğ˜©ğ˜ªğ˜´ ğ˜ªğ˜´ ğ˜¢ ğ˜´ğ˜°ğ˜­ğ˜·ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜ªğ˜¯ğ˜§ğ˜³ğ˜¢ğ˜´ğ˜µğ˜³ğ˜¶ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦ ğ˜±ğ˜³ğ˜°ğ˜£ğ˜­ğ˜¦ğ˜®. ğ˜ğ˜µ'ğ˜´ ğ˜¯ğ˜°ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜§ğ˜¶ğ˜¯ğ˜¥ğ˜¢ğ˜®ğ˜¦ğ˜¯ğ˜µğ˜¢ğ˜­ ğ˜§ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦.)

Also don't say: "Building a classifier to predict query difficulty." (ğ˜›ğ˜©ğ˜ªğ˜´ ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜«ğ˜¶ğ˜¯ğ˜ªğ˜°ğ˜³ ğ˜¢ğ˜±ğ˜±ğ˜³ğ˜°ğ˜¢ğ˜¤ğ˜©. ğ˜‹ğ˜ªğ˜§ğ˜§ğ˜ªğ˜¤ğ˜¶ğ˜­ğ˜µğ˜º ğ˜ªğ˜´ ğ˜´ğ˜¶ğ˜£ğ˜«ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜¢ğ˜¯ğ˜¥ ğ˜±ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µ-ğ˜¥ğ˜¦ğ˜±ğ˜¦ğ˜¯ğ˜¥ğ˜¦ğ˜¯ğ˜µ.)

In a cascade, your cheap model acts as the gatekeeper. Ideally, it should say, "I don't know this, ask the big guy." But if your cheap model is confidently wrong, your cascade breaks immediately. The user gets a hallucination, and the expensive model never even sees the prompt.

The real bottleneck is ğ‘ğğ¥ğ¢ğšğ›ğ¥ğ ğ‚ğ¨ğ§ğŸğ¢ğğğ§ğœğ ğ„ğ¬ğ­ğ¢ğ¦ğšğ­ğ¢ğ¨ğ§.

To build a production-grade cascade (like FrugalGPT), you are essentially solving a meta-problem:

1ï¸âƒ£ ğ˜Šğ˜¢ğ˜­ğ˜ªğ˜£ğ˜³ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ªğ˜´ ğ˜¸ğ˜¦ğ˜¢ğ˜¬: LLMs are notoriously overconfident. A log_prob of 0.99 often means nothing about factual accuracy.

2ï¸âƒ£ ğ˜—ğ˜³ğ˜°ğ˜¹ğ˜º ğ˜”ğ˜°ğ˜¥ğ˜¦ğ˜­ğ˜´ ğ˜¢ğ˜³ğ˜¦ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ğ˜¦ğ˜¥: You can't just trust the model's raw output score. You often need a lightweight "judge" or "verification" head just to decide if the first answer was garbage.

3ï¸âƒ£ ğ˜›ğ˜©ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜¥ğ˜¦-ğ˜°ğ˜§ğ˜§: If your verification step is too heavy, you burn all the compute savings you were trying to gain.

It's like trying to save money on a mechanic by asking a random stranger to fix your car first, but you have no way of knowing if they actually fixed it until the engine explodes.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"The bottleneck is failure detection. Without a calibrated confidence score or a lightweight verifier, we can't reliably distinguish easy from hard, meaning we either degrade user trust with bad cheap answers or waste money routing everything to the expensive model anyway."
