You're in a Senior AI Engineer interview at Meta and the interviewer asks:

"You're A/B testing two 70B models - one Multi-Head Attention (MHA), one Grouped Query Attention (GQA). Your colleague argues they'll have the same inference speed since FLOPs and parameter counts are identical. Is this assumption correct?"

Don't say: "Well, the FLOPs are similar, so maybe the GQA implementation has better kernel fusion or is just slightly more optimized..."

You just walked straight into the trap.

The reality is that inference is not ğœğ¨ğ¦ğ©ğ®ğ­ğ-ğ›ğ¨ğ®ğ§ğ; it is ğ¦ğğ¦ğ¨ğ«ğ²-ğ›ğ¨ğ®ğ§ğ.

Your colleague is making a classic mistake: ğ˜µğ˜©ğ˜¦ğ˜º'ğ˜³ğ˜¦ ğ˜µğ˜©ğ˜ªğ˜¯ğ˜¬ğ˜ªğ˜¯ğ˜¨ ğ˜¢ğ˜£ğ˜°ğ˜¶ğ˜µ ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨, ğ˜¸ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜ºğ˜°ğ˜¶ ğ˜±ğ˜³ğ˜°ğ˜¤ğ˜¦ğ˜´ğ˜´ ğ˜­ğ˜¢ğ˜³ğ˜¨ğ˜¦ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜¦ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜ğ˜“ğ˜–ğ˜—ğ˜´ ğ˜¢ğ˜³ğ˜¦ ğ˜¬ğ˜ªğ˜¯ğ˜¨.

ğ€ğ®ğ­ğ¨-ğ«ğğ ğ«ğğ¬ğ¬ğ¢ğ¯ğ ğğğœğ¨ğğ¢ğ§ğ  (generating text) is the complete opposite. It's a large matrix-vector operation, and the real bottleneck is the memory bandwidth required to read the KV Cache.

Here's the breakdown:
- For every single token you generate, the GPU must read the entire history of Keys (K) and Values (V) from high-bandwidth memory (HBM).
- ğŒğ‡ğ€ (ğŒğ®ğ¥ğ­ğ¢-ğ‡ğğšğ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§): Has a Key and Value head for every Query head (e.g., 64 Q, 64 K, 64 V). This makes the KV Cache massive.
- ğ†ğğ€ (ğ†ğ«ğ¨ğ®ğ©ğğ-ğğ®ğğ«ğ² ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§): Shares K/V heads across groups of Query heads (e.g., 64 Q, but only 8 K and 8 V).

This is big approach.

GQA drastically shrinks the size of the KV Cache, often by 4-8x. A smaller cache means less data to read from HBM on every single step.

You're no longer saturating your memory bandwidth. This isn't a 5% optimization; it's the core reason Llama 2/3 and Mistral models are so fast.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:

"FLOPs are irrelevant here. Inference performance is dictated by memory bandwidth, and the primary bottleneck is the KV Cache. GQA is faster because it directly shrinks the cache size by reducing K/V heads, dramatically cutting HBM read pressure during decoding."
