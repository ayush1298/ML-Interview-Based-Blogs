ğ™ğ™ğ™š ğ™†ğ™‘ ğ˜¾ğ™–ğ™˜ğ™ğ™š ğ™ğ™§ğ™–ğ™œğ™¢ğ™šğ™£ğ™©ğ™–ğ™©ğ™ğ™¤ğ™£ ğ™ğ™§ğ™–ğ™¥ ğŸ§±

You're in a Senior ML Interview at NVIDIA. The interviewer sets a trap:

"ğ˜ğ˜¦ ğ˜¢ğ˜³ğ˜¦ ğ˜¥ğ˜¦ğ˜±ğ˜­ğ˜°ğ˜ºğ˜ªğ˜¯ğ˜¨ ğ˜¢ 70ğ˜‰ ğ˜±ğ˜¢ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜µğ˜¦ğ˜³ ğ˜“ğ˜“ğ˜” (ğ˜“ğ˜­ğ˜¢ğ˜®ğ˜¢-2) ğ˜¢ğ˜¯ğ˜¥ ğ˜¶ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜’ğ˜¦ğ˜º-ğ˜ğ˜¢ğ˜­ğ˜¶ğ˜¦ (ğ˜’ğ˜) ğ˜Šğ˜¢ğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜° ğ˜¢ğ˜¤ğ˜¤ğ˜¦ğ˜­ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜¦ ğ˜¥ğ˜¦ğ˜¤ğ˜°ğ˜¥ğ˜ªğ˜¯ğ˜¨. ğ˜›ğ˜° ğ˜©ğ˜¢ğ˜¯ğ˜¥ğ˜­ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® 32,000 ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜´ğ˜ªğ˜»ğ˜¦, ğ˜¸ğ˜¦ ğ˜±ğ˜³ğ˜¦-ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¤ğ˜¢ğ˜µğ˜¦ ğ˜¢ ğ˜©ğ˜¶ğ˜¨ğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜ªğ˜¨ğ˜¶ğ˜°ğ˜¶ğ˜´ ğ˜ğ˜™ğ˜ˆğ˜” ğ˜£ğ˜­ğ˜°ğ˜¤ğ˜¬ ğ˜§ğ˜°ğ˜³ ğ˜µğ˜©ğ˜¦ ğ˜’ğ˜ ğ˜¤ğ˜¢ğ˜¤ğ˜©ğ˜¦ ğ˜°ğ˜§ ğ˜¦ğ˜·ğ˜¦ğ˜³ğ˜º ğ˜¤ğ˜°ğ˜¯ğ˜¤ğ˜¶ğ˜³ğ˜³ğ˜¦ğ˜¯ğ˜µ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µ. ğ˜ğ˜°ğ˜°ğ˜¥ ğ˜ªğ˜¥ğ˜¦ğ˜¢?"

ğŸ•¸ï¸ 90% of candidates walk right into the trap.

Their answer is: "ğ˜ ğ˜¦ğ˜´, ğ˜±ğ˜³ğ˜¦-ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¤ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¦ğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜¦ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜¢ğ˜¤ğ˜©ğ˜¦ ğ˜ªğ˜´ ğ˜¢ğ˜·ğ˜¢ğ˜ªğ˜­ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜ªğ˜®ğ˜®ğ˜¦ğ˜¥ğ˜ªğ˜¢ğ˜µğ˜¦ğ˜­ğ˜º, ğ˜³ğ˜¦ğ˜¥ğ˜¶ğ˜¤ğ˜ªğ˜¯ğ˜¨ ğ˜­ğ˜¢ğ˜µğ˜¦ğ˜¯ğ˜¤ğ˜º ğ˜¢ğ˜¯ğ˜¥ ğ˜¢ğ˜·ğ˜°ğ˜ªğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜³ğ˜¶ğ˜¯ğ˜µğ˜ªğ˜®ğ˜¦ ğ˜®ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¤ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜°ğ˜·ğ˜¦ğ˜³ğ˜©ğ˜¦ğ˜¢ğ˜¥."

It sounds conservative. It destroys your throughput.

âŒ The Reality: They aren't accounting for Internal Fragmentation.

The KV Cache is the largest memory consumer in LLM inference. Most requests are short (e.g., 500-1000 tokens) but are pre-allocated for the maximum 32,000 tokens.

- This massive pre-allocation means 80-90% of your VRAM is reserved but unused empty space, leading to severe ğ—¶ğ—»ğ˜ğ—²ğ—¿ğ—»ğ—®ğ—¹ ğ—³ğ—¿ğ—®ğ—´ğ—ºğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—».
 
- The GPU quickly runs out of ğ˜¢ğ˜·ğ˜¢ğ˜ªğ˜­ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜®ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º, severely limiting the number of requests you can batch concurrently, thus flatlining your GPU utilization and throughput.
 


âœ… The Solution: You must treat VRAM like an Operating System treats RAM.

The senior-level solution is ğ—£ğ—®ğ—´ğ—²ğ—±ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—».

- ğ——ğ˜†ğ—»ğ—®ğ—ºğ—¶ğ—° ğ—”ğ—¹ğ—¹ğ—¼ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»: PagedAttention breaks the KV cache into small, fixed-size blocks ("pages") that do not need to be contiguous in memory.
 
- ğ—•ğ—¹ğ—¼ğ—°ğ—¸ ğ—§ğ—®ğ—¯ğ—¹ğ—²: A mapping (like a page table in an OS) tracks where the logical blocks of a sequence are stored in physical memory.
 
- This approach allocates memory only on-demand, eliminates internal fragmentation, and uniquely enables ğ—¸ğ—²ğ˜†-ğ˜ƒğ—®ğ—¹ğ˜‚ğ—² ğ˜€ğ—µğ—®ğ—¿ğ—¶ğ—»ğ—´ across requests (e.g., for beam search or prompt-sharing).
 

âœï¸ ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±
"ğ˜—ğ˜³ğ˜¦-ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¤ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜ªğ˜¨ğ˜¶ğ˜°ğ˜¶ğ˜´ ğ˜’ğ˜ ğ˜¤ğ˜¢ğ˜¤ğ˜©ğ˜¦ ğ˜®ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º ğ˜³ğ˜¦ğ˜´ğ˜¶ğ˜­ğ˜µğ˜´ ğ˜ªğ˜¯ ğ˜¤ğ˜³ğ˜ªğ˜±ğ˜±ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜ªğ˜¯ğ˜µğ˜¦ğ˜³ğ˜¯ğ˜¢ğ˜­ ğ˜§ğ˜³ğ˜¢ğ˜¨ğ˜®ğ˜¦ğ˜¯ğ˜µğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯, ğ˜¸ğ˜¢ğ˜´ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜¶ğ˜± ğ˜µğ˜° 90% ğ˜°ğ˜§ ğ˜ğ˜™ğ˜ˆğ˜”. ğ˜›ğ˜©ğ˜¦ ğ˜±ğ˜³ğ˜°ğ˜¥ğ˜¶ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯-ğ˜¨ğ˜³ğ˜¢ğ˜¥ğ˜¦ ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ªğ˜´ ğ™‹ğ™–ğ™œğ™šğ™™ğ˜¼ğ™©ğ™©ğ™šğ™£ğ™©ğ™ğ™¤ğ™£. ğ˜‰ğ˜º ğ˜µğ˜³ğ˜¦ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ ğ˜’ğ˜ ğ˜¤ğ˜¢ğ˜¤ğ˜©ğ˜¦ ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜·ğ˜ªğ˜³ğ˜µğ˜¶ğ˜¢ğ˜­ ğ˜®ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º, ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¤ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜¯ğ˜°ğ˜¯-ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜ªğ˜¨ğ˜¶ğ˜°ğ˜¶ğ˜´ ğ˜£ğ˜­ğ˜°ğ˜¤ğ˜¬ğ˜´ ğ˜°ğ˜¯ ğ˜¥ğ˜¦ğ˜®ğ˜¢ğ˜¯ğ˜¥, ğ˜¸ğ˜¦ ğ˜¦ğ˜­ğ˜ªğ˜®ğ˜ªğ˜¯ğ˜¢ğ˜µğ˜¦ ğ˜§ğ˜³ğ˜¢ğ˜¨ğ˜®ğ˜¦ğ˜¯ğ˜µğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯, ğ˜ªğ˜¯ğ˜¤ğ˜³ğ˜¦ğ˜¢ğ˜´ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜¦ğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜£ğ˜º 2-3ğ˜¹, ğ˜¢ğ˜¯ğ˜¥ ğ˜®ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜ªğ˜»ğ˜¦ ğ˜ğ˜—ğ˜œ ğ˜µğ˜©ğ˜³ğ˜°ğ˜¶ğ˜¨ğ˜©ğ˜±ğ˜¶ğ˜µ."
