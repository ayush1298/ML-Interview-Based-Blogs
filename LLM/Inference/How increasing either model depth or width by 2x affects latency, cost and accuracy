what happens when you scale **depth vs width** in a large language model (LLM).

---

# ðŸš€ **If you double WIDTH vs double DEPTH of an LLM, how does it affect:**

* **Latency**
* **Accuracy / Quality**
* **Training Stability**
* **Inference Cost**
* **Which is better?**

Letâ€™s break down both scaling directions.

---

# ðŸ§© **1. What is WIDTH vs DEPTH in a Transformer?**

### **WIDTH = hidden dimension (d_model), number of heads, FFN size**

Doubling width â†’

* d_model Ã—2
* FFN size Ã—2
* attention head dimension increases

### **DEPTH = number of transformer layers**

Doubling depth â†’

* blocks Ã—2
* residual stream becomes longer

---

# âš¡ 2. **Latency Impact**

## **Doubling WIDTH (O(dÂ²) cost)**

Latency increases *dramatically*.

Compute cost per layer:
[
O(d^2)
]
If you double d:
[
(2d)^2 = 4d^2
]
â†’ **4Ã— slower per layer**
â†’ latency roughly **4Ã— higher**

This is why large-width models (GPT-J, GPT-Neo) were expensive.

---

## **Doubling DEPTH**

Latency increases **linearly**.

More layers â†’ more sequential compute:
[
\text{latency} \propto L
]

Doubling depth â†’
**~2Ã— slower**, NOT 4Ã—

---

## **Latency Summary**

| Scaling      | Latency Increase |
| ------------ | ---------------- |
| **Width Ã—2** | âŒ ~4Ã— slower     |
| **Depth Ã—2** | âœ… ~2Ã— slower     |

ðŸ‘‰ **Depth scaling is far cheaper for latency.**

---

# ðŸŽ¯ 3. **Accuracy Impact**

Deep vs wide has been extensively studied (Kaplan scaling laws, Chinchilla, PaLM).

## **Increasing Depth â†’ improves reasoning, compositionality**

Deep networks:

* build hierarchical features
* better abstraction
* handle long-range dependencies
* benefit more from residual streams

High depth = better scaling of cross-entropy loss.

---

## **Increasing Width â†’ diminishing returns**

Large widths:

* saturate quickly
* mostly increase memorization
* do NOT significantly improve reasoning
* can harm stability

Empirically (PaLM, Llama, Chinchilla):

> **Past a certain point, more depth > more width for quality.**

---

## **Accuracy Summary**

| Scaling      | Accuracy Gain        |
| ------------ | -------------------- |
| **Width Ã—2** | âš ï¸ Small to moderate |
| **Depth Ã—2** | â­ Strong improvement |

---

# ðŸ’¸ 4. **Inference Cost (Memory + Compute)**

### **Width scaling is very expensive**

Weights scale:
[
O(d^2)
]
So doubling width â†’ nearly **4Ã— VRAM** needed.

### **Depth scaling is affordable**

Weights scale:
[
O(L d^2)
]
Increasing layers â†’ linear memory increase.

So doubling depth:
â†’ **~2Ã— VRAM**

---

## **Inference Cost Summary**

| Scaling      | VRAM Cost | FLOPs Cost |
| ------------ | --------- | ---------- |
| **Width Ã—2** | âŒ ~4Ã—     | âŒ ~4Ã—      |
| **Depth Ã—2** | âš ï¸ ~2Ã—    | âš ï¸ ~2Ã—     |

---

# ðŸ”¥ 5. **Training Stability**

### **Depth doubling**

* Historically unstable (exploding gradients)
* But **Pre-Norm + RMSNorm + warmup** solved it
* Modern LLMs scale depth to hundreds of layers safely

### **Width doubling**

* Generally stable (wide networks train easily)
* But require huge batch sizes and learning rate tuning

---

# ðŸ§  6. **Which is better overall?**

Modern LLM design converged to a rule:

> **To scale a model, increase depth, not width â€” until you hit optimal ratio.**

This is why:

* GPT-3: depth-heavy
* PaLM: depth-heavy
* LLaMA: depth-heavy
* Chinchilla: depth-heavy

Width hits diminishing returns quickly.
Depth continues to improve reasoning and quality.

---

# ðŸ”š Final Answer (Interview-Ready Summary)

**Doubling WIDTH (hidden size):**

* Latency â†’ 4Ã— slower
* VRAM â†’ 4Ã— more
* Accuracy â†’ small improvement
* Cost â†’ huge increase
* Stability â†’ fine

**Doubling DEPTH (layers):**

* Latency â†’ 2Ã— slower
* VRAM â†’ 2Ã— more
* Accuracy â†’ strong improvement
* Cost â†’ moderate increase
* Stability â†’ good with pre-norm

ðŸ‘‰ **Conclusion:**
**Depth scaling is almost always better than width scaling for large language models.**
It gives better accuracy, better scaling behavior, and lower latency/cost increase.
