ğ™ğ™ğ™š ğ™ğ™©ğ™–ğ™©ğ™ğ™˜ ğ˜½ğ™–ğ™©ğ™˜ğ™ğ™ğ™£ğ™œ ğ™ğ™©ğ™–ğ™¡ğ™¡ ğ™ğ™§ğ™–ğ™¥ ğŸ”

You're in a Systems Interview at OpenAI. The interviewer asks:

"ğ˜ğ˜¦ ğ˜¢ğ˜³ğ˜¦ ğ˜´ğ˜¦ğ˜³ğ˜·ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜¤ğ˜©ğ˜¢ğ˜µğ˜£ğ˜°ğ˜µ. ğ˜›ğ˜° ğ˜®ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜ªğ˜»ğ˜¦ ğ˜µğ˜©ğ˜³ğ˜°ğ˜¶ğ˜¨ğ˜©ğ˜±ğ˜¶ğ˜µ, ğ˜¸ğ˜¦ ğ˜¸ğ˜¢ğ˜ªğ˜µ ğ˜µğ˜° ğ˜¢ğ˜¤ğ˜¤ğ˜¶ğ˜®ğ˜¶ğ˜­ğ˜¢ğ˜µğ˜¦ ğ˜¢ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜°ğ˜§ 32 ğ˜¶ğ˜´ğ˜¦ğ˜³ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µğ˜´ ğ˜£ğ˜¦ğ˜§ğ˜°ğ˜³ğ˜¦ ğ˜³ğ˜¶ğ˜¯ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜ªğ˜¯ğ˜§ğ˜¦ğ˜³ğ˜¦ğ˜¯ğ˜¤ğ˜¦. ğ˜œğ˜´ğ˜¦ğ˜³ğ˜´ ğ˜¢ğ˜³ğ˜¦ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜­ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¢ğ˜£ğ˜°ğ˜¶ğ˜µ ğ˜³ğ˜¢ğ˜¯ğ˜¥ğ˜°ğ˜® ğ˜­ğ˜¢ğ˜µğ˜¦ğ˜¯ğ˜¤ğ˜º ğ˜´ğ˜±ğ˜ªğ˜¬ğ˜¦ğ˜´. ğ˜ğ˜©ğ˜º?"

ğŸ•¸ï¸ 90% of candidates walk right into the trap.

They say: "ğ˜›ğ˜©ğ˜¦ ğ˜²ğ˜¶ğ˜¦ğ˜¶ğ˜¦ ğ˜µğ˜ªğ˜®ğ˜¦ ğ˜ªğ˜´ ğ˜µğ˜°ğ˜° ğ˜­ğ˜°ğ˜¯ğ˜¨. ğ˜ğ˜¦ ğ˜´ğ˜©ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜³ğ˜¦ğ˜¥ğ˜¶ğ˜¤ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜µğ˜ªğ˜®ğ˜¦ğ˜°ğ˜¶ğ˜µ ğ˜°ğ˜³ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜¢ğ˜¹ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜µğ˜° 16."

This effectively admits defeat. You are sacrificing throughput to fix latency.

The Reality: They are suffering from ğ—§ğ—µğ—² ğ—¦ğ˜ğ—¿ğ—®ğ—´ğ—´ğ—¹ğ—²ğ—¿ ğ—˜ğ—³ğ—³ğ—²ğ—°ğ˜.

In a static batch, all requests must finish before the batch returns.

â€¢ Request A generates 5 tokens (short).
â€¢ Request B generates 500 tokens (long).

Request A finishes instantly, but the GPU sits idle (or masked out) for that slot while it waits for Request B to finish its 500th token. You are coupling the latency of your fastest user to the latency of your slowest user.

âœ… The Solution: You must implement Continuous (iteration-Level) Batching.

Instead of batching at the ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µ level, you batch at the ğ˜ªğ˜µğ˜¦ğ˜³ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ level.

â€¢ After every single token generation step, the scheduler checks if a request has finished.
â€¢ If Request A finishes, it is immediately evicted.
â€¢ The Insertion: A new Request C is immediately inserted into that empty slot ğ˜®ğ˜ªğ˜¥-ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯.
 

The GPU never waits. It is always fully saturated with active tokens.

âœï¸ ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¢ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±:
"ğ˜šğ˜µğ˜¢ğ˜µğ˜ªğ˜¤ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜¤ğ˜°ğ˜¶ğ˜±ğ˜­ğ˜¦ğ˜´ ğ˜´ğ˜©ğ˜°ğ˜³ğ˜µ ğ˜¢ğ˜¯ğ˜¥ ğ˜­ğ˜°ğ˜¯ğ˜¨ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µğ˜´, ğ˜¤ğ˜¢ğ˜¶ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜ªğ˜¥ğ˜­ğ˜¦ ğ˜µğ˜ªğ˜®ğ˜¦. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜´ğ˜¸ğ˜ªğ˜µğ˜¤ğ˜© ğ˜µğ˜° ğ˜Šğ˜°ğ˜¯ğ˜µğ˜ªğ˜¯ğ˜¶ğ˜°ğ˜¶ğ˜´ ğ˜‰ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ (ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜ªğ˜¯ ğ˜–ğ˜³ğ˜¤ğ˜¢ ğ˜°ğ˜³ ğ˜·ğ˜“ğ˜“ğ˜”). ğ˜‰ğ˜º ğ˜´ğ˜¤ğ˜©ğ˜¦ğ˜¥ğ˜¶ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜¢ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜ªğ˜µğ˜¦ğ˜³ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜­ğ˜¦ğ˜·ğ˜¦ğ˜­, ğ˜¸ğ˜¦ ğ˜¤ğ˜¢ğ˜¯ ğ˜¦ğ˜·ğ˜ªğ˜¤ğ˜µ ğ˜§ğ˜ªğ˜¯ğ˜ªğ˜´ğ˜©ğ˜¦ğ˜¥ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜ªğ˜¯ğ˜´ğ˜¦ğ˜³ğ˜µ ğ˜¯ğ˜¦ğ˜¸ ğ˜°ğ˜¯ğ˜¦ğ˜´ ğ˜ªğ˜¯ğ˜´ğ˜µğ˜¢ğ˜¯ğ˜µğ˜­ğ˜º, ğ˜¥ğ˜¦ğ˜¤ğ˜°ğ˜¶ğ˜±ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜¶ğ˜´ğ˜¦ğ˜³ ğ˜­ğ˜¢ğ˜µğ˜¦ğ˜¯ğ˜¤ğ˜º ğ˜§ğ˜³ğ˜°ğ˜® ğ˜µğ˜©ğ˜¦ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜©'ğ˜´ ğ˜­ğ˜°ğ˜¯ğ˜¨ğ˜¦ğ˜´ğ˜µ ğ˜´ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦."
