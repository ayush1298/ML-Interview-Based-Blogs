ğ™ğ™ğ™š "ğ™•ğ™šğ™§ğ™¤-ğ™‹ğ™–ğ™™ğ™™ğ™ğ™£ğ™œ" ğ˜¾ğ™¤ğ™¢ğ™¥ğ™ªğ™©ğ™š ğ™ğ™ğ™ğ™šğ™› ğŸ§±

You're in an LLM Optimization interview at Together AI. The interviewer asks:

"ğ˜ğ˜¦ ğ˜¢ğ˜³ğ˜¦ ğ˜´ğ˜¦ğ˜³ğ˜·ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜“ğ˜­ğ˜¢ğ˜®ğ˜¢-3 ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­. ğ˜–ğ˜¶ğ˜³ ğ˜ªğ˜¯ğ˜¤ğ˜°ğ˜®ğ˜ªğ˜¯ğ˜¨ ğ˜¶ğ˜´ğ˜¦ğ˜³ ğ˜±ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µğ˜´ ğ˜·ğ˜¢ğ˜³ğ˜º ğ˜¸ğ˜ªğ˜­ğ˜¥ğ˜­ğ˜º ğ˜ªğ˜¯ ğ˜­ğ˜¦ğ˜¯ğ˜¨ğ˜µğ˜©, ğ˜´ğ˜°ğ˜®ğ˜¦ ğ˜¢ğ˜³ğ˜¦ 50 ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ğ˜´, ğ˜´ğ˜°ğ˜®ğ˜¦ ğ˜¢ğ˜³ğ˜¦ 4,000. ğ˜›ğ˜° ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜µğ˜©ğ˜¦ğ˜®, ğ˜¸ğ˜¦ ğ˜±ğ˜¢ğ˜¥ ğ˜¦ğ˜·ğ˜¦ğ˜³ğ˜º ğ˜´ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜ªğ˜¯ ğ˜µğ˜©ğ˜¦ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜µğ˜° ğ˜µğ˜©ğ˜¦ ğ˜­ğ˜¦ğ˜¯ğ˜¨ğ˜µğ˜© ğ˜°ğ˜§ ğ˜µğ˜©ğ˜¦ ğ˜­ğ˜°ğ˜¯ğ˜¨ğ˜¦ğ˜´ğ˜µ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µ. ğ˜–ğ˜¶ğ˜³ ğ˜ğ˜—ğ˜œ ğ˜¶ğ˜µğ˜ªğ˜­ğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ªğ˜´ ğ˜©ğ˜ªğ˜¨ğ˜©, ğ˜£ğ˜¶ğ˜µ ğ˜µğ˜©ğ˜³ğ˜°ğ˜¶ğ˜¨ğ˜©ğ˜±ğ˜¶ğ˜µ ğ˜ªğ˜´ ğ˜­ğ˜°ğ˜¸. ğ˜ğ˜©ğ˜º?"

ğŸ—£ï¸ ğŸµğŸ¬% ğ—¼ğ—³ ğ—°ğ—®ğ—»ğ—±ğ—¶ğ—±ğ—®ğ˜ğ—²ğ˜€ ğ˜„ğ—®ğ—¹ğ—¸ ğ—¿ğ—¶ğ—´ğ—µğ˜ ğ—¶ğ—»ğ˜ğ—¼ ğ˜ğ—µğ—² ğ˜ğ—¿ğ—®ğ—½.

They say: "ğ˜”ğ˜¢ğ˜ºğ˜£ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜ªğ˜´ ğ˜µğ˜°ğ˜° ğ˜´ğ˜®ğ˜¢ğ˜­ğ˜­? ğ˜–ğ˜³ ğ˜¸ğ˜¦ ğ˜¢ğ˜³ğ˜¦ ğ˜®ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º ğ˜£ğ˜¢ğ˜¯ğ˜¥ğ˜¸ğ˜ªğ˜¥ğ˜µğ˜© ğ˜£ğ˜°ğ˜¶ğ˜¯ğ˜¥?"

ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜†: ğ—¬ğ—¼ğ˜‚ ğ—®ğ—¿ğ—² ğ—°ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—¶ğ—»ğ—´ ğ—¼ğ—» ğ˜ğ—µğ—¶ğ—» ğ—®ğ—¶ğ—¿.

When you pad a 50-token request to match a 4,000-token neighbor:

â€¢ You fill 3,950 slots with "Zero" (Pad tokens).
 
â€¢ The GPU performs matrix multiplications on those zeros just as hard as it does on real data.
 
â€¢ ğŸ¯ğŸ¬% ğ˜ğ—¼ ğŸ±ğŸ¬% ğ—¼ğ—³ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—™ğ—Ÿğ—¢ğ—£ğ˜€ are wasted processing padding that will be masked out anyway.
 

âœ… ğ—§ğ—µğ—² ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—¦ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ—°ğ—² ğ—£ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´ (ğ—¼ğ—¿ ğ—©ğ—®ğ—¿ğ—¶ğ—®ğ—¯ğ—¹ğ—² ğ—Ÿğ—²ğ—»ğ—´ğ˜ğ—µ ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—»).

You must treat the batch as a 1D stream, not a 2D rectangle.

â€¢ ğ—£ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´: You concatenate multiple short sequences into a single "pack" that fits the max length. (e.g., Sequence A + Sequence B + Sequence C = 4,000 tokens).
 
â€¢ ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» ğ— ğ—®ğ˜€ğ—¸ğ—¶ğ—»ğ—´: You use a block-diagonal attention mask to ensure Sequence A cannot attend to Sequence B within the same pack.
 

The GPU sees one full dense vector. No padding. Zero waste.

âœï¸ ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±:

"ğ˜•ğ˜¢ğ˜ªğ˜·ğ˜¦ ğ˜±ğ˜¢ğ˜¥ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜¸ğ˜¢ğ˜´ğ˜µğ˜¦ğ˜´ ğ˜®ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜·ğ˜¦ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜¶ğ˜µğ˜¦ ğ˜°ğ˜¯ ğ˜»ğ˜¦ğ˜³ğ˜°ğ˜´. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜ªğ˜®ğ˜±ğ˜­ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µ ğ˜šğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜—ğ˜¢ğ˜¤ğ˜¬ğ˜ªğ˜¯ğ˜¨ (ğ˜”ğ˜¶ğ˜­ğ˜µğ˜ªğ˜±ğ˜¢ğ˜¤ğ˜¬). ğ˜ğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜¤ğ˜¢ğ˜µğ˜¦ğ˜¯ğ˜¢ğ˜µğ˜¦ ğ˜®ğ˜¶ğ˜­ğ˜µğ˜ªğ˜±ğ˜­ğ˜¦ ğ˜´ğ˜©ğ˜°ğ˜³ğ˜µ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µğ˜´ ğ˜ªğ˜¯ğ˜µğ˜° ğ˜¢ ğ˜´ğ˜ªğ˜¯ğ˜¨ğ˜­ğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜ªğ˜¯ğ˜¶ğ˜°ğ˜¶ğ˜´ ğ˜´ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜µğ˜° ğ˜§ğ˜ªğ˜­ğ˜­ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜¸ğ˜ªğ˜¯ğ˜¥ğ˜°ğ˜¸ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜­ğ˜¦ğ˜µğ˜¦ğ˜­ğ˜º. ğ˜‰ğ˜º ğ˜®ğ˜°ğ˜¥ğ˜ªğ˜§ğ˜ºğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ ğ˜¢ğ˜µğ˜µğ˜¦ğ˜¯ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜®ğ˜¢ğ˜´ğ˜¬ (ğ˜±ğ˜°ğ˜´ğ˜ªğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ğ˜‹ğ˜´) ğ˜µğ˜° ğ˜±ğ˜³ğ˜¦ğ˜·ğ˜¦ğ˜¯ğ˜µ ğ˜¤ğ˜³ğ˜°ğ˜´ğ˜´-ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¢ğ˜®ğ˜ªğ˜¯ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜£ğ˜¦ğ˜µğ˜¸ğ˜¦ğ˜¦ğ˜¯ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µğ˜´, ğ˜¸ğ˜¦ ğ˜¦ğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜ğ˜—ğ˜œ ğ˜ªğ˜´ ğ˜¢ğ˜­ğ˜¸ğ˜¢ğ˜ºğ˜´ ğ˜¤ğ˜³ğ˜¶ğ˜¯ğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜·ğ˜¢ğ˜­ğ˜ªğ˜¥ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ğ˜´, ğ˜±ğ˜°ğ˜µğ˜¦ğ˜¯ğ˜µğ˜ªğ˜¢ğ˜­ğ˜­ğ˜º ğ˜¥ğ˜°ğ˜¶ğ˜£ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜¦ğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜µğ˜©ğ˜³ğ˜°ğ˜¶ğ˜¨ğ˜©ğ˜±ğ˜¶ğ˜µ."
