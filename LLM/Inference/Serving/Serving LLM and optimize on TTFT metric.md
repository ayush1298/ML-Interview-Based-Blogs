"ğ—ªğ—² ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ˜€ğ—²ğ—¿ğ˜ƒğ—² ğ—Ÿğ—¹ğ—®ğ—ºğ—®-ğŸ¯-ğŸ³ğŸ¬ğ—• ğ˜ğ—¼ ğŸ±,ğŸ¬ğŸ¬ğŸ¬ ğ—°ğ—¼ğ—»ğ—°ğ˜‚ğ—¿ğ—¿ğ—²ğ—»ğ˜ ğ˜‚ğ˜€ğ—²ğ—¿ğ˜€. ğ—ªğ—² ğ—°ğ—®ğ—»ğ—»ğ—¼ğ˜ ğ—¹ğ—²ğ˜ ğ˜ğ—µğ—² ğ—§ğ—¶ğ—ºğ—²-ğ—§ğ—¼-ğ—™ğ—¶ğ—¿ğ˜€ğ˜-ğ—§ğ—¼ğ—¸ğ—²ğ—» (ğ—§ğ—§ğ—™ğ—§) ğ—²ğ˜…ğ—°ğ—²ğ—²ğ—± ğŸ®ğŸ¬ğŸ¬ğ—ºğ˜€." âŒ›

The scaling team says "Just add more GPUs." The finance team says "No."

ğŸ…°ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—”: ğ— ğ—®ğ˜…ğ—¶ğ—ºğ—¶ğ˜‡ğ—² ğ—•ğ—®ğ˜ğ—°ğ—µ ğ—¦ğ—¶ğ˜‡ğ—² We batch user requests together (Batch Size = 64) to saturate the A100s. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: The "Bus Problem." Fast requests get stuck waiting for slow requests to finish generation. Latency spikes for everyone. TTFT fails.

ğŸ…±ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—•: ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—¶ğ—»ğ—´ / ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ğ—¶ğ˜€ğ—º Split the model across 4 GPUs to run faster. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: This helps a single user, but cuts total throughput. Youâ€™re using 4 GPUs to do the work of 1, just faster. It doesn't scale to 5,000 users without massive cost.

ğŸ”‘ ğ—§ğ—µğ—² "ğ—§ğ—µğ—¶ğ—¿ğ—± ğ——ğ—¼ğ—¼ğ—¿" ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—–ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ—•ğ—®ğ˜ğ—°ğ—µğ—¶ğ—»ğ—´ (ğ—œğ˜ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»-ğ—¹ğ—²ğ˜ƒğ—²ğ—¹ ğ—¦ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—¶ğ—»ğ—´) We abandon the idea of "The Batch" entirely. We use an engine like ğ˜ƒğ—Ÿğ—Ÿğ—  or ğ—¢ğ—¿ğ—°ğ—®.

1. When a request finishes ğ˜°ğ˜¯ğ˜¦ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯, we check if a new request has arrived.
 
2. If yes, we inject the new request into the batch ğ˜®ğ˜ªğ˜¥-ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯.
 
3. We manage memory with PagedAttention to ensure no fragmentation.
 

ğ—§ğ—µğ—² ğ—¢ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—²: The GPU never waits. We get the throughput of high-batching with the latency profile of single-stream inference.

ğŸ“– ğ—§ğ—µğ—² ğ—Ÿğ—²ğ˜€ğ˜€ğ—¼ğ—»: Static batching is for training. ğ——ğ˜†ğ—»ğ—®ğ—ºğ—¶ğ—° ğ˜€ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—¶ğ—»ğ—´ is for serving.
