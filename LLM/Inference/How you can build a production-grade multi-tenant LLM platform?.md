2 months ago â€¢ Edited â€¢ Visible to anyone on or off LinkedIn

ğŸ”¥ You're in a ML Engineer interview at Microsoft, and the interviewer asks:
"Your team manages enterprise LLM infrastructure for 15 engineering teams. You are getting frequent of budget overshoot and inefficient request throttling, How can you build a production-grade multi-tenant LLM platform ? "

ğŸš¨ ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º
Shared LLM infrastructure without governance = chaos: 
ğŸ’¸ No cost visibility â†’ Can't bill teams accurately
âš¡ Token free-for-all â†’ Critical apps throttled
ğŸ¯ Model sprawl â†’ GPT-4o for everything (even logs)
ğŸ“Š Zero accountability â†’ Teams overconsume freely

ğŸ—ï¸ ğ—§ğ—µğ—² ğŸ°-ğ—Ÿğ—®ğ˜†ğ—²ğ—¿ ğ—£ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¦ğ˜ğ—®ğ—°ğ—¸
ğŸ®ï¸âƒ£ ğ—¥ğ—®ğ˜ğ—² ğ—Ÿğ—¶ğ—ºğ—¶ğ˜ğ—¶ğ—»ğ—´: Token Quotas That Actually Work
In LLM systems, not all requests have equal impactâ€”a short prompt to a small model uses minimal resources while long queries to large models consume significant GPU time, requiring rate limits across multiple dimensions beyond just request count.

Multi-dimensional limits:

team_alpha:
 tokens_per_minute: 50K
 requests_per_second: 100
 concurrent_requests: 10
 monthly_quota: 10M tokens
Organizations configure limits tailored by functionâ€”for instance, a healthcare system may allow higher throughput for clinical documentation during peak hours. 

ğŸ®ï¸âƒ£ ğ—–ğ—µğ—®ğ—¿ğ—´ğ—²ğ—¯ğ—®ğ—°ğ—¸: Token-Level Cost Attribution
Platform teams implement chargeback by tagging every LLM request with metadata like customer_id, business_unit, or feature_name, enabling precise cost attribution across teams, features, and customers.

Production pattern:

llm_gateway.generate(
 prompt=query,
 metadata={
 "team_id": "data-science",
 "project": "recommender-v2",
 "cost_center": "CC-8472"
 }
)
Track metrics including tokens per request to normalize usage patterns, cost per user/team/feature for showback reporting, cache hit ratio to reveal spend savings, and requests routed to expensive models to shift non-essential traffic.


ğŸ¯ï¸âƒ£ ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—”ğ—°ğ—°ğ—²ğ˜€ğ˜€ ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹: Tier-Based Availability
Consider multi-tenant model access when there's no PII, cost efficiency is a priority, no stringent SLAs exist, and rate-limiting mechanisms are required for different tenants.

Deployment tiers:

Shared pool: GPT-3.5, Claude Haiku (all teams)
Premium tier: GPT-4o (approved teams, separate quota)
Experimental: o3-mini (ML team only, isolated budget)

ğŸ°ï¸âƒ£ ğ—¢ğ—¯ğ˜€ğ—²ğ—¿ğ˜ƒğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ˜†: Real-Time Usage Monitoring
Route all inference traffic through an observability-enabled gateway that captures comprehensive data including token usage, request latency, model selection, and cost attribution with metadata tags.

Alert thresholds:

Team approaching 80% monthly quota
Token consumption spike >3Ïƒ from baseline
429 throttling errors per team/model
Cost anomaly (team suddenly using premium models)

ğŸ’¬ The Interview Answer (30 seconds)
We deploy an LLM gateway with four governance layers:
ğŸ“Œ Rate limiting
ğŸ“Œ Chargeback
ğŸ“Œ Model Access Control
ğŸ“Œ Observability
