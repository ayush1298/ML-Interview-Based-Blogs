You're in an interview for a Senior ML Engineer role at Google DeepMind. The interviewer asks: 

"Our new reasoning model is great, but it uses a 2000 token Chain of Thought even for simple questions like 'What is 2+2?'. This is killing our inference budget. How do you fix this without sacrificing its ability to solve complex problems?"

Most candidates say: "I'd train two models, a small, fast one for simple queries and our large reasoning model for hard ones."

Wrong approach. Now you have a complex routing problem, double the hosting costs, and a maintenance nightmare.

The reality: You don't need two models. ğ˜ğ¨ğ® ğ§ğğğ ğ¨ğ§ğ ğ¦ğ¨ğğğ¥ ğ°ğ¢ğ­ğ¡ ğ­ğ°ğ¨ ğ¦ğ¨ğğğ¬.

Your reasoning model ğ˜¢ğ˜­ğ˜³ğ˜¦ğ˜¢ğ˜¥ğ˜º knows '2+2=4'. It's just been trained to always show its work, like an over-eager student. You just need to teach it when not to.

This isn't an RL problem. This is a production-level SFT problem.

The solution is "ğ“ğ¡ğ¢ğ§ğ¤ğ¢ğ§ğ  ğŒğ¨ğğ ğ…ğ®ğ¬ğ¢ğ¨ğ§":
Step 1. Take your final, fully-trained reasoning model.
Step 2. Create a new fine-tuning dataset with special instruction tags.
Step 3. For hard problems, format it: <ğ˜µğ˜©ğ˜ªğ˜¯ğ˜¬> [ğ˜˜ğ˜œğ˜Œğ˜šğ˜›ğ˜ğ˜–ğ˜•] <ğ˜Šğ˜°ğ˜›> ... [ğ˜ˆğ˜•ğ˜šğ˜ğ˜Œğ˜™]
Step 4. For simple problems, format it: <ğ˜¯ğ˜°_ğ˜µğ˜©ğ˜ªğ˜¯ğ˜¬> [ğ˜˜ğ˜œğ˜Œğ˜šğ˜›ğ˜ğ˜–ğ˜•] [ğ˜‹ğ˜ğ˜™ğ˜Œğ˜Šğ˜› ğ˜ˆğ˜•ğ˜šğ˜ğ˜Œğ˜™]

You fine-tune the ğ˜´ğ˜ªğ˜¯ğ˜¨ğ˜­ğ˜¦ model on this mixed dataset.

Now, your inference-time logic is trivial. For simple queries, just add the <ğ˜¯ğ˜°_ğ˜µğ˜©ğ˜ªğ˜¯ğ˜¬> tag to the prompt. The model will suppress its Chain of Thought and answer directly.

No routing, no extra VRAM, same set of weights.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ: 

"Don't build a multi-model routing system - build a single, controllable fusion model. Routing adds cost and complexity.
Fusion adds control and capability. With ğ“ğ¡ğ¢ğ§ğ¤ğ¢ğ§ğ  ğŒğ¨ğğ ğ…ğ®ğ¬ğ¢ğ¨ğ§ you teach one model to decide when to think and when to act.
