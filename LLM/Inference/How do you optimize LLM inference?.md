You're in an AI Infrastructure interview at OpenAI, and the interviewer asks:

"We need to serve GPT-4 to 10M users with <200ms TTFT. How do you optimize LLM inference?"

Here's how NOT to answer: "Just add more GPUs."

The 16 techniques that separate senior from staff engineers:

ð— ð—²ð—ºð—¼ð—¿ð˜† ð—§ð—¿ð—¶ð—°ð—¸ð˜€: 
ðŸ“Œ PagedAttention: Treat KV cache like OS paging (16-token blocks). No fragmentation = 2-3x throughput (https://lnkd.in/g2tfDYPF)

ðŸ“Œ KV Quantization: FP8 cache vs FP16 = 2x capacity, <1% quality loss (https://lnkd.in/g2tfDYPF)

ðŸ“Œ Memory Offload: CPU stores cold KVs, GPU hot path stays fast

ð—•ð—®ð˜ð—°ð—µð—¶ð—»ð—´ ð— ð—®ð—´ð—¶ð—°: 
ðŸ“Œ Continuous Batching: Add new requests mid-generation (vLLM's secret weapon) 

ðŸ“Œ Dynamic Batching: Adjust batch size real-time based on queue depth 

ðŸ“Œ Request Coalescing: Merge similar prefixes before processing

ð—¦ð—½ð—²ð—²ð—± ð—›ð—®ð—°ð—¸ð˜€: 
ðŸ“Œ Speculative Decoding: Draft model proposes 5 tokens, big model validates in 1 pass. 2-3x faster decode 

ðŸ“Œ CUDA Graphs: Pre-record GPU ops, replay = zero kernel launch overhead 

ðŸ“Œ FP8 Kernels: Half the bandwidth, 2x faster matmuls

ð—¦ð—ºð—®ð—¿ð˜ ð—¦ð—°ð—µð—²ð—±ð˜‚ð—¹ð—¶ð—»ð—´: 
ðŸ“Œ Prefetch Pipelines: Load next layer weights while computing current 

ðŸ“Œ Asynchronous Prefill: Process prompts on separate GPUs from decode 

ðŸ“Œ GPU-CPU Overlap: Copy next batch while computing current

ð—”ð—±ð˜ƒð—®ð—»ð—°ð—²ð—±: 
ðŸ“Œ Token Parallelism: Generate multiple tokens per step 

ðŸ“Œ Context Streaming: Don't load full 128K context, stream as needed 

ðŸ“Œ Early Exit: Small queries exit at layer 12/40, save 70% compute

The brutal reality: Most teams only know batching. The 10x engineers stack 5+ techniques.

Interview killer answer: "I'd profile first. Memory-bound? PagedAttention + KV quant. Compute-bound? Speculative decode + FP8. Latency-critical? Disaggregate prefill/decode with async scheduling. Then continuous batching for throughput."
