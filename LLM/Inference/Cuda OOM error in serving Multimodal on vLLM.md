2 hours ago â€¢ Edited â€¢ Visible to anyone on or off LinkedIn


Youâ€™re in a systems / ML infra interview at a top GenAI startup.

The interviewer asks:

â€œYouâ€™re serving a Multimodal LLM on vLLM to analyze videos. 
Requests are sequential.
max_num_seqs = 1.
Yet after a few videos, you hit CUDA OOM.
Why?â€

Donâ€™t answer:
â€œKV cache is growing across requests.â€

That sounds right â€” but itâ€™s wrong.

â¸»

GPU memory in vLLM isnâ€™t one thing.
Itâ€™s two very different behaviors.

ğ…ğ¢ğ±ğğ ğŒğğ¦ğ¨ğ«ğ² (ğŠğ• ğ‚ğšğœğ¡ğ)
 â€¢ Preâ€‘allocated per GPU based on max_model_len
 â€¢ Reused across requests
 â€¢ Contents reset, memory not freed
 â€¢ Does not grow over time

ğƒğ²ğ§ğšğ¦ğ¢ğœ ğŒğğ¦ğ¨ğ«ğ² (ğ•ğ¢ğğğ¨ ğğ«ğğŸğ¢ğ¥ğ¥)
 â€¢ Video creates variableâ€‘size tensors
 â€¢ PyTorch keeps memory reserved
 â€¢ Alloc/free cycles cause fragmentation
 â€¢ Eventually no contiguous block exists â†’ OOM

Even with:
 â€¢ Sequential requests
 â€¢ max_num_seqs = 1
 â€¢ Tensor parallelism

Any single GPU can OOM â€” because memory isnâ€™t pooled.

â¸»

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ

â€œThe KV cache isnâ€™t accumulating.
The failure comes from memory fragmentation caused by variableâ€‘size video prefills combined with aggressive KV preâ€‘allocation.
The fix is reducing max_model_len, leaving GPU headroom, and configuring the CUDA allocator.â€
