The interview is for a GenAI Engineer role at Anthropic.

Interviewer: "Your prompt gives perfect answers during testing - but fails randomly in production. What’s wrong?"

You: “Ah, the prompt drift problem.
 Identical prompts can yield different outputs due to sampling (temperature/top-p) or shift entirely under paraphrased inputs."

Interviewer: "Meaning?"

You: "LLMs don't understand instructions - they predict them.
A single rephrased sentence, longer context, or slight temperature change can push the model into a different completion path.
What looks deterministic in a 10-example test collapses under real-world input diversity."

Interviewer: "So how do you fix it?"

You: Treat prompts like production code:
 1. Prompt templates - lock phrasing with {{placeholders}} for user input.
 2. Lock sampling - fix temperature=0, top_p=1 for reproducibility.
 3. System-level guardrails - e.g., "Always respond in valid JSON matching this schema: {{schema}}"
 4. Fuzz-test inputs - run 1k+ paraphrased variants pre-deploy.
 5. Delimiters + structure -> Prevents bleed and enforces parsing:
 """USER_INPUT: {{input}}"""
 """OUTPUT_FORMAT: {{schema}}"""
 
Interviewer: "So prompt reliability is more about engineering than creativity?"

You: "Exactly.
 Creative prompting gets you demos.
 Structured prompting gets you products."

Interviewer: "What’s your golden rule for prompt design?"

You: “Prompts are code.
 They need versioning, testing, and regression tracking - not vibes.
 If you can’t reproduce the output, you can’t trust it."

Interviewer: "So prompt drift is basically a reliability bug?"

You: "Yes - and fixing it turns GenAI from a prototype into a platform."
