**linear vs. sparse attention** â€” what they are, why they are used, and how they differ.

---

# ğŸ”µ **1. Standard / Quadratic Attention (Baseline)**

In normal Transformers:

<img width="362" height="78" alt="image" src="https://github.com/user-attachments/assets/18a1d39e-cd32-436b-bbf2-4f7a129838b0" />

* Q is LÃ—d
* K is LÃ—d
* So (QK^\top) is **L Ã— L** â†’ compares every token to every other token.

### â— Complexity:

[
O(L^2)
]

This becomes expensive when L (sequence length) is large.

---

# ğŸ”µ **2. Sparse Attention (Reduce number of connections)**

Sparse attention reduces cost by computing attention **only on selected positions**, not all L tokens.

### How it works:

Instead of attending to all tokens, a token attends only to:

* Local window (neighboring tokens)
* Strided tokens (every kth token)
* Global tokens (special tokens like CLS)
* Random subsets

This reduces number of Qâ€“K interactions.

### Example (BigBird):

* Local  (w neighbors)
* Global (g tokens)
* Random (r tokens)

Total cost:

[
O(L(w + r + g))
]

This is **sub-quadratic** â†’ often **O(L)** or **O(LâˆšL)** depending on implementation.

### Intuition:

Sparse attention = **skip most connections**.

### Benefits:

âœ” Works for long sequences
âœ” Keeps important global structure
âœ” Lower memory and compute than dense attention

---

# ğŸ”µ **3. Linear Attention (Kernelization trick)**

Linear attention makes the whole attention computation **mathematically linear** in L.

<img width="530" height="532" alt="image" src="https://github.com/user-attachments/assets/98a585e5-1776-4186-bbac-a9492423ff7c" />


### Intuition:

Linear attention = **reorder softmax + use kernel trick**
It avoids computing the big LÃ—L matrix entirely.

### Benefits:

âœ” True **O(L)** scaling
âœ” Much faster for long sequences
âœ” Lower memory usage

---

# ğŸ”µ **Sparse Attention vs Linear Attention â€” Comparison**

| Feature            | Sparse Attention                           | Linear Attention                        |
| ------------------ | ------------------------------------------ | --------------------------------------- |
| **Idea**           | Attend to fewer tokens                     | Reparameterize attention so itâ€™s linear |
| **Scaling**        | Sub-quadratic, often O(L)                  | Strict O(L)                             |
| **Accuracy**       | Usually high (keeps global/local patterns) | Sometimes lower (approximation error)   |
| **Memory**         | Lower than dense                           | Very low                                |
| **Used in**        | Longformer, BigBird                        | Performer, Linear Transformer           |
| **Works well for** | NLP, structured long text                  | Very long sequences, streaming          |

---

# ğŸ§  **Simple Analogy**

### **Sparse attention**

â€œYou donâ€™t talk to everyone in the room, only a few selected people.â€

### **Linear attention**

â€œYou compress conversations into one summary, then talk using that summary.â€

---

# ğŸ”¥ Final Summary **

* **Standard attention = O(LÂ²)** because every token attends to every other.
* **Sparse attention** reduces this by restricting which tokens attend to which â†’ sub-quadratic, often nearly O(L).
* **Linear attention** rewrites attention using kernel tricks so it becomes mathematically O(L) without forming the full attention matrix.

---

A good blog on Sparse attention in detail: https://medium.com/@vishal09vns/sparse-attention-dad17691478c
