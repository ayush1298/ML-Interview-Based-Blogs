**linear vs. sparse attention** â€” what they are, why they are used, and how they differ.

---

# ğŸ”µ **1. Standard / Quadratic Attention (Baseline)**

In normal Transformers:

[
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
]

* Q is LÃ—d
* K is LÃ—d
* So (QK^\top) is **L Ã— L** â†’ compares every token to every other token.

### â— Complexity:

[
O(L^2)
]

This becomes expensive when L (sequence length) is large.

---

# ğŸ”µ **2. Sparse Attention (Reduce number of connections)**

Sparse attention reduces cost by computing attention **only on selected positions**, not all L tokens.

### How it works:

Instead of attending to all tokens, a token attends only to:

* Local window (neighboring tokens)
* Strided tokens (every kth token)
* Global tokens (special tokens like CLS)
* Random subsets

This reduces number of Qâ€“K interactions.

### Example (BigBird):

* Local  (w neighbors)
* Global (g tokens)
* Random (r tokens)

Total cost:

[
O(L(w + r + g))
]

This is **sub-quadratic** â†’ often **O(L)** or **O(LâˆšL)** depending on implementation.

### Intuition:

Sparse attention = **skip most connections**.

### Benefits:

âœ” Works for long sequences
âœ” Keeps important global structure
âœ” Lower memory and compute than dense attention

---

# ğŸ”µ **3. Linear Attention (Kernelization trick)**

Linear attention makes the whole attention computation **mathematically linear** in L.

The trick:

Standard attention:
[
\text{softmax}(QK^\top)V
]

Key idea:
Use a **kernel function** Ï† such that:

[
\text{softmax}(QK^\top) \approx \phi(Q)\left(\phi(K)^\top V\right)
]

This lets you reorder the computation:

### **Compute KV aggregation first:**

[
S = \sum_{j=1}^{L} \phi(K_j) V_j \quad\ (\text{cost } = O(L))
]

### **Then compute output:**

[
\text{output}_i = \phi(Q_i) S  \quad(\text{cost } = O(L))
]

Total complexity:

[
O(Ld)
]

This is **fully linear** in sequence length.

### Intuition:

Linear attention = **reorder softmax + use kernel trick**
It avoids computing the big LÃ—L matrix entirely.

### Benefits:

âœ” True **O(L)** scaling
âœ” Much faster for long sequences
âœ” Lower memory usage

---

# ğŸ”µ **Sparse Attention vs Linear Attention â€” Comparison**

| Feature            | Sparse Attention                           | Linear Attention                        |
| ------------------ | ------------------------------------------ | --------------------------------------- |
| **Idea**           | Attend to fewer tokens                     | Reparameterize attention so itâ€™s linear |
| **Scaling**        | Sub-quadratic, often O(L)                  | Strict O(L)                             |
| **Accuracy**       | Usually high (keeps global/local patterns) | Sometimes lower (approximation error)   |
| **Memory**         | Lower than dense                           | Very low                                |
| **Used in**        | Longformer, BigBird                        | Performer, Linear Transformer           |
| **Works well for** | NLP, structured long text                  | Very long sequences, streaming          |

---

# ğŸ§  **Simple Analogy**

### **Sparse attention**

â€œYou donâ€™t talk to everyone in the room, only a few selected people.â€

### **Linear attention**

â€œYou compress conversations into one summary, then talk using that summary.â€

---

# ğŸ”¥ Final Summary (Interview-Ready)**

* **Standard attention = O(LÂ²)** because every token attends to every other.
* **Sparse attention** reduces this by restricting which tokens attend to which â†’ sub-quadratic, often nearly O(L).
* **Linear attention** rewrites attention using kernel tricks so it becomes mathematically O(L) without forming the full attention matrix.

---
