# âœ… **Why do we divide by âˆšdâ‚– instead of âˆšd?**

The attention score is:

[
\text{Attention}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
]

This scaling is needed for **numerical stability**.

---

# ðŸ”¥ **Intuition: Dot products grow with dimension**

Let:

[
Q, K \in \mathbb{R}^{d_k}
]

Assume each component is drawn from a normal distribution:

[
Q_i, K_i \sim \mathcal{N}(0,1)
]

Then the dot product:

[
Q \cdot K = \sum_{i=1}^{d_k} Q_i K_i
]

Each product (Q_i K_i) has:

* mean = 0
* variance = 1

When you sum **dâ‚–** such terms, the variance becomes:

[
\mathrm{Var}(Q\cdot K) = d_k
]

Meaning:

ðŸ‘‰ **The dot product grows in magnitude proportional to dâ‚–**

So if:

* (dâ‚– = 64), scores are moderate
* (dâ‚– = 512), scores explode (8Ã— larger variance)

---

# ðŸš¨ What goes wrong without scaling?

The softmax will see very large positive and negative logits:

[
\exp(\text{big number}) \rightarrow \text{overflow}
]

This results in:

* **softmax becoming extremely peaky**
* almost **one token gets all the attention**
* **gradients explode**
* training becomes unstable

---

# â­ Solution: Normalize the dot product

We divide the dot product by:

[
\sqrt{d_k}
]

Why?

Because:

[
\mathrm{StdDev}(Q\cdot K) = \sqrt{d_k}
]

So dividing by âˆšdâ‚– **normalizes the variance back to 1**.

Then the softmax receives logits with controlled scale.

---

# ðŸ§  But why not âˆšd?

Because attention is computed **per head**, and each head works on vectors of size **dâ‚–**, NOT the model dimension d.

Even if the model hidden dimension is large, like:

* GPT-3 (d = 12,288)
* But with 96 heads â†’ dâ‚– = 128

The dot-product dimension is **128**, so scaling must match **that**.

---

# ðŸ” Summary Table

| Symbol | Meaning                      | Used For                         |
| ------ | ---------------------------- | -------------------------------- |
| **d**  | Full model dimension         | Embeddings, positional encodings |
| **dâ‚–** | Per-head query/key dimension | Attention score scaling          |

We scale by âˆšdâ‚– because:

* The dot product is computed over dâ‚– dimensions
* The variance grows as dâ‚–
* We must normalize using âˆšdâ‚–
* Scaling by âˆšd would under/over-correct depending on #heads

---

# ðŸŽ¯ **Interview-ready one-liner**

> We divide by âˆšdâ‚– because the dot product QÂ·K has variance dâ‚–, not d. Scaling by âˆšdâ‚– normalizes the distribution so softmax doesnâ€™t blow up. Attention is computed per-head, so the dimension that matters is the head dimension dâ‚–.
