# ‚úÖ **Why do we divide by ‚àöd‚Çñ instead of ‚àöd?**

The attention score is:

<img width="352" height="73" alt="image" src="https://github.com/user-attachments/assets/276d5d87-9b15-4259-82fd-fc8b1c711b76" />

This scaling is needed for **numerical stability**.

---

# üî• **Intuition: Dot products grow with dimension**

<img width="426" height="537" alt="image" src="https://github.com/user-attachments/assets/56ec43fb-9012-43c0-a88f-98249fc8c4f2" />


---

# üö® What goes wrong without scaling?

The softmax will see very large positive and negative logits:

exp(big number)‚Üíoverflow

This results in:

* **softmax becoming extremely peaky**
* almost **one token gets all the attention**
* **gradients explode**
* training becomes unstable

---

# ‚≠ê Solution: Normalize the dot product

We divide the dot product by:
 ‚àöd‚Çñ

Why?

Because:

StdDev(Q‚ãÖK)=  ‚àöd‚Çñ
So dividing by ‚àöd‚Çñ **normalizes the variance back to 1**.

Then the softmax receives logits with controlled scale.

---

# üß† But why not ‚àöd?

Because attention is computed **per head**, and each head works on vectors of size **d‚Çñ**, NOT the model dimension d.

Even if the model hidden dimension is large, like:

* GPT-3 (d = 12,288)
* But with 96 heads ‚Üí d‚Çñ = 128

The dot-product dimension is **128**, so scaling must match **that**.

---

# üîç Summary Table

| Symbol | Meaning                      | Used For                         |
| ------ | ---------------------------- | -------------------------------- |
| **d**  | Full model dimension         | Embeddings, positional encodings |
| **d‚Çñ** | Per-head query/key dimension | Attention score scaling          |

We scale by ‚àöd‚Çñ because:

* The dot product is computed over d‚Çñ dimensions
* The variance grows as d‚Çñ
* We must normalize using ‚àöd‚Çñ
* Scaling by ‚àöd would under/over-correct depending on #heads

---

# üéØ **Interview-ready one-liner**

> We divide by ‚àöd‚Çñ because the dot product Q¬∑K has variance d‚Çñ, not d. Scaling by ‚àöd‚Çñ normalizes the distribution so softmax doesn‚Äôt blow up. Attention is computed per-head, so the dimension that matters is the head dimension d‚Çñ.


Detail answer: 

Why does attention divide by ‚àöd‚Çñ and why does training fall apart without it

This small scaling factor is one of the most important stability choices in the transformer architecture.

In self-attention, the model computes a dot product between query and key vectors to measure relevance.

The dot product looks like this:
q ¬∑ k = q‚ÇÅk‚ÇÅ + q‚ÇÇk‚ÇÇ + ‚Ä¶ + q_d‚Çñk_d‚Çñ

Each attention head operates in a space of dimension d‚Çñ, which is the embedding dimension divided by the number of heads.

As d‚Çñ increases, the dot product becomes a sum of more terms.

Summing more terms naturally increases the magnitude of the result.

Adding 64 numbers produces smaller values than adding 512 numbers, even if each individual number follows the same distribution.

This means that larger d‚Çñ leads to larger attention scores.
These scores are then passed into the softmax function:
softmax(x·µ¢) = eÀ£·µ¢ / Œ£ eÀ£‚±º

When one attention score becomes much larger than the others, the softmax output collapses toward a one-hot distribution.

One token receives almost all the probability mass, while the rest receive almost none.

At this point, gradients begin to vanish.
The gradient of softmax is proportional to p(1 ‚àí p).
When p approaches 1 or 0, the gradient approaches zero.

This causes attention to behave like a hard selector instead of a smooth weighting mechanism, which severely limits learning and harms stability in deeper models.

Dividing the dot product by ‚àöd‚Çñ corrects this behavior.
The variance of the dot product grows with the dimension, and scaling by ‚àöd‚Çñ normalizes that growth.

 As a result, attention scores remain within a stable range.
This keeps the softmax distribution smooth, preserves useful gradients, and allows attention to remain expressive across many layers.

It is a small mathematical adjustment, but it plays a foundational role in making transformers trainable at scale.
