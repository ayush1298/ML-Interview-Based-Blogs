# âœ… **Why do we divide by âˆšdâ‚– instead of âˆšd?**

The attention score is:

[
\text{Attention}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
]

This scaling is needed for **numerical stability**.

---

# ðŸ”¥ **Intuition: Dot products grow with dimension**

<img width="426" height="537" alt="image" src="https://github.com/user-attachments/assets/56ec43fb-9012-43c0-a88f-98249fc8c4f2" />


---

# ðŸš¨ What goes wrong without scaling?

The softmax will see very large positive and negative logits:

exp(big number)â†’overflow

This results in:

* **softmax becoming extremely peaky**
* almost **one token gets all the attention**
* **gradients explode**
* training becomes unstable

---

# â­ Solution: Normalize the dot product

We divide the dot product by:
 âˆšdâ‚–

Why?

Because:

StdDev(Qâ‹…K)=  âˆšdâ‚–
So dividing by âˆšdâ‚– **normalizes the variance back to 1**.

Then the softmax receives logits with controlled scale.

---

# ðŸ§  But why not âˆšd?

Because attention is computed **per head**, and each head works on vectors of size **dâ‚–**, NOT the model dimension d.

Even if the model hidden dimension is large, like:

* GPT-3 (d = 12,288)
* But with 96 heads â†’ dâ‚– = 128

The dot-product dimension is **128**, so scaling must match **that**.

---

# ðŸ” Summary Table

| Symbol | Meaning                      | Used For                         |
| ------ | ---------------------------- | -------------------------------- |
| **d**  | Full model dimension         | Embeddings, positional encodings |
| **dâ‚–** | Per-head query/key dimension | Attention score scaling          |

We scale by âˆšdâ‚– because:

* The dot product is computed over dâ‚– dimensions
* The variance grows as dâ‚–
* We must normalize using âˆšdâ‚–
* Scaling by âˆšd would under/over-correct depending on #heads

---

# ðŸŽ¯ **Interview-ready one-liner**

> We divide by âˆšdâ‚– because the dot product QÂ·K has variance dâ‚–, not d. Scaling by âˆšdâ‚– normalizes the distribution so softmax doesnâ€™t blow up. Attention is computed per-head, so the dimension that matters is the head dimension dâ‚–.
