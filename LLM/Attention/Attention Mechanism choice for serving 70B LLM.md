You're in a ML Systems interview at Google, and the interviewer asks:

"We're serving a 70B LLM. Inference latency is killing us. Walk me through attention mechanism choices."

Here's how you answer:

A. Most candidates know "GQA reduces memory." That's kindergarten-level.

B. There are 4 brutal truths that separate senior from junior engineers.

ğŸ­. ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ ğ—•ğ—¼ğ˜ğ˜ğ—¹ğ—²ğ—»ğ—²ğ—°ğ—¸ - ğ—ğ—© ğ—–ğ—®ğ—°ğ—µğ—² ğ—¶ğ˜€ ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—˜ğ—»ğ—²ğ—ºğ˜†

Llama-2-13B at FP16:

1.3 MB per token

At 4K context? 5.2 GB per sequence

Batch size 32? 166 GB just for KV cache

Decoding is memory-bandwidth bound, not compute-bound. You spend more time moving data than doing math.

ğŸ®. ğ— ğ˜‚ğ—¹ğ˜ğ—¶-ğ—›ğ—²ğ—®ğ—± ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» (ğ— ğ—›ğ—”) - ğ—§ğ—µğ—² ğ—•ğ—®ğ˜€ğ—²ğ—¹ğ—¶ğ—»ğ—²

Each of n_heads gets independent K, V projections.

KV cache per token: 2 Ã— n_heads Ã— d_head

Perfect for training. Disaster for inference at scale.

ğŸ¯. ğ— ğ˜‚ğ—¹ğ˜ğ—¶-ğ—¤ğ˜‚ğ—²ğ—¿ğ˜† ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» (ğ— ğ—¤ğ—”) - ğ—§ğ—¼ğ—¼ ğ—”ğ—´ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ˜ƒğ—²

All query heads share ONE KV head.

KV cache: 2 Ã— 1 Ã— d_head (32Ã— reduction for 32-head attention)

The problem: Single KV head = massive information bottleneck. Quality degrades on complex reasoning.

Never use in production.

ğŸ°. ğ—šğ—¿ğ—¼ğ˜‚ğ—½ğ—²ğ—± ğ—¤ğ˜‚ğ—²ğ—¿ğ˜† ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» (ğ—šğ—¤ğ—”) - ğ—§ğ—µğ—² ğ—£ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¦ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—±

Why choose 1 or n_heads? Use n_groups heads.

Typical: 32 query heads â†’ 8 KV heads (4 queries share 1 KV)

The win:

4Ã— KV cache reduction vs MHA

95% quality retention

Llama 2/3, Mistral, Qwen2 all use GQA

Critical insight: GQA reduces memory bandwidth (moving data), NOT FLOPs (compute). Query matrix Q stays full size.

ğŸ±. ğ— ğ˜‚ğ—¹ğ˜ğ—¶-ğ—›ğ—²ğ—®ğ—± ğ—Ÿğ—®ğ˜ğ—²ğ—»ğ˜ ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» (ğ— ğ—Ÿğ—”) - ğ—§ğ—µğ—² ğ—™ğ˜‚ğ˜ğ˜‚ğ—¿ğ—²

Don't store K and V. Store compressed latent.

C^KV = compress(input) // d_c << n_heads Ã— d_head

Then: K, V = decompress(C^KV) at runtime

DeepSeek-V2: 93.3% KV cache reduction, 5.76Ã— throughput boost

Why not everywhere?

Custom CUDA kernels required

Implementation complexity

Lower throughput in some scenarios despite cache savings

The Decision Matrix

Use MHA: Training, models <7B, contexts <2K

Use GQA: Production (90% of cases), 7B-70B+ models, proven at scale

Use MLA: Ultra-long contexts (128K+), training from scratch, bleeding edge

ğ— ğ—¶ğ˜€ğ˜ğ—®ğ—¸ğ—²ğ˜€ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ ğ—¬ğ—¼ğ˜‚ ğ—¥ğ—²ğ—·ğ—²ğ—°ğ˜ğ—²ğ—±

âŒ "GQA is faster because less memory" - Explain bandwidth vs compute bottleneck
âŒ "MLA is always better" - Trade-offs: implementation complexity, lower throughput
âŒ "Use MQA for speed" - Quality degradation unacceptable at scale

âœ… Right answer: Identify bottleneck â†’ Quantify KV cache â†’ Choose based on model size, context length, quality needs, infrastructure

Final truth: Industry has spoken. GQA is production standard. MLA is future. MHA is training-only. MQA is a cautionary tale.

<img width="800" height="410" alt="image" src="https://github.com/user-attachments/assets/fc671ff3-af0b-4f34-8d1f-3196afc913b7" />
