The interview is for a Generative AI Engineer role at Cohere.

Interviewer: "Your client complains that the LLM keeps losing track of earlier details in a long chat. What's happening?"

You: "That's a classic context window problem.
 Every LLM has a fixed memory limit - say 8k, 32k, or 200k tokens.
 Once that's exceeded, earlier tokens get dropped or compressed, and the model literally forgets."

Interviewer: "So you just buy a bigger model?"

You: "You can, but that's like using a megaphone when you need a microphone.
A larger context window costs more, runs slower, and doesn't always reason better."

Interviewer: "Then how do you manage long-term memory?"

You:
 1. Summarization memory - periodically condense earlier chat segments into concise summaries.
 2. Vector memory - store older context as embeddings; retrieve only the relevant pieces later.
 3. Hybrid memory - combine summaries for continuity and retrieval for precision.

Interviewer: "So you’re basically simulating memory?"

You: "Yep.
LLMs are stateless by design.
You build memory on top of them - a retrieval layer that acts like long-term memory.
Otherwise, your chatbot becomes a goldfish."

Interviewer: "And how do you know if the memory strategy works?"

You: "When the system recalls context correctly without bloating cost or latency.
 If a user says, 'Remind me what I told you last week,' and it answers from stored embeddings - that’s memory done right."

Interviewer: "So context management isn’t a model issue - it’s an architecture issue?"

You: "Exactly.
 Most think 'context length' equals intelligence.
 But true intelligence is recall with relevance - not recall with redundancy."
