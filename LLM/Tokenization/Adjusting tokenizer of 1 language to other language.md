You're in a Senior Machine Learning Interview at Anthropic. The interviewer sets a trap:

"We need to adapt our English-centric LLM to support Arabic and Hebrew. How do you adjust the tokenizer?"

95% of candidates walk right into the trap.

They say: "I'll just retrain the BPE (Byte-Pair Encoding) tokenizer on the new multilingual corpus. BPE is standard, it breaks words into subwords (prefixes, roots, suffixes), so it will naturally learn the grammar."

This answer reveals they only understand ğ˜Šğ˜°ğ˜¯ğ˜¤ğ˜¢ğ˜µğ˜¦ğ˜¯ğ˜¢ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜­ğ˜¢ğ˜¯ğ˜¨ğ˜¶ğ˜¢ğ˜¨ğ˜¦ğ˜´ (like English).

English is easy. They add blocks to the ends of words:
play â†’ play + ing
help â†’ un + help + ful

The root ("play", "help") remains a solid, contiguous block. BPE loves this.

But Semitic languages (Arabic, Hebrew) use ğ˜ğ˜¯ğ˜§ğ˜ªğ˜¹ ğ˜”ğ˜°ğ˜³ğ˜±ğ˜©ğ˜°ğ˜­ğ˜°ğ˜¨ğ˜º. The root doesn't just get a suffix, it changes internally.

Think of the English irregularity "Goose" â†’ "Geese."
- If you BPE "Goose", you might get [Go, ose].
- If you BPE "Geese", you might get [Ge, ese].

The token IDs are totally disjoint. The model has to learn "Goose" and "Geese" as two completely unrelated concepts from scratch.

In Arabic, almost every word behaves like "Goose/Geese." A standard BPE tokenizer will shatter the semantic root into random, unrelated shards across different conjugations. We call this The ğ‘ğ¨ğ¨ğ­ ğ’ğ¡ğšğ­ğ­ğğ« ğğšğ«ğšğğ¨ğ±.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: You don't optimize for subword statistics; you optimize for ğ’ğğ¦ğšğ§ğ­ğ¢ğœ ğŒğ¨ğ«ğ©ğ¡ğ¨ğ¥ğ¨ğ ğ²..

To fix this, you propose ğƒğğœğ¨ğ¦ğ©ğ¨ğ¬ğ¢ğ­ğ¢ğ¨ğ§ğšğ¥ ğ“ğ¨ğ¤ğğ§ğ¢ğ³ğšğ­ğ¢ğ¨ğ§:

1ï¸âƒ£ ğ˜”ğ˜°ğ˜³ğ˜±ğ˜©ğ˜°ğ˜­ğ˜°ğ˜¨ğ˜ªğ˜¤ğ˜¢ğ˜­ ğ˜ˆğ˜¯ğ˜¢ğ˜­ğ˜ºğ˜»ğ˜¦ğ˜³ğ˜´: Use a pre-processing layer (like a morphological disambiguator) that separates the "Root" (usually 3 consonants) from the "Template."
- Input: "kitab" (book) â†’ [k-t-b, Pattern_A]
- Input: "kutub" (books) â†’ [k-t-b, Pattern_B]

2ï¸âƒ£ ğ˜Šğ˜©ğ˜¢ğ˜³ğ˜¢ğ˜¤ğ˜µğ˜¦ğ˜³-ğ˜ˆğ˜¸ğ˜¢ğ˜³ğ˜¦ ğ˜”ğ˜°ğ˜¥ğ˜¦ğ˜­ğ˜´: Instead of static subword embeddings, use a Character-CNN or a dedicated character-level encoder that builds the vector dynamically. This allows the model to "see" the k-t-b root pattern persist across variations.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ
"Standard BPE treats non-concatenative languages like random noise. I would implement a morphological decomposition step to ensure the embedding space shares parameters for the semantic root, rather than relearning every conjugation as a unique token."
