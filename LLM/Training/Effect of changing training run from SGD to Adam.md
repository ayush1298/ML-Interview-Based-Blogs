You are in a Senior Machine Learning Engineer interview at Google DeepMind and the interviewer asks:

"You just switched a 7B parameter training run from SGD to Adam to speed up convergence. The model size is identical, but the cluster immediately crashes with a ğ˜Šğ˜œğ˜‹ğ˜ˆ ğ˜–ğ˜¶ğ˜µ-ğ˜–ğ˜§-ğ˜”ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º (ğ˜–ğ˜–ğ˜”) error. Why?"

ğŸš« Don't say:
"Adam is computationally more expensive so it uses more memory," or "I probably need to lower the batch size."

That's a junior guess. It ignores the mechanics of the optimizer.

The reality is that Adam isn't just an algorithm, it's a VRAM glutton. The candidates fell into the ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğğ« ğ’ğ­ğšğ­ğ ğ“ğ«ğšğ©.

Unlike SGD, which is stateless, Adam maintains two additional scalar states for every single parameter in your network to track the learning trajectory:
- ğ˜”ğ˜°ğ˜®ğ˜¦ğ˜¯ğ˜µğ˜¶ğ˜® (ğ˜ğ˜ªğ˜³ğ˜´ğ˜µ ğ˜”ğ˜°ğ˜®ğ˜¦ğ˜¯ğ˜µ)
- ğ˜ğ˜¢ğ˜³ğ˜ªğ˜¢ğ˜¯ğ˜¤ğ˜¦ (ğ˜šğ˜¦ğ˜¤ğ˜°ğ˜¯ğ˜¥ ğ˜”ğ˜°ğ˜®ğ˜¦ğ˜¯ğ˜µ)

Here is the "3ğ± ğğšğ«ğšğ¦ğğ­ğğ« ğ‘ğ®ğ¥ğ" strictly for memory planning:
- ğ˜šğ˜ğ˜‹: ğ˜™ğ˜¦ğ˜²ğ˜¶ğ˜ªğ˜³ğ˜¦ğ˜´ ğ˜”ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º â‰ˆ ğ˜ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´ + ğ˜ğ˜³ğ˜¢ğ˜¥ğ˜ªğ˜¦ğ˜¯ğ˜µğ˜´.
- ğ˜ˆğ˜¥ğ˜¢ğ˜®: ğ˜™ğ˜¦ğ˜²ğ˜¶ğ˜ªğ˜³ğ˜¦ğ˜´ ğ˜”ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º â‰ˆ ğ˜ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´ + ğ˜ğ˜³ğ˜¢ğ˜¥ğ˜ªğ˜¦ğ˜¯ğ˜µğ˜´ + ğ˜”ğ˜°ğ˜®ğ˜¦ğ˜¯ğ˜µğ˜¶ğ˜® + ğ˜ğ˜¢ğ˜³ğ˜ªğ˜¢ğ˜¯ğ˜¤ğ˜¦.

For a 7B model in FP16 (2 bytes/param), your weights are ~14GB. But Adam demands an additional ~28GB just to store those optimizer states.

You didn't OOM because the model is too big. You OOM'd because you tripled your memory footprint without checking your hardware envelope.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
If you are VRAM-constrained, you don't abandon Adam. You use PagedAdam from bitsandbytes to offload those massive optimizer states to system RAM (CPU), fetching them only when needed during the update step.

ğŸ’¡ ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:
"Adam imposes a 3x memory penalty compared to pure model weights due to stored momentum and variance states. To fix the OOM without losing convergence speed, I would implement PagedAdam to offload the optimizer states to CPU memory, or re-architect the cluster capacity to account for the 3x state overhead."

Quick follow-up: Ever wonder why Adam's states are FP32 by default in mixed-precision? It's for stability, FP16 can cause underflow in variance estimates. But if you're bold, try fused Adam variants in Apex for potential savings

If PagedAdam's too slow (paging overhead ~10-20% perf hit), go for ZeroRedundancyOptimizer in DeepSpeed. Shards states across GPUs, no CPU offload needed. Perfect for cluster-scale
