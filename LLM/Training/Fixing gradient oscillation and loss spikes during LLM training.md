You're in a Senior ML Interview at Meta. The interviewer sets a trap:

"You're training a 7B parameter Llama-style model. In the first 1000 steps, your gradients start oscillating wildly and the loss spikes. How do you fix it?"

90% of candidates walk right into the trap.

Most candidates immediately answer:
"I would significantly lower the learning rate or add more ğ˜‰ğ˜¢ğ˜µğ˜¤ğ˜© ğ˜•ğ˜°ğ˜³ğ˜®ğ˜¢ğ˜­ğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜­ğ˜¢ğ˜ºğ˜¦ğ˜³ğ˜´."

It feels like the safe answer.

The interviewer checks a box marked "No Hire." Why? Because they just killed your training efficiency.

By "nuking" the learning rate (e.g., dropping from 3e-4 to 1e-5), they aren't solving the underlying geometry problem. They are just slowing the model's convergence to a crawl. They are wasting thousands of GPU hours to mask a symptom, not cure the disease.

The problem usually isn't that the direction of your gradient is wrong. The problem is that the step size is physically too large for the current curvature of the loss landscape.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: The Senior Engineer applies what I call The ğ”ğ§ğ¢ğ­ ğ‚ğ¥ğ¢ğ© ğŒğšğ§ğğ®ğ¯ğğ«.

Instead of touching the learning rate, you apply Gradient Clipping (specifically, clipping the Global L2 Norm to 1.0).

Here is the mechanical difference:
- ğ˜“ğ˜°ğ˜¸ğ˜¦ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜“ğ˜™: Shrinks the update vector indiscriminately.
- ğ˜ğ˜³ğ˜¢ğ˜¥ğ˜ªğ˜¦ğ˜¯ğ˜µ ğ˜Šğ˜­ğ˜ªğ˜±ğ˜±ğ˜ªğ˜¯ğ˜¨: Calculates the Norm (magnitude) of the gradient vector g. If ||g|| > 1.0, it rescales g to g/||g||.

This forces the gradient update to respect a maximum step size while strictly preserving the direction of the descent. You keep the velocity high where it's safe, but you install a "speed governor" for the cliffs where the gradients explode.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ
"I don't touch the learning rate yet. I apply Global Gradient Clipping (Norm = 1.0). This decouples the step size from the gradient magnitude, preventing exploding gradients without artificially stalling the model's convergence."
