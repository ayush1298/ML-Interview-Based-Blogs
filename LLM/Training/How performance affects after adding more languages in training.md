You're in a Senior ML Interview at Google DeepMind. The interviewer sets a trap:

"We have a 1B parameter Transformer that is SOTA on 10 languages. We wants to add 90 more languages to the training mix. What happens to our English benchmarks?"

90% of candidates walk right into the trap and say:

"It will improve! Adding 90 languages acts as massive data augmentation and regularization. The model learns universal grammar structures, so the original 10 languages will benefit from the transfer learning."

They just crashed the English production metrics. They aren't optimizing for ğ†ğğ§ğğ«ğšğ¥ ğˆğ§ğ­ğğ¥ğ¥ğ¢ğ ğğ§ğœğ, they are optimizing a zero-sum game of parameter allocation.

This is the ğ‚ğ®ğ«ğ¬ğ ğ¨ğŸ ğŒğ®ğ¥ğ­ğ¢ğ¥ğ¢ğ§ğ ğ®ğšğ¥ğ¢ğ­ğ².

With a fixed budget (1B params), the per-language capacity C decreases as the number of languages N increases.

While low-resource languages (e.g., ğ˜ ğ˜°ğ˜³ğ˜¶ğ˜£ğ˜¢) might see a boost from transfer, your high-resource languages (ğ˜Œğ˜¯ğ˜¨ğ˜­ğ˜ªğ˜´ğ˜©, ğ˜ğ˜³ğ˜¦ğ˜¯ğ˜¤ğ˜©, ğ˜Šğ˜©ğ˜ªğ˜¯ğ˜¦ğ˜´ğ˜¦) are now fighting for space in the same weights. They get diluted. The loss curve for your most profitable markets will flatline or regress.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: To solve this without 10x-ing your compute, you deploy ğ“ğ¡ğ ğğ¨ğ­ğ­ğ¥ğğ§ğğœğ¤ ğˆğ§ğ£ğğœğ­ğ¢ğ¨ğ§.

Instead of retraining the whole dense model or training 100 separate models:
1ï¸âƒ£ Freeze the shared 1B parameter backbone (it handles the universal syntax).
2ï¸âƒ£ Inject lightweight ğ€ğğšğ©ğ­ğğ« ğŒğ¨ğğ®ğ¥ğğ¬ for each language.

These are tiny bottleneck layers (Down-projection â†’ ReLU â†’ Up-projection) inserted between the frozen Transformer blocks.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"Don't dilute the weights. Freeze the backbone for shared transfer, and use ğ‹ğšğ§ğ ğ®ğšğ ğ-ğ’ğ©ğğœğ¢ğŸğ¢ğœ ğ€ğğšğ©ğ­ğğ«ğ¬ to reserve capacity for high-resource markets. We get the universal reach without the regression."
