ğ˜¿ğ™¤ğ™£'ğ™© ğ™ğ™˜ğ™–ğ™¡ğ™š ğ™©ğ™ğ™š ğ™ˆğ™¤ğ™™ğ™šğ™¡. ğ™ğ™˜ğ™–ğ™¡ğ™š ğ™©ğ™ğ™š ğ™ğ™ğ™¤ğ™ªğ™œğ™ğ™©. ğŸ’­

You're in a Research Scientist interview at Anthropic and the interviewer asks:

"ğ˜ğ˜¦ ğ˜©ğ˜¢ğ˜·ğ˜¦ ğ˜¢ ğ˜§ğ˜ªğ˜¹ğ˜¦ğ˜¥ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜¶ğ˜µğ˜¦ ğ˜£ğ˜¶ğ˜¥ğ˜¨ğ˜¦ğ˜µ. ğ˜ğ˜¦ ğ˜¸ğ˜¢ğ˜¯ğ˜µ ğ˜µğ˜° ğ˜®ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜ªğ˜»ğ˜¦ ğ˜°ğ˜¶ğ˜³ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­'ğ˜´ ğ˜±ğ˜¦ğ˜³ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¢ğ˜¯ğ˜¤ğ˜¦ ğ˜°ğ˜¯ ğ˜µğ˜©ğ˜¦ ğ˜”ğ˜ˆğ˜›ğ˜-500 ğ˜£ğ˜¦ğ˜¯ğ˜¤ğ˜©ğ˜®ğ˜¢ğ˜³ğ˜¬. ğ˜‹ğ˜° ğ˜¸ğ˜¦ ğ˜´ğ˜±ğ˜¦ğ˜¯ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜£ğ˜¶ğ˜¥ğ˜¨ğ˜¦ğ˜µ ğ˜°ğ˜¯ ğ˜±ğ˜³ğ˜¦-ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜­ğ˜¢ğ˜³ğ˜¨ğ˜¦ğ˜³ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­, ğ˜°ğ˜³ ğ˜°ğ˜¯ ğ˜¨ğ˜¢ğ˜µğ˜©ğ˜¦ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜®ğ˜°ğ˜³ğ˜¦ ğ˜©ğ˜ªğ˜¨ğ˜©-ğ˜²ğ˜¶ğ˜¢ğ˜­ğ˜ªğ˜µğ˜º ğ˜¥ğ˜¢ğ˜µğ˜¢?"

Most candidates say: "ğ˜‹ğ˜¢ğ˜µğ˜¢ ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜¯ğ˜¦ğ˜¸ ğ˜°ğ˜ªğ˜­. ğ˜'ğ˜¥ ğ˜§ğ˜°ğ˜­ğ˜­ğ˜°ğ˜¸ ğ˜µğ˜©ğ˜¦ ğ˜Šğ˜©ğ˜ªğ˜¯ğ˜¤ğ˜©ğ˜ªğ˜­ğ˜­ğ˜¢ ğ˜šğ˜¤ğ˜¢ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜“ğ˜¢ğ˜¸ğ˜´. ğ˜'ğ˜¥ ğ˜¬ğ˜¦ğ˜¦ğ˜± ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜§ğ˜ªğ˜¹ğ˜¦ğ˜¥ ğ˜¢ğ˜¯ğ˜¥ ğ˜´ğ˜±ğ˜¦ğ˜¯ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜£ğ˜¶ğ˜¥ğ˜¨ğ˜¦ğ˜µ ğ˜°ğ˜¯ ğ˜¤ğ˜¶ğ˜³ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜®ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜·ğ˜¦, ğ˜©ğ˜ªğ˜¨ğ˜©-ğ˜²ğ˜¶ğ˜¢ğ˜­ğ˜ªğ˜µğ˜º ğ˜¥ğ˜¢ğ˜µğ˜¢ğ˜´ğ˜¦ğ˜µ ğ˜µğ˜° ğ˜°ğ˜·ğ˜¦ğ˜³-ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­."

ğ—ªğ—¿ğ—¼ğ—»ğ—´. You just spent ğ˜®ğ˜ªğ˜­ğ˜­ğ˜ªğ˜°ğ˜¯ğ˜´ to hit a performance ceiling.

ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜†: 
The era of "Pre-training Scaling" is hitting diminishing returns. We are running out of high-quality tokens, and making models bigger yields smaller marginal gains.

You are missing the third axis of scaling: ğ—§ğ—²ğ˜€ğ˜-ğ—§ğ—¶ğ—ºğ—² ğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—².

System 1 vs. System 2 Thinking:

- Standard LLMs are ğ—¦ğ˜†ğ˜€ğ˜ğ—²ğ—º ğŸ­: They react instantly. They must output the next token immediately, even for a complex math proof.
 
- ğ—¦ğ˜†ğ˜€ğ˜ğ—²ğ—º ğŸ® (Reasoning): Humans don't answer math problems instantly. We think, we draft, we backtrack, we verify.
 

The Solution: ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—²-ğ—§ğ—¶ğ—ºğ—² ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´. 
Instead of a 70B parameter model that answers in 1 second, you use a smaller 7B model but give it the "budget" to generate 10,000 "thought tokens" before outputting the final answer. You use Reinforcement Learning (like DeepSeek-R1) to train the model ğ˜©ğ˜°ğ˜¸ to use this test-time compute effectively, learning to verify its own steps and backtrack when stuck.

ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±:

"ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ğ˜¯'ğ˜µ ğ˜£ğ˜­ğ˜ªğ˜¯ğ˜¥ğ˜­ğ˜º ğ˜´ğ˜¤ğ˜¢ğ˜­ğ˜¦ ğ˜±ğ˜³ğ˜¦-ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜­ğ˜¦ğ˜·ğ˜¦ğ˜³ğ˜¢ğ˜¨ğ˜¦ ğ™ğ™šğ™¨ğ™©-ğ™ğ™ğ™¢ğ™š ğ˜¾ğ™¤ğ™¢ğ™¥ğ™ªğ™©ğ™š ğ™ğ™˜ğ™–ğ™¡ğ™ğ™£ğ™œ. ğ˜'ğ˜¥ ğ˜´ğ˜©ğ˜ªğ˜§ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜¶ğ˜µğ˜¦ ğ˜£ğ˜¶ğ˜¥ğ˜¨ğ˜¦ğ˜µ ğ˜§ğ˜³ğ˜°ğ˜® ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜° ğ˜ªğ˜¯ğ˜§ğ˜¦ğ˜³ğ˜¦ğ˜¯ğ˜¤ğ˜¦, ğ˜ªğ˜®ğ˜±ğ˜­ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜¢ 'ğ˜™ğ˜¦ğ˜¢ğ˜´ğ˜°ğ˜¯ğ˜ªğ˜¯ğ˜¨' ğ˜±ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ (ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜™1 ğ˜°ğ˜³ ğ˜°1). ğ˜‰ğ˜º ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¸ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜µğ˜° ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜¦ ğ˜©ğ˜ªğ˜¥ğ˜¥ğ˜¦ğ˜¯ ğ˜Šğ˜©ğ˜¢ğ˜ªğ˜¯-ğ˜°ğ˜§-ğ˜›ğ˜©ğ˜°ğ˜¶ğ˜¨ğ˜©ğ˜µ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜¦ğ˜¹ğ˜±ğ˜­ğ˜°ğ˜³ğ˜¦ ğ˜®ğ˜¶ğ˜­ğ˜µğ˜ªğ˜±ğ˜­ğ˜¦ ğ˜±ğ˜¢ğ˜µğ˜©ğ˜´ ğ˜¢ğ˜µ ğ˜³ğ˜¶ğ˜¯ğ˜µğ˜ªğ˜®ğ˜¦, ğ˜¸ğ˜¦ ğ˜¤ğ˜¢ğ˜¯ ğ˜¢ğ˜¤ğ˜©ğ˜ªğ˜¦ğ˜·ğ˜¦ ğ˜šğ˜–ğ˜›ğ˜ˆ ğ˜®ğ˜¢ğ˜µğ˜© ğ˜±ğ˜¦ğ˜³ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¢ğ˜¯ğ˜¤ğ˜¦ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜¢ ğ˜§ğ˜³ğ˜¢ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜°ğ˜§ ğ˜µğ˜©ğ˜¦ ğ˜±ğ˜¢ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜µğ˜¦ğ˜³ ğ˜¤ğ˜°ğ˜¶ğ˜¯ğ˜µ."
