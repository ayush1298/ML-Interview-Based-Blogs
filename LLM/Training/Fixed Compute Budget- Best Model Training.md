You're in an AI Engineer interview at Anthropic and the interviewer asks:

"We have a fixed compute budget - 32 H100s for two weeks. To get the best possible model, should we train a larger model on less data, or a smaller model on more data? How do you begin to answer this?"

Most candidates freeze. They guess.

"Uh, I'd probably train a smaller model on more data? That seems safer."

Wrong. You're burning the entire budget on a gut feeling.

The reality: at this scale, you never guess. A wrong guess doesn't just waste time; it wastes hundreds of thousands of dollars in compute.

You don't answer with a choice. You answer with a methodology.

The answer is ğ’ğœğšğ¥ğ¢ğ§ğ  ğ‹ğšğ°ğ¬.

Running the full job is the last step, not the first. Your first step is to use a fraction of that budget to build a predictive model.

ğ‡ğğ«ğ'ğ¬ ğ­ğ¡ğ ğ¬ğğ§ğ¢ğ¨ğ«-ğ¥ğğ¯ğğ¥ ğšğ©ğ©ğ«ğ¨ğšğœğ¡:
- ğ˜‹ğ˜¦ğ˜§ğ˜ªğ˜¯ğ˜¦ ğ˜ ğ˜°ğ˜¶ğ˜³ ğ˜”ğ˜¦ğ˜µğ˜³ğ˜ªğ˜¤: "Best" is vague. Are we optimizing for perplexity? MMLU? Latency? Define the target first.
- ğ˜™ğ˜¶ğ˜¯ ğ˜šğ˜®ğ˜¢ğ˜­ğ˜­-ğ˜šğ˜¤ğ˜¢ğ˜­ğ˜¦ ğ˜Œğ˜¹ğ˜±ğ˜¦ğ˜³ğ˜ªğ˜®ğ˜¦ğ˜¯ğ˜µğ˜´: Use a small part of your budget (e.g., 4 GPUs for 1 day) to run a dozen small training runs. You vary two things: model size (e.g., 100M, 500M, 1B params) and data size (e.g., 2B, 10B, 20B tokens).
- ğ˜ğ˜ªğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜“ğ˜°ğ˜´ğ˜´ ğ˜Šğ˜¶ğ˜³ğ˜·ğ˜¦: You now have data. You plot these [params, tokens, final_loss] points and fit a power-law function to them. This formula predicts your final loss based on model size and data.
- ğ˜Œğ˜¹ğ˜µğ˜³ğ˜¢ğ˜±ğ˜°ğ˜­ğ˜¢ğ˜µğ˜¦ ğ˜µğ˜° ğ˜ğ˜ªğ˜¯ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜–ğ˜±ğ˜µğ˜ªğ˜®ğ˜¶ğ˜®: Now you use this formula to answer the interviewer's question. For your full compute budget, you can now predict the (N, D) pair (Model Size, Data Size) that will give you the lowest possible loss.

This is how you find the Chinchilla-optimal balance. You're not guessing; you're using a data-driven model to de-risk a massive engineering investment.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:

"I wouldn't guess. I'd use 10% of the budget to run a series of small-scale experiments, fit a scaling law to the loss, and then use that predictive model to find the optimal parameter and token allocation for the full 32-H100 run."
