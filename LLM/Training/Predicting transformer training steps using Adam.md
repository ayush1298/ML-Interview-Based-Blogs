You're in a Senior ML Interview at Google DeepMind. The interviewer hands you a marker and sets the trap:

"We are training a ğ˜›ğ˜³ğ˜¢ğ˜¯ğ˜´ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¦ğ˜³ from scratch using ğ˜ˆğ˜¥ğ˜¢ğ˜®. We set a constant Learning Rate of 1e-3. Predict the first 1000 steps."

90% of candidates walk right into the trap.

Most candidates say: "It converges. ğ˜ˆğ˜¥ğ˜¢ğ˜® is an adaptive optimizer, it adjusts per-parameter learning rates automatically. 1e-3 is a standard default. It might be noisy at first, but the loss will go down."

The reality? Your loss curve doesn't just oscillate, it explodes. You hit NaNs within 50 steps. You just wasted a cluster run.

Here is the physics of why: ğ˜›ğ˜³ğ˜¢ğ˜¯ğ˜´ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¦ğ˜³ğ˜´, unlike ğ˜™ğ˜¦ğ˜´ğ˜•ğ˜¦ğ˜µğ˜´, lack strong inductive biases at initialization. 
- At Step 0, the layers are completely misaligned.
- If you apply a full magnitude update (1e-3) on a randomly initialized Transformer, the gradients aren't just "large", they are structurally incoherent. 
- You push the parameters into a region of the loss landscape with such steep curvature that they can never recover.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: To pass the interview, you need to reference The ğ•ğšğ«ğ¢ğšğ§ğœğ ğ„ğ±ğ©ğ¥ğ¨ğ¬ğ¢ğ¨ğ§.

In the first few hundred steps of ğ˜›ğ˜³ğ˜¢ğ˜¯ğ˜´ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¦ğ˜³ training, the gradient variance is effectively infinite. To fix this, you cannot use a constant LR. You must implement a ğ‹ğ¢ğ§ğğšğ« ğ–ğšğ«ğ¦-ğ®ğ© ğ’ğœğ¡ğğğ®ğ¥ğ.
- ğ˜—ğ˜©ğ˜¢ğ˜´ğ˜¦ 1 (ğ˜ğ˜¢ğ˜³ğ˜®-ğ˜¶ğ˜±): Start LR at 0.0. Linearly increase it over the first ~4,000 steps. This allows the variance of the gradients to stabilize as the layers align.
- ğ˜—ğ˜©ğ˜¢ğ˜´ğ˜¦ 2 (ğ˜‹ğ˜¦ğ˜¤ğ˜¢ğ˜º): Once the "curvature" is stable, switch to an Inverse Square Root decay to converge.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"Transformers don't tolerate high learning rates at initialization due to high gradient variance. I would implement a linear warm-up from 0 to d_model^(-0.5) to let the curvature stabilize, preventing early divergence before switching to the decay schedule."
