You're in an AI Engineer interview at Google DeepMind and the interviewer asks: 

"Your 1B parameter proxy model trains perfectly with a 1.2e-4 learning rate. You scale the model to 70B, and the training immediately explodes. What's the most ğ˜­ğ˜ªğ˜¬ğ˜¦ğ˜­ğ˜º reason and how do you fix it ğ°ğ¢ğ­ğ¡ğ¨ğ®ğ­ running a new, expensive hyperparameter sweep?"

Most candidates say: "The model is too big, so the updates are unstable. I'd add gradient clipping and just keep lowering the learning rate manually until it's stable."

Wrong. That's a patch, not a solution. You're just masking the root cause and wasting millions in compute cycles trying to find a new LR.

The reality: This isn't a ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨ problem, it's a ğ˜±ğ˜¢ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜µğ˜¦ğ˜³ğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ problem.

You're seeing a classic failure of ğ’ğ­ğšğ§ğğšğ«ğ ğğšğ«ğšğ¦ğğ­ğğ«ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ (ğ’ğ).
In SP models, the optimal learning rate ğ˜´ğ˜©ğ˜ªğ˜§ğ˜µğ˜´ as you scale the model's width. The LR that was perfect for your 1B proxy is now catastrophically large for the 70B model because the update dynamics didn't scale uniformly with the parameters.

The fix is to use ğŒğšğ±ğ¢ğ¦ğ®ğ¦ ğ”ğ©ğğšğ­ğ ğğšğ«ğšğ¦ğğ­ğğ«ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ (ğŒğ”ğ).

MUP is ğ˜¯ğ˜°ğ˜µ just another initialization scheme. It's a set of rules that scales both the initializations AND the ğ˜±ğ˜¦ğ˜³-ğ˜­ğ˜¢ğ˜ºğ˜¦ğ˜³ ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜³ğ˜¢ğ˜µğ˜¦ğ˜´ (e.g., scaling them by 1/width ).

This re-parameterization does one magical thing: it makes the optimal hyperparameters, especially the learning rate, scale-invariant.

This means the optimal LR you found on your cheap 1B proxy directly transfers to your 70B monster. No new sweep needed.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ

"With Standard Parameterization, you're forced to find a new, unstable learning rate for every scale.
With MUP, you find the optimal LR once on a small proxy, and it remains optimal at any scale. You don't scale the LR; you build the model to fit the LR."
