You're in a Senior ML Interview at a OpenAI. The interviewer sets a trap:

"You just implemented a complex Transformer from a new paper. The code runs without errors. The training loop executes. But the loss curve is completely flat. What is your first move?"

90% of candidates walk right into the trap. 

Most candidates immediately jump to optimization.

They start listing hyperparameter fixes:
- "I'd lower the learning rate from 1e-3 to 1e-4."
- "I'd swap AdamW for SGD to stabilize convergence."
- "I'd double-check the data normalization stats."

They treat it like a tuning problem.

This answer fails because you are solving for performance when you should be solving for correctness.

In standard software engineering, a bug throws a ğ˜™ğ˜¶ğ˜¯ğ˜µğ˜ªğ˜®ğ˜¦ğ˜Œğ˜³ğ˜³ğ˜°ğ˜³ or a ğ˜šğ˜¦ğ˜¨ğ˜ğ˜¢ğ˜¶ğ˜­ğ˜µ. The code crashes.

In ML Systems, bugs are silent. A broken dataloader, a detached gradient graph, or a silent tensor shape mismatch won't stop the code from running. It just ensures the model learns nothing.

If you start tuning learning rates on a broken implementation, you are polishing a brick.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: The Senior Engineer knows the first step is ğ“ğ¡ğ ğ’ğ¢ğ§ğ ğ¥ğ-ğğšğ­ğœğ¡ ğğ¯ğğ«ğŸğ¢ğ­.

Before you touch a single hyperparameter, you must prove the architecture is capable of memorization.
- ğˆğ¬ğ¨ğ¥ğšğ­ğ: Take exactly ONE batch of data (e.g., 32 samples).
- ğ’ğ­ğ«ğ¢ğ©: Turn off all regularization (Dropout = 0.0, Weight Decay = 0.0, Data Augmentation = Off).
- ğ…ğ¨ğ«ğœğ: Train on that single batch for 1,000 epochs.

If the model implementation is correct, the loss should drive to absolute zero (0.00) and training accuracy should hit 100%. The model should perfectly memorize the inputs.

If it cannot "cheat" and memorize 32 samples, your code is fundamentally broken. No amount of hyperparameter tuning will fix a logic bug.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"I don't tune a model until I prove it can learn. I strip regularization and force the model to overfit a single batch. If it can't memorize the training data, it will never generalize to the test data."
