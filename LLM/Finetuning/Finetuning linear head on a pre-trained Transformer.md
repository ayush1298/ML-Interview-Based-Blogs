You're in a Senior ML Interview at NVIDIA. The interviewer sets a trap:

"You attach a new, random linear head to a pre-trained Transformer. Do you unfreeze all layers and start backprop immediately?"

90% of candidates walk right into the trap.

Their answer is: "Of course. End-to-end training allows the backbone to adapt to the new task immediately. If we have the compute, why artificially restrict the model by freezing layers?"

It feels efficient. It works in the tutorials.
-----
ğ“ğ¡ğ ğ‘ğğšğ¥ğ¢ğ­ğ²: They aren't accounting for ğ“ğ¡ğ ğ†ğ«ğšğğ¢ğğ§ğ­ ğ’ğ¡ğ¨ğœğ¤ğ°ğšğ¯ğ.

Their classification head is initialized with random noise. This means their initial loss is high, and their initial gradients are mathematical garbage.

When they backpropagate this "noise" through their perfectly pre-trained backbone, they aren't adapting the features. They are shattering them.

The backbone features (which were already robust) are forced to move aggressively to accommodate a random, untrained head. You destroy the pre-trained structure before the head even learns which way is up.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: You must respect the initialization gap. The fix is executing "ğ‹ğ¢ğ§ğğšğ« ğğ«ğ¨ğ›ğ¢ğ§ğ , ğ­ğ¡ğğ§ ğ…ğ¢ğ§ğ-ğ“ğ®ğ§ğ¢ğ§ğ " ğğ«ğ¨ğ­ğ¨ğœğ¨ğ¥

- ğ˜—ğ˜©ğ˜¢ğ˜´ğ˜¦ 1 (ğ˜›ğ˜©ğ˜¦ ğ˜šğ˜©ğ˜ªğ˜¦ğ˜­ğ˜¥): Freeze the backbone parameters. Train only the new linear head. Since the features are fixed, this is effectively a convex optimization problem. It converges rapidly.
- ğ˜—ğ˜©ğ˜¢ğ˜´ğ˜¦ 2 (ğ˜›ğ˜©ğ˜¦ ğ˜™ğ˜¦ğ˜§ğ˜ªğ˜¯ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µ): Once the head is sensible, unfreeze the backbone.
- ğ˜—ğ˜©ğ˜¢ğ˜´ğ˜¦ 3 (ğ˜›ğ˜©ğ˜¦ ğ˜‹ğ˜³ğ˜ªğ˜§ğ˜µ): Lower your learning rate (e.g., from 1e-3 to 1e-5) and fine-tune the full model to adapt the features slightly for the specific task.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ: "Never fine-tune a backbone against a random head. We must align the head to the existing feature space before we allow the feature space to shift."
