You're in a GenAI Engineer interview at Microsoft, and the interviewer asks:
"We need to build a model capable of complex legal reasoning and self-correction. Should we stick to Supervised Fine-Tuning (SFT) or implement Group Relative Policy Optimization (GRPO)? Justify your choice."

Here's how you can answer:

A. Most candidates fumble here because they say "SFT is standard, supervised, GRPO = RL-based.â€ Incomplete answer.

B. There are 5 critical factors every GenAI engineer should understand cold.


ğŸ­. ğ—§ğ—µğ—² ğ—¢ğ—¯ğ—·ğ—²ğ—°ğ˜ğ—¶ğ˜ƒğ—² ğ——ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² â€” ğ—ªğ—µğ—®ğ˜ ğ—½ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ğ—®ğ—¿ğ—² ğ˜†ğ—¼ğ˜‚ ğ—¥ğ—˜ğ—”ğ—Ÿğ—Ÿğ—¬ ğ˜€ğ—¼ğ—¹ğ˜ƒğ—¶ğ—»ğ—´?
Supervised Fine Tuning (SFT)

You are teaching the model what to say.
Great for:

 â€¢ Instruction following
 â€¢ Domain adaptation
 â€¢ Style alignment
 â€¢ Knowledge grounding

BUT: SFT reinforces imitation, not preference optimization.
It doesnâ€™t enforce long-horizon reasoning quality.

GRPO (Generative Reward Policy Optimization)

You are teaching the model why something is better.
It optimizes preference, reasoning quality, and long-context decision consistency.

Key truth:
SFT shapes behavior.
GRPO shapes judgment.


ğŸ®. ğ——ğ—®ğ˜ğ—® ğ—¥ğ—²ğ—¾ğ˜‚ğ—¶ğ—¿ğ—²ğ—ºğ—²ğ—»ğ˜ğ˜€ â€” ğ—§ğ—µğ—² ğ—½ğ—¼ğ—¶ğ—»ğ˜ ğ—»ğ—¼ğ—¯ğ—¼ğ—±ğ˜† ğ˜ğ—®ğ—¹ğ—¸ğ˜€ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜
SFT Data: Cheap, abundant, scalable

You can use:

 â€¢ Human-annotated Q/A
 â€¢ Conversation logs
 â€¢ Synthetic data from larger LLMs

GRPO Data: Expensive, specialized, high-quality

Requires preference pairs or reward models that actually capture quality.

Labeling cost: higher
Quality sensitivity: extreme


ğŸ¯. ğ—¦ğ˜ğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ˜† & ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹ â€” ğ—ªğ—µğ˜† ğ—²ğ—»ğ˜ğ—²ğ—¿ğ—½ğ—¿ğ—¶ğ˜€ğ—²ğ˜€ ğ—³ğ—²ğ—®ğ—¿ ğ—¥ğ—Ÿ-ğ—¯ğ—®ğ˜€ğ—²ğ—± ğ—ºğ—²ğ˜ğ—µğ—¼ğ—±ğ˜€
SFT

 â€¢ Stable
 â€¢ Deterministic
 â€¢ No mode collapse

Typical failure mode:
Model becomes too verbose or too polite.

GRPO

 â€¢ Can oscillate
 â€¢ Can over-optimize rewards
 â€¢ Can hallucinate confidently if reward is misaligned


4. ğ—§ğ—µğ—² ğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—˜ğ—°ğ—¼ğ—»ğ—¼ğ—ºğ—¶ğ—°ğ˜€ - ğ—§ğ—µğ—² ğ—µğ—¶ğ—±ğ—±ğ—²ğ—» ğ˜ğ—®ğ˜… ğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜„ğ—µğ—®ğ˜ ğ˜€ğ—²ğ—½ğ—®ğ—¿ğ—®ğ˜ğ—²ğ˜€ ğ—·ğ˜‚ğ—»ğ—¶ğ—¼ğ—¿ ğ—³ğ—¿ğ—¼ğ—º ğ˜€ğ—²ğ—»ğ—¶ğ—¼ğ—¿ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ˜€:

SFT: Linear compute cost. One forward/backward pass per token.

GRPO: Massive overhead. GRPO training throughput is 10x-20x lower than SFT

ğŸ±. ğ—§ğ—µğ—² ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜ ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜† - ğ—™ğ—¼ğ—¿ğ—ºğ—®ğ˜ ğ˜ƒğ˜€. ğ—Ÿğ—¼ğ—´ğ—¶ğ—°

SFT: Wins on strict formatting (JSON, SQL schemas, brand voice).

GRPO: Wins on open-ended problem solving, math, and coding where the "path" isn't fixed.

When to Use What â€” The production rulebook
When SFT WINS (use SFT):

âœ… Youâ€™re adapting a base model to a domain
âœ… You want consistent formatting or style
âœ… You need predictable, controllable outputs
âœ… Safety, stability, and low risk are critical


When GRPO WINS (use GRPO):

âœ… You need reasoning quality increase
âœ… Output quality is subjective or preference-based
âœ… Your pipeline tolerates RL instability
âœ… Youâ€™re optimizing business-critical KPIs
