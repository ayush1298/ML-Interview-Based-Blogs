You're in a Senior ML Engineer interview at Google DeepMind. The interviewer sets a quiet trap:

"You are implementing ğ‹ğ¨ğ‘ğ€ (ğ‹ğ¨ğ°-ğ‘ğšğ§ğ¤ ğ€ğğšğ©ğ­ğšğ­ğ¢ğ¨ğ§) from scratch. How do you initialize the down-projection matrix A and the up-projection matrix B?"

90% of candidates walk right into a brick wall.

Most candidates reflexively answer: "I'd use standard deep learning best practices. ğ—ğšğ¯ğ¢ğğ«/ğ†ğ¥ğ¨ğ«ğ¨ğ­ or ğŠğšğ¢ğ¦ğ¢ğ§ğ  initialization for both matrices to ensure stable variance and healthy gradients."

It feels like the safe, textbook answer.
In reality, you just broke the model before the first training step.

The candidate forgot what LoRA actually is. It isn't a new layer, it is a residual update.

The effective weight formula is:
W_new = W_frozen + (B x A)

If they initialize both A and B with random weights (ğ˜Ÿğ˜¢ğ˜·ğ˜ªğ˜¦ğ˜³/ğ˜’ğ˜¢ğ˜ªğ˜®ğ˜ªğ˜¯ğ˜¨), their product (B x A) becomes a matrix of random noise.

At Step 0, they are adding this random noise directly to your carefully pre-trained 70B parameter weights. 

They aren't fine-tuning, they are lobotomizing the model's existing knowledge with a "cold start" shock. 

The loss starts high, and the model has to spend the first few epochs just un-learning your initialization.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: To pass the interview, you need to prove you understand ğˆğğğ§ğ­ğ¢ğ­ğ² ğğ«ğğ¬ğğ«ğ¯ğšğ­ğ¢ğ¨ğ§.

The correct approach is asymmetric initialization:
- ğ˜ğ˜¯ğ˜ªğ˜µğ˜ªğ˜¢ğ˜­ğ˜ªğ˜»ğ˜¦ ğ˜”ğ˜¢ğ˜µğ˜³ğ˜ªğ˜¹ ğ˜ˆ (ğ˜‹ğ˜°ğ˜¸ğ˜¯-ğ˜—ğ˜³ğ˜°ğ˜«ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯): Use Random Gaussian. We need noise here to break symmetry and allow gradients to flow.
- ğ˜ğ˜¯ğ˜ªğ˜µğ˜ªğ˜¢ğ˜­ğ˜ªğ˜»ğ˜¦ ğ˜”ğ˜¢ğ˜µğ˜³ğ˜ªğ˜¹ ğ˜‰ (ğ˜œğ˜±-ğ˜—ğ˜³ğ˜°ğ˜«ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯): Use Exact Zeros.

Mathematically, because B is zero, the product (B x A) is Zero.

W_new = W_frozen + 0

At Step 0, your fine-tuned model is mathematically identical to the base model. You haven't degraded the pre-trained performance. As training begins, B slowly moves away from zero, introducing the adaptation gradually.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ
"We initialize the Up-projection to zero and the Down-projection to random. This ensures the adapter starts as an identity function, preserving the pre-trained weights at Step 0 while avoiding the gradient symmetry problem of setting both to zero."
