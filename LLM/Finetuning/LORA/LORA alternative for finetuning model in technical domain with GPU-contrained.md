You're in a Senior ML System Interview at Meta. The interviewer sets a trap:

"We need to adapt Llama-3 70B to the highly technical Medical domain. We are GPU-constrained, so we can't do full fine-tuning. How do we proceed?"

95% of candidates walk right into the trap.

Most candidates say...
"Easy. We use standard ğ‹ğ¨ğ‘ğ€ (ğ‹ğ¨ğ°-ğ‘ğšğ§ğ¤ ğ€ğğšğ©ğ­ğšğ­ğ¢ğ¨ğ§). It freezes the backbone, injects low-rank matrices, and saves us 70%+ VRAM. Itâ€™s the industry standard."

The interviewer nods, notes "ğ˜’ğ˜¯ğ˜°ğ˜¸ğ˜­ğ˜¦ğ˜¥ğ˜¨ğ˜¦ ğ˜ğ˜¢ğ˜±," and moves on.

Why? The candidate thinks they're teaching the model deep medical knowledge (ğ’ğ®ğ›ğ¬ğ­ğšğ§ğœğ), but standard LoRA mostly just teaches it to write like a doctor (ğ’ğ­ğ²ğ¥ğ).

Standard LoRA is fantastic for ğ˜ğ˜¯ğ˜´ğ˜µğ˜³ğ˜¶ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜›ğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨ (teaching a model to chat, summarize, or follow rules). But for ğ˜Šğ˜°ğ˜¯ğ˜µğ˜ªğ˜¯ğ˜¶ğ˜¢ğ˜­ ğ˜—ğ˜³ğ˜¦-ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ (injecting new, complex domain knowledge like oncology or case law), vanilla LoRA fails.

Recent research ("LoRA Learns Less and Forgets Less - 
arXiv:2405.09673 ") proves that standard LoRA updates are mathematically insufficient to capture high-rank data shifts. The model doesn't learn the medicine, it just mimics the doctor's tone.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
To match Full Fine-Tuning performance on a LoRA budget, you need to implement ğ“ğ¡ğ ğ‡ğ²ğ©ğğ«-ğ€ğğšğ©ğ­ğšğ­ğ¢ğ¨ğ§ ğ“ğ«ğ¢ğŸğğœğ­ğš.

You don't just "ğ˜ªğ˜®ğ˜±ğ˜°ğ˜³ğ˜µ ğ˜±ğ˜¦ğ˜§ğ˜µ." You engineer the adaptation:

1. ğ˜™ğ˜¢ğ˜¯ğ˜¬-ğ˜šğ˜µğ˜¢ğ˜£ğ˜ªğ˜­ğ˜ªğ˜»ğ˜¦ğ˜¥ ğ˜“ğ˜°ğ˜™ğ˜ˆ (ğ˜™ğ˜š-ğ˜“ğ˜°ğ˜™ğ˜ˆ): Standard LoRA scales adapters by 1/r. This collapses learning as you increase rank. You must switch to scaling by 1/sqrt(r) to stabilize gradients at higher ranks (e.g., r=256).

2. ğ˜“ğ˜°ğ˜§ğ˜µğ˜˜ ğ˜ğ˜¯ğ˜ªğ˜µğ˜ªğ˜¢ğ˜­ğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯: Random initialization of adapters is inefficient. Use LoftQ to quantize the backbone and initialize adapters to minimize the approximation error immediately.

3. ğ˜‹ğ˜ªğ˜§ğ˜§ğ˜¦ğ˜³ğ˜¦ğ˜¯ğ˜µğ˜ªğ˜¢ğ˜­ ğ˜“ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜™ğ˜¢ğ˜µğ˜¦ğ˜´: Not all layers learn at the same speed. You must apply a lower LR to embedding layers (to retain vocabulary stability) and a higher LR to the projection layers.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"Standard LoRA is for changing behavior. The Trifecta is for acquiring knowledge. To match full fine-tuning performance on a budget, I don't increase compute; I increase mathematical efficiency using RS-LoRA and Differential Learning Rates."
