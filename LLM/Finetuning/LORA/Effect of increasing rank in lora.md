You're in a Senior ML Interview at OpenAI. The interviewer sets a trap:

"Our LoRA fine-tuning isn't capturing the domain complexity. We increased the rank ğ« from 8 to 256 to give the model more capacity. But the loss curve flatlined. Why?"

90% of candidates walk right into it.

They say: "It's overfitting. Rank 256 is too high for a ğ˜“ğ˜°ğ˜¸-ğ˜™ğ˜¢ğ˜¯ğ˜¬ adaptation. The model is just memorizing noise, so we should reduce r back to 16 or 32."

-----
ğ“ğ¡ğ ğ‘ğğšğ¥ğ¢ğ­ğ²: They aren't overfitting. You are suffocating the model.

In standard LoRA, the weight updates are scaled by a factor of Î±/ğ« . This works fine when r is small (8 or 16).

But as you scale ğ« to 256, that denominator gets massive. You are mathematically forcing your gradient updates toward zero. You added millions of parameters, but you prevented them from learning anything.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: You have encountered the ğ•ğšğ§ğ¢ğ¬ğ¡ğ¢ğ§ğ  ğ”ğ©ğğšğ­ğ ğğšğ«ğšğğ¨ğ±.

To fix this, you don't need less rank. You need ğ‘ğšğ§ğ¤-ğ’ğ­ğšğ›ğ¢ğ¥ğ¢ğ³ğğ ğ‹ğ¨ğ‘ğ€ (ğ«ğ¬ğ‹ğ¨ğ‘ğ€).
- ğ˜šğ˜µğ˜¢ğ˜¯ğ˜¥ğ˜¢ğ˜³ğ˜¥ ğ˜“ğ˜°ğ˜™ğ˜ˆ: Scales updates by Î±/r. As r grows, updates shrink.
- ğ˜³ğ˜´ğ˜“ğ˜°ğ˜™ğ˜ˆ: Scales updates by Î±/sqrt(r).

This simple square root change stabilizes the gradient norms across all rank sizes. It allows the adapter to actually utilize the extra capacity at r=256, often matching the performance of full fine-tuning.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"Standard LoRA scaling penalizes high ranks. I would switch to rsLoRA to correct the gradient collapse. This lets us scale r indefinitely to capture domain complexity without stalling convergence."
