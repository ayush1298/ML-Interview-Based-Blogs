3 weeks ago â€¢ Visible to anyone on or off LinkedIn

You're in a Machine Learning Architect interview at NVIDIA, and the interviewer asks:
"We need to scale our LLM to 100B+ parameters but keep inference costs reasonable. Should we use dense or Mixture of Experts architecture? Justify your choice."

Here's how you can answer:
A. Most candidates fumble here because they only know "MoE uses multiple experts." Incomplete answer.
B. There are 5 critical factors every ML architect should understand cold.

ğŸ­. ğ—§ğ—µğ—² ğ—¦ğ—½ğ—®ğ—¿ğ˜€ğ—² ğ˜ƒğ˜€ ğ——ğ—²ğ—»ğ˜€ğ—² ğ—§ğ—¿ğ—®ğ—±ğ—²ğ—¼ğ—³ğ—³ - ğ—§ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—²
Dense models activate ALL parameters:
70B model = 70B active parameters every forward pass
Predictable but expensive
Sparse MoE activates only a SUBSET:
Mixtral 8x7B = 56B total, only 14B active per token
4x fewer FLOPs during inference
The brutal truth? MoE gives you 70B-class quality with 14B-class inference cost.

ğŸ®. ğ—§ğ—µğ—² ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜† - ğ—ªğ—µğ—²ğ—¿ğ—² ğŸµğŸ¬% ğ—¼ğ—³ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ˜€ ğ—´ğ—¼ ğ˜„ğ—¿ğ—¼ğ—»ğ—´
Most people think "MoE = less memory."
Wrong move.
You need to LOAD all experts:
Mixtral 8x7B needs ~95GB VRAM (not 14GB)
All 8 experts must be resident
Active parameters â‰  loaded parameters
Real deployment? You need MORE VRAM than dense models but FASTER inference.

ğŸ¯. ğ—§ğ—µğ—² ğ—¥ğ—¼ğ˜‚ğ˜ğ—¶ğ—»ğ—´ ğ— ğ—²ğ—°ğ—µğ—®ğ—»ğ—¶ğ˜€ğ—º - ğ—§ğ—µğ—² ğ—µğ—¶ğ—±ğ—±ğ—²ğ—» ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜…ğ—¶ğ˜ğ˜†
Token Choice (Top-K):
 - Router selects K experts per token
 - Most common: K=2 (Mixtral, Deepseek)
 - Weighted combination of outputs
Expert Choice:
 - Experts select top N tokens
 - Fixed capacity per expert
 - Better load balancing
Soft MoE:
 - Weighted average of ALL experts
 - Better for vision models
 - Higher compute cost
The counterintuitive reality? Top-1 routing often matches Top-2 quality.

ğŸ°. ğ—§ğ—µğ—² ğ—Ÿğ—¼ğ—®ğ—± ğ—•ğ—®ğ—¹ğ—®ğ—»ğ—°ğ—¶ğ—»ğ—´ ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º - ğ—ªğ—µğ˜† ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—¶ğ˜€ ğ˜ğ—¿ğ—¶ğ—°ğ—¸ğ˜†
Without proper balancing:
Some experts get 80% of tokens
Other experts barely train
Routing collapse during training
Solution stack:
Auxiliary Loss: Penalizes uneven expert usage (weight: 0.01-0.1) Expert Capacity: Hard limit on tokens per expert Top-K with Noise: Prevents always picking same experts
Real production insight? Auxiliary loss weight is THE most important hyperparameter.

ğŸ±. ğ—§ğ—µğ—² ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜ ğ—–ğ—¼ğ˜€ğ˜ - ğ—§ğ—µğ—² ğ˜ğ—¿ğ˜‚ğ˜ğ—µ ğ—»ğ—¼ğ—¯ğ—¼ğ—±ğ˜† ğ˜ğ—®ğ—¹ğ—¸ğ˜€ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜
Dense 70B: 50 tokens/sec throughput Mixtral 8x7B: 120 tokens/sec throughput
But here's the hidden cost:
Expert parallelism needs high-bandwidth interconnect
All-to-all communication patterns
2-3x network traffic vs dense models

ğ—ªğ—µğ—²ğ—» ğ— ğ—¼ğ—˜ ğ˜„ğ—¶ğ—»ğ˜€:
âœ… Throughput bottlenecked (not memory) 
âœ… Need 70B+ quality on 20B budget 
âœ… Diverse task distribution 
âœ… Can afford larger VRAM

ğ—ªğ—µğ—²ğ—» ğ——ğ—²ğ—»ğ˜€ğ—² ğ˜„ğ—¶ğ—»ğ˜€:
âœ… Memory constrained deployments 
âœ… Homogeneous workloads 
âœ… Need predictable latency 
âœ… Smaller models (<30B)
