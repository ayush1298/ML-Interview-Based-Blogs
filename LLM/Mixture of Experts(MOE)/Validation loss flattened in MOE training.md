You're in a Senior ML Engineer interview at Google DeepMind and the interviewer asks: 

"You have just launched a new Mixture of Experts (MoE) training run. After a few thousand steps, you check the logs and see the validation loss has flatlined. What is the ğ¦ğ¨ğ¬ğ­ ğ¥ğ¢ğ¤ğğ¥ğ² ğœğšğ®ğ¬ğ specific to an MoE, and how do you fix it?"

Most candidates say: "My learning rate is too high," or "I have a bug in my data pipeline."

Wrong. Those are generic problems. They don't show you understand the ğ˜¶ğ˜¯ğ˜ªğ˜²ğ˜¶ğ˜¦ failure mode of an MoE.

The reality? ğ˜ğ¨ğ®ğ« ğ«ğ¨ğ®ğ­ğğ« ğ¡ğšğ¬ ğœğ¨ğ¥ğ¥ğšğ©ğ¬ğğ.

This is the classic MoE production trap. Here's what's ğ˜³ğ˜¦ğ˜¢ğ˜­ğ˜­ğ˜º happening:
- Early in training, the router "discovers" one or two experts that are slightly better than the others.
- It starts sending all the tokens to these "hero" experts.
- The other 62 experts get zero tokens. They become "dead" parameters - just sitting in VRAM, consuming memory, and learning nothing.
- You're paying the memory cost for a 500B model but getting the performance of a 50B one.

This isn't a team; it's a few overworked employees and a room full of people doing nothing.

ğ“ğ¡ğ ğŸğ¢ğ± ğ¢ğ¬ ğ­ğ¡ğ ğšğ®ğ±ğ¢ğ¥ğ¢ğšğ«ğ² ğ›ğšğ¥ğšğ§ğœğ¢ğ§ğ  ğ¥ğ¨ğ¬ğ¬.

This is the non-negotiable "tax" you must add to your main loss function. Itâ€™s a heuristic that explicitly penalizes the router for this imbalance.

It forces the router to spread the tokens around, even to "worse" experts, ensuring all experts are forced to learn and specialize. It's the key that prevents ğğ±ğ©ğğ«ğ­ ğ¬ğ­ğšğ«ğ¯ğšğ­ğ¢ğ¨ğ§.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:

"The most likely cause is ğ«ğ¨ğ®ğ­ğğ« ğœğ¨ğ¥ğ¥ğšğ©ğ¬ğ leading to expert starvation. I'd immediately check the expert utilization metrics. The fix is to tune the coefficient of the auxiliary balancing loss to force a more even token distribution, even if it slightly hurts the main loss in the short term."
