ğ™ğ™ğ™š ğ˜¾ğ™¤ğ™£ğ™©ğ™šğ™­ğ™© ğ™‹ğ™–ğ™§ğ™–ğ™¡ğ™¡ğ™šğ™¡ğ™ğ™¨ğ™¢ ğ™ğ™§ğ™–ğ™¥ ğŸ’

You're in a train-team interview at MosaicML (now Databricks). The interviewer puts a constraint on the whiteboard:

"ğ˜ğ˜¦ ğ˜¢ğ˜³ğ˜¦ ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¢ 100ğ˜‰ ğ˜±ğ˜¢ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜µğ˜¦ğ˜³ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜¢ ğŸ­ ğ™ˆğ™ğ™¡ğ™¡ğ™ğ™¤ğ™£ ğ™©ğ™¤ğ™ ğ™šğ™£ ğ™˜ğ™¤ğ™£ğ™©ğ™šğ™­ğ™© ğ™¬ğ™ğ™£ğ™™ğ™¤ğ™¬. ğ˜ğ˜¦ ğ˜©ğ˜¢ğ˜·ğ˜¦ ğ˜±ğ˜­ğ˜¦ğ˜¯ğ˜µğ˜º ğ˜°ğ˜§ ğ˜100ğ˜´. ğ˜ğ˜°ğ˜¸ ğ˜¥ğ˜° ğ˜ºğ˜°ğ˜¶ ğ˜´ğ˜©ğ˜¢ğ˜³ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜µğ˜° ğ˜©ğ˜¢ğ˜¯ğ˜¥ğ˜­ğ˜¦ ğ˜µğ˜©ğ˜ªğ˜´ ğ˜´ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜­ğ˜¦ğ˜¯ğ˜¨ğ˜µğ˜©?"

Most candidates immediately say: 
"Easy. We use ğ—™ğ—¹ğ—®ğ˜€ğ—µğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—»-ğŸ® to minimize memory usage, and we shard the model using ğ—§ğ—²ğ—»ğ˜€ğ—¼ğ—¿ ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ğ—¶ğ˜€ğ—º (ğ—§ğ—£) to split the weights across GPUs."

ğ—§ğ—µğ—² ğ—¶ğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ğ—²ğ—¿ ğ˜€ğ˜ğ—¼ğ—½ğ˜€ ğ˜†ğ—¼ğ˜‚. "ğ˜ ğ˜°ğ˜¶ ğ˜«ğ˜¶ğ˜´ğ˜µ ğ˜–ğ˜–ğ˜”'ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜­ğ˜¶ğ˜´ğ˜µğ˜¦ğ˜³." 

ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜†: 
Tensor Parallelism splits the ğ˜¸ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´ (MatMuls), but it duplicates the ğ˜¢ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ğ˜´ (or gathers them) across the sequence dimension in ways that don't help the KV cache enough.

ğŸ¦£ At 1M tokens, the ğ—ğ—© ğ—–ğ—®ğ—°ğ—µğ—² itself becomes the monster. 
Even with FlashAttention, the sheer size of the Key and Value matrices for a batch of sequences at length 1M is terabytes of data. It physically does not fit on a single GPU's HBM, even if you slice the weights.

You cannot fit the ğ˜´ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦ on one GPU. You need to split the ğ˜´ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜ªğ˜µğ˜´ğ˜¦ğ˜­ğ˜§.

ğ—§ğ—µğ—² ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—¥ğ—¶ğ—»ğ—´ ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» (ğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ğ—¶ğ˜€ğ—º). 
You don't just split the model depth (Pipeline Parallelism) or width (Tensor Parallelism). You split the ğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜.

â€¢ ğ—šğ—£ğ—¨ ğŸ­ holds tokens [0...100k]
â€¢ ğ—šğ—£ğ—¨ ğŸ® holds tokens [100k...200k]
â€¢ ...
 

ğ—•ğ˜‚ğ˜ ğ—µğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜ğ—µğ—² ğ˜ğ—¿ğ—¶ğ—°ğ—¸:
Attention is global. Token 999k needs to attend to Token 1. 
In a naive split, this requires massive communication. ğ—¥ğ—¶ğ—»ğ—´ ğ—”ğ˜ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—» solves this by passing blocks of Key/Value pairs around the GPUs in a ring during the forward and backward pass. You compute local attention, pass the KV block to your neighbor, compute again, and accumulate.

ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±: 
"ğ˜ğ˜­ğ˜¢ğ˜´ğ˜©ğ˜ˆğ˜µğ˜µğ˜¦ğ˜¯ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¯ğ˜¥ ğ˜›ğ˜— ğ˜¢ğ˜³ğ˜¦ğ˜¯'ğ˜µ ğ˜¦ğ˜¯ğ˜°ğ˜¶ğ˜¨ğ˜© ğ˜§ğ˜°ğ˜³ 1ğ˜” ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜£ğ˜¦ğ˜¤ğ˜¢ğ˜¶ğ˜´ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜’ğ˜ ğ˜¤ğ˜¢ğ˜¤ğ˜©ğ˜¦ ğ˜¢ğ˜­ğ˜°ğ˜¯ğ˜¦ ğ˜¦ğ˜¹ğ˜¤ğ˜¦ğ˜¦ğ˜¥ğ˜´ ğ˜ğ˜‰ğ˜” ğ˜¤ğ˜¢ğ˜±ğ˜¢ğ˜¤ğ˜ªğ˜µğ˜º. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜ªğ˜®ğ˜±ğ˜­ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µ ğ™ğ™ğ™£ğ™œ ğ˜¼ğ™©ğ™©ğ™šğ™£ğ™©ğ™ğ™¤ğ™£ (ğ˜¾ğ™¤ğ™£ğ™©ğ™šğ™­ğ™© ğ™‹ğ™–ğ™§ğ™–ğ™¡ğ™¡ğ™šğ™¡ğ™ğ™¨ğ™¢). ğ˜ğ˜¦ ğ˜¥ğ˜ªğ˜´ğ˜µğ˜³ğ˜ªğ˜£ğ˜¶ğ˜µğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜´ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜¥ğ˜ªğ˜®ğ˜¦ğ˜¯ğ˜´ğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¤ğ˜³ğ˜°ğ˜´ğ˜´ ğ˜®ğ˜¶ğ˜­ğ˜µğ˜ªğ˜±ğ˜­ğ˜¦ ğ˜ğ˜—ğ˜œğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜¤ğ˜ªğ˜³ğ˜¤ğ˜¶ğ˜­ğ˜¢ğ˜µğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜’ğ˜ ğ˜£ğ˜­ğ˜°ğ˜¤ğ˜¬ğ˜´ ğ˜ªğ˜¯ ğ˜¢ ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜°ğ˜±ğ˜°ğ˜­ğ˜°ğ˜¨ğ˜º, ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¸ğ˜ªğ˜¯ğ˜¨ ğ˜¶ğ˜´ ğ˜µğ˜° ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ ğ˜°ğ˜¯ ğ˜¯ğ˜¦ğ˜¢ğ˜³-ğ˜ªğ˜¯ğ˜§ğ˜ªğ˜¯ğ˜ªğ˜µğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜­ğ˜¦ğ˜¯ğ˜¨ğ˜µğ˜©ğ˜´ ğ˜¸ğ˜ªğ˜µğ˜©ğ˜°ğ˜¶ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜°ğ˜®ğ˜®ğ˜¶ğ˜¯ğ˜ªğ˜¤ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜°ğ˜·ğ˜¦ğ˜³ğ˜©ğ˜¦ğ˜¢ğ˜¥ ğ˜°ğ˜§ ğ˜¢ğ˜­ğ˜­-ğ˜µğ˜°-ğ˜¢ğ˜­ğ˜­ ğ˜¨ğ˜¢ğ˜µğ˜©ğ˜¦ğ˜³ğ˜´."
