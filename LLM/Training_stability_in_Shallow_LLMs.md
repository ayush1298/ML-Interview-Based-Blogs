ğŸ§© The Problem Warm-up Solves

When training starts, gradients are unstable because:

The LayerNorm scales (in Post-Norm) distort gradient magnitudes layer by layer.

The optimizerâ€™s learning rate is too large for the modelâ€™s randomly initialized weights.

Gradients can suddenly spike â†’ exploding gradients
or vanish â†’ training collapse.

So, the model needs a few steps to â€œfind its footingâ€ before applying full-strength updates.

âš™ï¸ The Warm-up Trick (used in â€œAttention Is All You Needâ€)

The paper proposed a learning rate schedule with linear warm-up, then decay:

<img width="571" height="162" alt="image" src="https://github.com/user-attachments/assets/1dc964dc-fee3-4092-b002-6e8ee9ebd9f0" />


ğŸ”¹ Phase 1: Linear Warm-up

Start with a tiny learning rate (e.g., 
1x(10^-7).

Gradually increase linearly for the first few thousand steps.

Purpose: Prevents big weight updates before the networkâ€™s internal scales (LayerNorm statistics, attention weights) stabilize.

This lets the model â€œease inâ€ to training.

ğŸ”¹ Phase 2: Inverse Square Root Decay

After warm-up, the learning rate decays as 1/âˆšt 

Keeps updates small and stable for long training runs.

Prevents oscillations after the model stabilizes.

ğŸ§  Why It Helps Post-Norm Models

Remember, in Post-Norm, gradients are repeatedly rescaled by LayerNorm during backprop.
At initialization, these rescaling factors are unpredictable.
If you start with a high learning rate, even small gradient distortions blow up quickly.

Warm-up prevents that:

During early steps, gradients are small â†’ weights adjust slowly â†’ activations stabilize.

After the model reaches a â€œsteady stateâ€ (activations have roughly consistent magnitudes), the normal LR can take over safely.

ğŸ§ª Example Schedule
| Step |	LR scaling |	Purpose |
| -------- | -------- | --------- |
| 0 â†’ 4000 |	Linear â†‘ from 0 â†’ 1.0Ã— |	Stabilize early training |
| 4000 â†’ end |	Decay âˆ 1/âˆšt |	Prevent late oscillations |

ğŸ§  TL;DR Summary

In shallow Post-Norm Transformers (like the original 6-layer model), learning rate warm-up is used to avoid exploding gradients during the unstable early phase of training.
It linearly ramps up the learning rate for a few thousand steps, letting the model stabilize internal statistics before training at full speed.
For deep models, this is insufficient â€” they need Pre-Norm, which inherently stabilizes gradients by preserving the clean residual path.
