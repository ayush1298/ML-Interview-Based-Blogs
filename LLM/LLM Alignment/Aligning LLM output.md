"ğ—¢ğ˜‚ğ—¿ ğ—°ğ—µğ—®ğ˜ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¶ğ˜€ ğ˜ğ—²ğ—°ğ—µğ—»ğ—¶ğ—°ğ—®ğ—¹ğ—¹ğ˜† ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ˜ğ—² ğ—¯ğ˜‚ğ˜ ğ—¿ğ˜‚ğ—±ğ—². ğ—ªğ—² ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—®ğ—¹ğ—¶ğ—´ğ—» ğ—¶ğ˜ ğ˜ğ—¼ 'ğ—›ğ—²ğ—¹ğ—½ğ—³ğ˜‚ğ—¹ & ğ—›ğ—®ğ—¿ğ—ºğ—¹ğ—²ğ˜€ğ˜€' ğ—¯ğ˜† ğ—™ğ—¿ğ—¶ğ—±ğ—®ğ˜†." ğŸ¤¯

The Safety Team has 10,000 human preference pairs (A vs B). The Engineering Lead is terrified of RLHF pipelines.

ğŸ…°ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—”: ğ—§ğ—µğ—² ğ—£ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—•ğ—®ğ—»ğ—±-ğ—”ğ—¶ğ—± We inject "Be nice and helpful" into the system prompt. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: ğ—§ğ—µğ—² ğ—ªğ—®ğ—¹ğ˜‚ğ—¶ğ—´ğ—¶ ğ—˜ğ—³ğ—³ğ—²ğ—°ğ˜. A determined user can easily "jailbreak" the persona. System prompts are suggestions, not constraints.

ğŸ…±ï¸ ğ—¢ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—•: ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—£ğ—£ğ—¢ (ğ—£ğ—¿ğ—¼ğ˜…ğ—¶ğ—ºğ—®ğ—¹ ğ—£ğ—¼ğ—¹ğ—¶ğ—°ğ˜† ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—») We train a Reward Model, then run PPO to optimize the policy. ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¢ğ˜ªğ˜­ğ˜¶ğ˜³ğ˜¦: ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—œğ—»ğ˜€ğ˜ğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ˜†. PPO is notoriously sensitive to hyperparameters. You need to manage four models in memory (Actor, Critic, Reward, Ref). The cluster runs out of VRAM, and the model collapses into gibberish (mode collapse).

ğŸ”‘ ğ—§ğ—µğ—² "ğ—§ğ—µğ—¶ğ—¿ğ—± ğ——ğ—¼ğ—¼ğ—¿" ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ——ğ—£ğ—¢ (ğ——ğ—¶ğ—¿ğ—²ğ—°ğ˜ ğ—£ğ—¿ğ—²ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—») We skip the Reward Model entirely.

1. We treat the preference data as a direct classification problem.
 
2. We implicitly optimize the reward by adjusting the likelihood of the "chosen" response vs. the "rejected" response, weighted by a KL-divergence constraint.
 
3. It's a simple cross-entropy loss calculation, stable as standard fine-tuning.
 

ğ—§ğ—µğ—² ğ—¢ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—²: We achieve state-of-the-art alignment performance with 1/4 of the memory footprint and zero Reinforcement Learning headaches.

ğŸ“–ğ—§ğ—µğ—² ğ—Ÿğ—²ğ˜€ğ˜€ğ—¼ğ—»: If you can mathematically eliminate a model from your pipeline, do it. Simplicity scales.