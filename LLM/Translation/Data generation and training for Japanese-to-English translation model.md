You're in a Senior NLP Engineer interview at Google DeepMind and the interviewer asks:

"We need to improve our ğ˜‘ğ˜¢ğ˜±ğ˜¢ğ˜¯ğ˜¦ğ˜´ğ˜¦-ğ˜µğ˜°-ğ˜Œğ˜¯ğ˜¨ğ˜­ğ˜ªğ˜´ğ˜© translation model. We have 10k parallel pairs and 1 billion lines of monolingual English text. To use ğğšğœğ¤-ğ“ğ«ğšğ§ğ¬ğ¥ğšğ­ğ¢ğ¨ğ§ effectively, which direction do we generate data, and exactly how do we pair it for training?"

Don't say: "We translate the English data into Japanese to check if the model is consistent."

And also don't say: "We create synthetic English targets so the model has more 'good' outputs to learn from."

ğ˜ğ˜§ ğ˜ºğ˜°ğ˜¶ ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ ğ˜ºğ˜°ğ˜¶ğ˜³ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜µğ˜° ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µ ğ˜´ğ˜ºğ˜¯ğ˜µğ˜©ğ˜¦ğ˜µğ˜ªğ˜¤ ğ˜µğ˜¦ğ˜¹ğ˜µ, ğ˜ºğ˜°ğ˜¶ ğ˜¢ğ˜³ğ˜¦ ğ˜µğ˜¦ğ˜¢ğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜ªğ˜µ ğ˜µğ˜° ğ˜®ğ˜ªğ˜®ğ˜ªğ˜¤ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜ªğ˜´ğ˜µğ˜¢ğ˜¬ğ˜¦ğ˜´ ğ˜°ğ˜§ ğ˜¢ ğ˜®ğ˜¢ğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¦.

The secret to ğğšğœğ¤-ğ“ğ«ğšğ§ğ¬ğ¥ğšğ­ğ¢ğ¨ğ§ isn't just "more data", it's about protecting the ğƒğğœğ¨ğğğ«'ğ¬ ğŸğ¥ğ®ğğ§ğœğ².

Here is the production-grade architecture you need to describe:
1ï¸âƒ£ ğ˜›ğ˜©ğ˜¦ "ğ˜™ğ˜¦ğ˜·ğ˜¦ğ˜³ğ˜´ğ˜¦" ğ˜ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯:
You take your massive Monolingual English (Target) corpus and use an intermediate model to translate it back into Synthetic Japanese (Source).

2ï¸âƒ£ ğ˜›ğ˜©ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜—ğ˜¢ğ˜ªğ˜³:
You train your final model on these pairs: (Synthetic Japanese Input, Real English Output).

ğ–ğ¡ğ² ğ­ğ¡ğ¢ğ¬ ğğ¢ğ«ğğœğ­ğ¢ğ¨ğ§ ğ¦ğšğ­ğ­ğğ«ğ¬?
- In NMT, the decoder needs to learn the probability distribution of real human language. By keeping the English side "real" (monolingual data), the decoder learns perfect grammar and style.

- The encoder learns to handle noisy, imperfect, or "translationese" Japanese inputs and still map them to clean English.

If you did it the other way (ğ˜™ğ˜¦ğ˜¢ğ˜­ ğ˜‘ğ˜¢ğ˜±ğ˜¢ğ˜¯ğ˜¦ğ˜´ğ˜¦ -> ğ˜šğ˜ºğ˜¯ğ˜µğ˜©ğ˜¦ğ˜µğ˜ªğ˜¤ ğ˜Œğ˜¯ğ˜¨ğ˜­ğ˜ªğ˜´ğ˜©), your model treats the Synthetic English as the ground truth. It learns to output "ğ˜”ğ˜¢ğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜¯ğ˜´ğ˜­ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜³ğ˜µğ˜ªğ˜§ğ˜¢ğ˜¤ğ˜µğ˜´" rather than human language.

Think of it like this: To teach a chef (the model), you show them a perfect gourmet dish (Real English) and ask them to guess the messy recipe (Synthetic Japanese). You never show them a bad dish and say "cook this."

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"We pair Synthetic Sources with Natural Targets. This forces the model to map potentially noisy inputs to high-quality, fluent outputs, ensuring the decoder learns the true distribution of natural English rather than overfitting to the artifacts of another model."
