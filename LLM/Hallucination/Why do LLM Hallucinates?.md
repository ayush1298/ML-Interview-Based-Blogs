The interview is for an AI Platform Specialist role at JPMC.

Interviewer: "Everyone blames hallucinations on the model. I want to know what you think. Why do LLMs make things up?"

You: "Before I answer, let me ask you something - if a model gives a wrong answer, do you assume it invented it, or that it lacked the right information to begin with?"

Interviewer: "Instinctively, I'd say it invented it."

You: "And that's the misconception. Hallucination is usually a symptom of missing grounding, not a failure of intelligence.
LLMs don't hallucinate because they want to. They hallucinate because they're too helpful - they'd rather approximate than admit ignorance."

Interviewer: "So you're saying the model isn't the root problem?‚Äù

You: "Yep. The real causes are:
 1. Bad or insufficient context - the model fills gaps with probability, not truth.
 2. Poor retrieval - RAG without accurate recall is like a GPS with blurry maps.
 3. Ambiguous prompts - unclear instructions lead to creative answers.
 4. Lack of constraints - without rules, the model improvises."

Interviewer: "Interesting. Then why do enterprises still talk about 'fixing hallucination' as if it's one problem?"

You: "Because it's easier to blame the model than the system around it.
But hallucinations exist at multiple layers:
 - Input layer: missing context
 - Reasoning layer: the model overgeneralizes
 - Retrieval layer: the system fetched the wrong snippet
 - Policy layer: missing guardrails
If you treat hallucination as one thing, you'll solve none of it."

Interviewer: "Alright then - what actually reduces hallucinations in production?"

You: "Three things:
 1. Grounding: Pulling answers from verifiable documents, not memory.
 2. Validation: Using secondary LLMs or rule-based checks to confirm reasoning.
 3. Escalation: Teaching the agent to say - I don't know when confidence drops.
Good AI isn't perfect. Good AI knows when to stop guessing."
