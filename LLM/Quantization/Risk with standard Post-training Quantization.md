ğ™ğ™š ğ™Šğ™ªğ™©ğ™¡ğ™ğ™šğ™§ ğ˜¿ğ™šğ™¨ğ™©ğ™§ğ™ªğ™˜ğ™©ğ™ğ™¤ğ™£ ğ™ğ™§ğ™–ğ™¥ ğŸ“‰

You're in a Machine Learning Infrastructure interview at Hugging Face. The interviewer asks:

"ğ˜ğ˜¦ ğ˜¢ğ˜³ğ˜¦ ğ˜¥ğ˜¦ğ˜±ğ˜­ğ˜°ğ˜ºğ˜ªğ˜¯ğ˜¨ ğ˜“ğ˜­ğ˜¢ğ˜®ğ˜¢-3-70ğ˜‰. ğ˜ğ˜µ ğ˜³ğ˜¦ğ˜²ğ˜¶ğ˜ªğ˜³ğ˜¦ğ˜´ 140ğ˜ğ˜‰ ğ˜°ğ˜§ ğ˜ğ˜™ğ˜ˆğ˜” ğ˜ªğ˜¯ ğ˜ğ˜—16, ğ˜¸ğ˜©ğ˜ªğ˜¤ğ˜© ğ˜ªğ˜´ ğ˜µğ˜°ğ˜° ğ˜¦ğ˜¹ğ˜±ğ˜¦ğ˜¯ğ˜´ğ˜ªğ˜·ğ˜¦. ğ˜ğ˜¦ ğ˜¸ğ˜¢ğ˜¯ğ˜µ ğ˜µğ˜° ğ˜²ğ˜¶ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜»ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜¸ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´ ğ˜µğ˜° ğ˜ğ˜•ğ˜›4 ğ˜µğ˜° ğ˜³ğ˜¶ğ˜¯ ğ˜°ğ˜¯ ğ˜¢ ğ˜´ğ˜ªğ˜¯ğ˜¨ğ˜­ğ˜¦ ğ˜ˆ100. ğ˜ğ˜©ğ˜¢ğ˜µ ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜£ğ˜ªğ˜¨ğ˜¨ğ˜¦ğ˜´ğ˜µ ğ˜³ğ˜ªğ˜´ğ˜¬ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜´ğ˜µğ˜¢ğ˜¯ğ˜¥ğ˜¢ğ˜³ğ˜¥ ğ˜±ğ˜°ğ˜´ğ˜µ-ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜²ğ˜¶ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯?"

ğŸ—£ï¸ Most candidates say:

"ğ˜›ğ˜©ğ˜¦ ğ˜³ğ˜ªğ˜´ğ˜¬ ğ˜ªğ˜´ ğ˜­ğ˜°ğ˜´ğ˜´ ğ˜°ğ˜§ ğ˜±ğ˜³ğ˜¦ğ˜¤ğ˜ªğ˜´ğ˜ªğ˜°ğ˜¯. ğ˜ğ˜¦ ğ˜´ğ˜©ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜¶ğ˜´ğ˜¦ ğ˜˜ğ˜¶ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ˆğ˜¸ğ˜¢ğ˜³ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ (ğ˜˜ğ˜ˆğ˜›) ğ˜µğ˜° ğ˜³ğ˜¦ğ˜¤ğ˜°ğ˜·ğ˜¦ğ˜³ ğ˜µğ˜©ğ˜¦ ğ˜¢ğ˜¤ğ˜¤ğ˜¶ğ˜³ğ˜¢ğ˜¤ğ˜º."

Wrong. You can't afford to re-train a 70B model (QAT). And standard PTQ (Post-Training Quantization) will break the model completely.

ğ—§ğ—µğ—² ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜†: ğ—”ğ—°ğ˜ğ—¶ğ˜ƒğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¢ğ˜‚ğ˜ğ—¹ğ—¶ğ—²ğ—¿ğ˜€ ğ—±ğ—²ğ˜€ğ˜ğ—¿ğ—¼ğ˜† ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—´ğ—¿ğ—¶ğ—±.

In large Transformers (>6B params), "Emergent Features" appear.

â€¢ Specific channels in the activation matrices have values up to ğŸ­ğŸ¬ğŸ¬ğ˜… ğ—¹ğ—®ğ—¿ğ—´ğ—²ğ—¿ than the rest.
 
â€¢ Standard quantization (Min-Max) stretches the quantization grid to accommodate these massive outliers.
 
â€¢ This causes the vast majority of "normal" values to be rounded to zero, effectively lobotomizing the model.
 

âœ… ğ—§ğ—µğ—² ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: ğ—”ğ—°ğ˜ğ—¶ğ˜ƒğ—®ğ˜ğ—¶ğ—¼ğ—»-ğ—”ğ˜„ğ—®ğ—¿ğ—² ğ—ªğ—²ğ—¶ğ—´ğ—µğ˜ ğ—¤ğ˜‚ğ—®ğ—»ğ˜ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» (ğ—”ğ—ªğ—¤).

You don't just quantize the weights based on the weights. You quantize the weights based on the ğ—®ğ—°ğ˜ğ—¶ğ˜ƒğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€.

â€¢ ğ—£ğ—¿ğ—¼ğ˜ğ—²ğ—°ğ˜ ğ˜ğ—µğ—² ğ—¦ğ—®ğ—¹ğ—¶ğ—²ğ—»ğ˜ ğ—ªğ—²ğ—¶ğ—´ğ—µğ˜ğ˜€: AWQ identifies which weights multiply these activation outliers.
 
â€¢ ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´: It effectively scales up the quantization buckets for those critical weights, preserving their precision, while aggressively compressing the rest.
 

âœï¸ ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±:

"ğ˜šğ˜µğ˜¢ğ˜¯ğ˜¥ğ˜¢ğ˜³ğ˜¥ ğ˜²ğ˜¶ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜§ğ˜¢ğ˜ªğ˜­ğ˜´ ğ˜¥ğ˜¶ğ˜¦ ğ˜µğ˜° ğ˜®ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜·ğ˜¦ ğ˜¢ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜°ğ˜¶ğ˜µğ˜­ğ˜ªğ˜¦ğ˜³ğ˜´ ğ˜ªğ˜¯ ğ˜­ğ˜¢ğ˜³ğ˜¨ğ˜¦ ğ˜“ğ˜“ğ˜”ğ˜´. ğ˜Šğ˜­ğ˜ªğ˜±ğ˜±ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ğ˜® ğ˜¥ğ˜¦ğ˜´ğ˜µğ˜³ğ˜°ğ˜ºğ˜´ ğ˜³ğ˜¦ğ˜¢ğ˜´ğ˜°ğ˜¯ğ˜ªğ˜¯ğ˜¨; ğ˜¢ğ˜¤ğ˜¤ğ˜°ğ˜®ğ˜®ğ˜°ğ˜¥ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ğ˜® ğ˜¥ğ˜¦ğ˜´ğ˜µğ˜³ğ˜°ğ˜ºğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜³ğ˜¦ğ˜´ğ˜°ğ˜­ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜§ğ˜°ğ˜³ ğ˜°ğ˜µğ˜©ğ˜¦ğ˜³ ğ˜±ğ˜¢ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜µğ˜¦ğ˜³ğ˜´. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜¶ğ˜´ğ˜¦ ğ˜ˆğ˜ğ˜˜ (ğ˜ˆğ˜¤ğ˜µğ˜ªğ˜·ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯-ğ˜¢ğ˜¸ğ˜¢ğ˜³ğ˜¦ ğ˜ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µ ğ˜˜ğ˜¶ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜»ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯). ğ˜ğ˜µ ğ˜¢ğ˜¯ğ˜¢ğ˜­ğ˜ºğ˜»ğ˜¦ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜¢ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¥ğ˜ªğ˜´ğ˜µğ˜³ğ˜ªğ˜£ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜µğ˜° ğ˜ªğ˜¥ğ˜¦ğ˜¯ğ˜µğ˜ªğ˜§ğ˜º ğ˜µğ˜©ğ˜¦ ~1% ğ˜°ğ˜§ ğ˜´ğ˜¢ğ˜­ğ˜ªğ˜¦ğ˜¯ğ˜µ ğ˜¸ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜±ğ˜³ğ˜°ğ˜µğ˜¦ğ˜¤ğ˜µğ˜´ ğ˜µğ˜©ğ˜¦ğ˜®, ğ˜¢ğ˜­ğ˜­ğ˜°ğ˜¸ğ˜ªğ˜¯ğ˜¨ ğ˜¶ğ˜´ ğ˜µğ˜° ğ˜³ğ˜¦ğ˜¢ğ˜¤ğ˜© ğ˜ğ˜•ğ˜›4 ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜¯ğ˜¦ğ˜¢ğ˜³-ğ˜ğ˜—16 ğ˜±ğ˜¦ğ˜³ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¢ğ˜¯ğ˜¤ğ˜¦."
