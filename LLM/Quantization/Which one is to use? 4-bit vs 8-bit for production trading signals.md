You're in a GenAI Engineer interview at Goldman Sachs, and the interviewer asks:
"We need to deploy a 70B parameter LLM for production trading signals. Should we use 4-bit or 8-bit quantization? Justify your choice."
Here's how you can answer:
A. Most candidates fumble here because they only know "quantization reduces model size." Incomplete answer.
B. There are 5 critical factors every GenAI engineer should understand cold.

ğŸ­. ğ—§ğ—µğ—² ğ—£ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—»-ğ—£ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² ğ—§ğ—¿ğ—®ğ—±ğ—²ğ—¼ğ—³ğ—³ - ğ—§ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—²
8-bit (INT8) maintains near-IDENTICAL accuracy:
Performance degradation < 1% on most tasks
Uses linear quantization: Q = round(scale Ã— W + zero_point)
4-bit (INT4/NF4) trades accuracy for efficiency:
Performance degradation 2-5% depending on the architecture
Uses non-linear quantization (NormalFloat4) to preserve distribution
The brutal truth? 8-bit is production-safe. 4-bit requires EXTENSIVE validation.

ğŸ®. ğ—§ğ—µğ—² ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ—™ğ—¼ğ—¼ğ˜ğ—½ğ—¿ğ—¶ğ—»ğ˜ - ğ—ªğ—µğ—²ğ—¿ğ—² ğŸµğŸ¬% ğ—¼ğ—³ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ˜€ ğ—´ğ—¼ ğ˜„ğ—¿ğ—¼ğ—»ğ—´
Most people think "4-bit = 2x smaller than 8-bit."
Wrong move.
FP32: 70B model = 280GB
INT8: 70B model = 70GB (4x compression)
INT4: 70B model = 35GB (8x compression)
But here's the catch - you STILL need overhead for KV cache, activations, and gradients.
Real-world 70B INT4 deployment? Needs 48-60GB minimum, not 35GB.

ğŸ¯. ğ—§ğ—µğ—² ğ—¤ğ˜‚ğ—®ğ—»ğ˜ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ— ğ—²ğ˜ğ—µğ—¼ğ—± - ğ—§ğ—µğ—² ğ—µğ—¶ğ—±ğ—±ğ—²ğ—» ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¸ğ—¶ğ—¹ğ—¹ğ—²ğ—¿
Here's what separates junior from senior GenAI engineers:
Post-Training Quantization (PTQ):
Fast setup (hours)
Works reliably for 8-bit
4-bit quality varies wildly
GPTQ/AWQ (Advanced PTQ):
Weight-only quantization with calibration
Industry standard for 4-bit LLMs
Requires representative calibration dataset (CRITICAL)
QAT (Quantization-Aware Training):
Expensive compute (days to weeks)
Required for mission-critical 4-bit deployments

ğŸ°. ğ—§ğ—µğ—² ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—¦ğ—½ğ—²ğ—²ğ—± ğ—§ğ—¿ğ—®ğ—±ğ—² - ğŸ±ğ˜… ğ˜ğ—µğ—¿ğ—¼ğ˜‚ğ—´ğ—µğ—½ğ˜‚ğ˜, ğ—¯ğ˜‚ğ˜ ğ˜„ğ—µğ˜†?
8-bit: Native GPU support (Tensor Cores)
Blazing fast matrix multiplication
1.5-2x throughput vs FP16
4-bit: Limited hardware support
Requires dequantization to FP16 for computation
Memory bandwidth bound, NOT compute bound
The counterintuitive reality? 4-bit isn't always faster despite being smaller.

ğŸ±. ğ—§ğ—µğ—² ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜ ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜† - ğ—§ğ—µğ—² ğ—°ğ—¼ğ˜€ğ˜ ğ—»ğ—¼ğ—¯ğ—¼ğ—±ğ˜† ğ˜ğ—®ğ—¹ğ—¸ğ˜€ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜
8-bit: A100 80GB fits 70B comfortably, batch size 8-16 supported
4-bit: RTX 4090 24GB runs 70B (barely), batch size 1-4 maximum

ğ—ªğ—µğ—²ğ—» ğŸ´-ğ—¯ğ—¶ğ˜ ğ˜„ğ—¶ğ—»ğ˜€:
âœ… Accuracy non-negotiable (finance, healthcare)
âœ… Production reliability > cost optimization
âœ… Batch inference workloads
ğ—ªğ—µğ—²ğ—» ğŸ°-ğ—¯ğ—¶ğ˜ ğ˜„ğ—¶ğ—»ğ˜€:
âœ… Extreme memory constraints
âœ… Cost optimization critical
âœ… Acceptable 2-5% quality degradation
âœ… Using GPTQ/AWQ with proper calibration
