You're in a ML Engineer interview at OpenAI and the interviewer asks:

"Your team is hitting OOM errors. An intern engineer proposes casting the entire model and optimizer state to bfloat16 to cut memory usage by 50%. Why is this a ticking time bomb that will cause training to go out of control, and what components must stay in FP32?"

Most candidates say:
"Actually, bfloat16 is safe because it shares the same dynamic range (exponent) as float32. Unlike float16, it doesn't suffer from overflow, so the intern engineer is right, you can cast everything to save memory without issues."

Wrong. That decision just killed your training run.

The reality is that we are confusing ğ‘ğšğ§ğ ğ ğ°ğ¢ğ­ğ¡ ğğ«ğğœğ¢ğ¬ğ¢ğ¨ğ§. bfloat16 achieves its dynamic range by aggressively sacrificing its significand (the fraction). It only has 7 bits for precision, compared to 23 bits in float32.

This leads to the ğŒğšğ§ğ­ğ¢ğ¬ğ¬ğš ğ“ğ«ğšğ©.

In deep learning, weight updates are often extremely small values (e.g., 1e-8). When you try to add this tiny gradient update to a larger model parameter stored in bfloat16, the hardware physically cannot represent the difference. The update rounds down to zero.

The model effectively "freezes" not because the math is wrong, but because the bit-width is too narrow to capture the learning signal.

You must use ğŒğ¢ğ±ğğ ğğ«ğğœğ¢ğ¬ğ¢ğ¨ğ§ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ :
- 1. ğ˜Šğ˜°ğ˜®ğ˜±ğ˜¶ğ˜µğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ (ğ˜ğ˜°ğ˜³ğ˜¸ğ˜¢ğ˜³ğ˜¥/ğ˜‰ğ˜¢ğ˜¤ğ˜¬ğ˜¸ğ˜¢ğ˜³ğ˜¥): bfloat16 (Fast, low memory).

- 2. ğ˜šğ˜µğ˜°ğ˜³ğ˜¢ğ˜¨ğ˜¦ (ğ˜–ğ˜±ğ˜µğ˜ªğ˜®ğ˜ªğ˜»ğ˜¦ğ˜³ ğ˜šğ˜µğ˜¢ğ˜µğ˜¦ğ˜´ & ğ˜”ğ˜¢ğ˜´ğ˜µğ˜¦ğ˜³ ğ˜ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´): float32.

You need the high precision of FP32 to "accumulate" those tiny updates before they are cast back to bfloat16 for the next pass.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"You can compute in low precision, but you must store in high precision. Casting optimizer states to bfloat16 causes 'swallowing' of small gradient updates due to low mantissa bits. You need FP32 Master Weights to maintain training stability."
