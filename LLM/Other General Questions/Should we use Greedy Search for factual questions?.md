You're in a Senior ML Interview at OpenAI. The interviewer sets a trap:

"We need a low-latency geography trivia bot. Since the questions are factual, should we just use ğ˜ğ˜³ğ˜¦ğ˜¦ğ˜¥ğ˜º ğ˜šğ˜¦ğ˜¢ğ˜³ğ˜¤ğ˜© to save compute?"

90% of candidates walk right into the "Yes" trap.

They say "Yes. ğ˜ğ˜³ğ˜¦ğ˜¦ğ˜¥ğ˜º ğ˜šğ˜¦ğ˜¢ğ˜³ğ˜¤ğ˜© is O(N), extremely fast, and if the model is well-trained, taking the highest probability token at each step (ğ˜¢ğ˜³ğ˜¨ğ˜®ğ˜¢ğ˜¹) should logically yield the most probable correct answer."

This fails because they are confusing ğ‹ğ¨ğœğšğ¥ ğğ©ğ­ğ¢ğ¦ğš with ğ†ğ¥ğ¨ğ›ğšğ¥ ğğ©ğ­ğ¢ğ¦ğš.

Greedy decoding cannot backtrack. Once it commits to a token, it is locked in forever, even if that token leads to a dead end.

Consider the probabilities for the next token in the answer to: "Where is The Liberty Bell located?"
- ğ("ğğğ§ğ§ğ¬ğ²ğ¥ğ¯ğšğ§ğ¢ğš") = 0.4 (The correct answer)
- ğ("ğğğ°") = 0.55 (The start of "New York" or "New Jersey")

ğ˜ğ˜³ğ˜¦ğ˜¦ğ˜¥ğ˜º ğ˜šğ˜¦ğ˜¢ğ˜³ğ˜¤ğ˜© sees 0.55 > 0.4. It immediately locks in "New".

The correct answer ("Pennsylvania") is now mathematically impossible to reach. Your bot just hallucinated a wrong state because it got distracted by a common, high-probability prefix.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: You need to solve for ğ“ğ¡ğ ğ†ğ«ğğğğ² ğ†ğšğ«ğğğ§ ğğšğ­ğ¡.

You don't need expensive exhaustive search, but you do need ğ˜‰ğ˜¦ğ˜¢ğ˜® ğ˜šğ˜¦ğ˜¢ğ˜³ğ˜¤ğ˜© with a narrow width (e.g., k=5).

By keeping the top 5 distinct paths active at every timestamp, you allow the model to "ğ˜¤ğ˜©ğ˜¢ğ˜¯ğ˜¨ğ˜¦ ğ˜ªğ˜µğ˜´ ğ˜®ğ˜ªğ˜¯ğ˜¥." The model might see that while "New" starts strong, the subsequent tokens drop in probability. Meanwhile, the "Pennsylvania" path, though starting lower, accumulates higher total probability over the full sequence.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"Never trade global coherence for local probability. Even for low-latency tasks, a Beam Width of 3-5 is the minimum requirement to prevent the model from locking itself into high-probability errors."
