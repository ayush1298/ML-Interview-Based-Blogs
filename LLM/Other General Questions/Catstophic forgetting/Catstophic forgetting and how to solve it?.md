You're in an AI Engineer interview at OpenAI and the interviewer asks: 

"You've successfully fine-tuned a model with RL. It's now excellent at following instructions, but it's become 'dumber' at general knowledge and creative writing. What is this phenomenon called, and what specific term would you add to your loss function to prevent this?"

Don't say: "That's catastrophic forgetting. We can fix it with a lower learning rate or by mixing in more general-purpose SFT data."

This is a textbook answer that misses the real RL-specific problem. It's not just "forgetting", it's active over-optimization.

The reality is your model hasn't just "forgotten" its knowledge, it's actively learning to exploit the flaws in your reward model (RM).

This is the "ğšğ¥ğ¢ğ ğ§ğ¦ğğ§ğ­ ğ­ğšğ±."

Your policy is chasing the reward signal so aggressively that it's diverging from the original, general-purpose model it started as. It's learning "hacks" to please the RM, sacrificing its underlying knowledge.

You don't just "remind" the model. You ğ˜¤ğ˜°ğ˜¯ğ˜´ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ it.

The solution is to add a ğŠğ‹ ğğ¢ğ¯ğğ«ğ ğğ§ğœğ ğ©ğğ§ğšğ¥ğ­ğ² to your objective function.
- Your goal isn't just to ğ˜®ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜ªğ˜»ğ˜¦ ğ˜³ğ˜¦ğ˜¸ğ˜¢ğ˜³ğ˜¥.
- Your goal is to maximize reward while ğ˜´ğ˜µğ˜¢ğ˜ºğ˜ªğ˜¯ğ˜¨ ğ˜¤ğ˜­ğ˜°ğ˜´ğ˜¦ ğ˜µğ˜° ğ˜ºğ˜°ğ˜¶ğ˜³ ğ˜°ğ˜³ğ˜ªğ˜¨ğ˜ªğ˜¯ğ˜¢ğ˜­ ğ˜±ğ˜°ğ˜­ğ˜ªğ˜¤ğ˜º.

You keep a frozen reference model (let's call it Ï€_ref, your original SFT model) and you penalize your new, active policy (Ï€_Î¸) for becoming too different from it.

The new loss term is a "leash." It forces the model to find answers that both get a high reward AND are "stylistically" similar to how the original, smart model would have responded.

This is the core mechanic of PPO. You're not just optimizing for the reward; you're optimizing for reward minus a divergence penalty.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:

"This is the ğšğ¥ğ¢ğ ğ§ğ¦ğğ§ğ­ ğ­ğšğ±, a form of over-optimization where the policy diverges from the pre-trained model's capabilities to exploit the reward model. I would add a ğŠğ‹ ğğ¢ğ¯ğğ«ğ ğğ§ğœğ ğ©ğğ§ğšğ¥ğ­ğ² to the loss, calculated between the active policy and a frozen ğ˜³ğ˜¦ğ˜§ğ˜¦ğ˜³ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­, to regularize the policy and preserve its general knowledge."
