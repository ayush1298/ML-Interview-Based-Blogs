You're in a Senior ML Engineer interview at Google and the interviewer asks:

"We're building a toxicity detection dataset where only 1% of comments are actually toxic. We hired two annotators. Their inter-annotator agreement is 99%. Are we good to go?"

Most candidates smile and say: "Yes! 99% agreement is amazing. The data is clearly high quality."

They just failed the interview since they confused ğœğ¨ğ§ğœğ®ğ«ğ«ğğ§ğœğ with ğ¬ğ¢ğ ğ§ğšğ¥.

The reality? ğ˜ğ˜¯ ğ˜ªğ˜®ğ˜£ğ˜¢ğ˜­ğ˜¢ğ˜¯ğ˜¤ğ˜¦ğ˜¥ ğ˜¥ğ˜¢ğ˜µğ˜¢ğ˜´ğ˜¦ğ˜µğ˜´, "ğ˜¢ğ˜¤ğ˜¤ğ˜¶ğ˜³ğ˜¢ğ˜¤ğ˜º" ğ˜ªğ˜´ ğ˜¢ ğ˜­ğ˜ªğ˜¢ğ˜³.

If your dataset is 99% safe and 1% toxic, an annotator could be asleep at the wheel, mark every single comment as "Safe," and still achieve 99% agreement with another lazy annotator. They found 0% of the toxicity, but on paper, they look perfect.

The Senior Engineer knows you are fighting ğ‚ğ¡ğšğ§ğœğ ğ€ğ ğ«ğğğ¦ğğ§ğ­.

When one class dominates (like "Safe" comments), the statistical probability of two people agreeing by accident skyrockets. You aren't measuring quality, you're measuring the class imbalance.

To fix this, you need to normalize for that baseline probability.

You need ğ‚ğ¨ğ¡ğğ§'ğ¬ ğŠğšğ©ğ©ğš (or ğ…ğ¥ğğ¢ğ¬ğ¬' ğŠğšğ©ğ©ğš for 3+ annotators).
- Accuracy = (Observed Agreement)
- Kappa = (Observed Agreement - Chance Agreement) / (1 - Chance Agreement)

In our 99% "safe" scenario:
- If both annotators just spam "Safe," their Accuracy is 99%.
- But their Kappa score is 0.0.

A Kappa of 0.0 reveals the truth: Your annotators aren't agreeing on the content, they are just agreeing that the rare class doesn't exist.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"99% agreement on a 1% minority class is meaningless due to chance agreement. I would calculate Cohen's Kappa to penalize the probability of random agreement. If the Kappa is below 0.6, we have a guideline problem, not a high-quality dataset."
