You're in a Senior Machine Learning Interview at Google DeepMind. The interviewer sets a trap. They hand you a dataset with 15% missing values in the "Age" column and ask a simple question:

"How do you handle these missing values before we start training?"

90% of candidates walk right into the trap.

The candidate immediately grabs the whiteboard marker.

"Easy. I'll calculate the median of the 'Age' column to handle outliers, then fill the empty cells with that value."

They might even write the Pandas equivalent:
 ğ˜¥ğ˜§['ğ˜¢ğ˜¨ğ˜¦'] = ğ˜¥ğ˜§['ğ˜¢ğ˜¨ğ˜¦'].ğ˜§ğ˜ªğ˜­ğ˜­ğ˜¯ğ˜¢(ğ˜¥ğ˜§['ğ˜¢ğ˜¨ğ˜¦'].ğ˜®ğ˜¦ğ˜¥ğ˜ªğ˜¢ğ˜¯())

The interviewer nods, smiles, and ends the interview 5 minutes later. They didn't get the job.

Why did they fail? Because they treated data cleaning as a generic "pre-processing" step. By calculating the median on the entire dataset (before splitting), they effectively "snooped" on the test set. Their model learned a distribution that includes data it shouldn't see until inference.

In the real world, they don't have access to tomorrow's data to calculate today's median.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: To pass this bar, you need to treat your pipeline like a time machine. You must protect the "future" (test set) from the "past" (training set).

The engineering reality:
- ğ’ğ©ğ¥ğ¢ğ­ ğ…ğ¢ğ«ğ¬ğ­: Divide your data into Train and Test immediately.
- ğ…ğ¢ğ­ ğ¨ğ§ ğ“ğ«ğšğ¢ğ§: Calculate your statistics (mean, median, std dev) only using the training set.
- ğ“ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ ğğ¨ğ­ğ¡: Apply those specific training values to fill gaps in both the Train AND the Test set.

If your training median is 34 and your test median is 41, you fill the test set holes with 34.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"I never calculate global statistics. I fit my imputers on the training set and apply that transformation to the test set to prevent data leakage. My pipeline must emulate the blindness of production."
