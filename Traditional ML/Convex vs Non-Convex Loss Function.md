## üîπ What is a Convex vs Non-Convex Loss Function?

* A **convex** loss function means the optimization surface has **a single global minimum** ‚Äî gradient descent is guaranteed (in theory) to find it.
  ‚û§ Mathematically:
  ( f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y) )

* A **non-convex** loss function has **multiple local minima** ‚Äî optimization can get stuck in suboptimal points.

---

<img width="976" height="419" alt="image" src="https://github.com/user-attachments/assets/137a474d-8d69-4a30-b589-d12de138cf27" />


## üîπ Common **Non-Convex** Loss Functions

Non-convex losses arise in **nonlinear models** (like neural networks).

| Model Type                                         | Loss Function                                        | Convexity    | Why Non-Convex                         |
| -------------------------------------------------- | ---------------------------------------------------- | ------------ | -------------------------------------- |
| **Neural Networks (any deep model)**               | **Cross-Entropy / MSE (with nonlinear activations)** | ‚ùå Non-convex | Due to nonlinear composition of layers |
| **Autoencoders / VAEs**                            | Reconstruction + Regularization losses               | ‚ùå Non-convex | Network weights + nonlinearities       |
| **GANs**                                           | Generator‚ÄìDiscriminator loss                         | ‚ùå Non-convex | Min‚Äìmax game between two networks      |
| **Matrix Factorization / Collaborative Filtering** | Squared loss on latent embeddings                    | ‚ùå Non-convex | Product of unknown matrices            |
| **K-Means Clustering**                             | Sum of squared distances to centroids                | ‚ùå Non-convex | Centroid assignments are discrete      |

---

## üîπ Intuition

* Convex losses ‚Üí **easy to optimize**, no local minima traps.
* Non-convex losses ‚Üí **harder**, but neural networks often work well due to good local minima and stochastic optimization (SGD noise helps escape bad ones).

---

### ‚úÖ Summary Table

| Loss Function                       | Convexity    | Typical Use                    |
| ----------------------------------- | ------------ | ------------------------------ |
| Mean Squared Error                  | ‚úÖ Convex     | Linear regression              |
| Cross Entropy (Logistic Regression) | ‚úÖ Convex     | Binary classification (linear) |
| Hinge Loss                          | ‚úÖ Convex     | SVM                            |
| Huber Loss                          | ‚úÖ Convex     | Robust regression              |
| Cross Entropy (Deep NN)             | ‚ùå Non-convex | Classification (deep models)   |
| MSE (Deep NN)                       | ‚ùå Non-convex | Regression (deep models)       |
| GAN Loss                            | ‚ùå Non-convex | Generative modeling            |
| K-Means Objective                   | ‚ùå Non-convex | Clustering                     |

---
