You're in a Senior ML System Design interview at Twitter. The interviewer sets a trap:

"We have a firehose of tweets coming in at 50k TPS. I need you to maintain a statistically representative sample of exactly 10,000 tweets for a training buffer at all times. The stream never stops. You cannot store the full history."

90% of candidates walk right into the wall.

Most candidates revert to ğğšğ­ğœğ¡ ğ“ğ¡ğ¢ğ§ğ¤ğ¢ğ§ğ . 

They say: "Easy. I'll buffer the last hour of data into S3, load it into a Dataframe, and run ğ˜¥ğ˜§.ğ˜´ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦(ğ˜¯=10000)" . Or they suggest:

"I will just flip a coin and keep every 100th tweet."

The interviewer stops you. "You just crashed production."

The Reality:
- ğ˜›ğ˜©ğ˜¦ ğ˜”ğ˜¦ğ˜®ğ˜°ğ˜³ğ˜º ğ˜ğ˜¢ğ˜­ğ˜­: The stream is potentially infinite ( N â†’ âˆ ). You cannot buffer all data without hitting an OOM (Out of Memory) error.
- ğ˜›ğ˜©ğ˜¦ ğ˜—ğ˜³ğ˜°ğ˜£ğ˜¢ğ˜£ğ˜ªğ˜­ğ˜ªğ˜µğ˜º ğ˜‰ğ˜ªğ˜¢ğ˜´: If you pick every 100th tweet, you introduce periodicity bias. If you pick with a fixed probability P, you end up with a variable sample size, not a fixed size k.
- ğ˜›ğ˜©ğ˜¦ ğ˜œğ˜¯ğ˜¬ğ˜¯ğ˜°ğ˜¸ğ˜¯ ğ˜•: You don't know how long the stream runs. You cannot calculate the probability 1/N because N is changing every millisecond.

The Solution: You don't need more memory. You need ğ“ğ¡ğ ğˆğ§ğŸğ¢ğ§ğ¢ğ­ğ ğ‘ğğ¬ğğ«ğ¯ğ¨ğ¢ğ« ğğ«ğ¨ğ­ğ¨ğœğ¨ğ¥.

To solve this, you implement ğ‘ğğ¬ğğ«ğ¯ğ¨ğ¢ğ« ğ’ğšğ¦ğ©ğ¥ğ¢ğ§ğ :

- Initialize: Create a buffer of size k (10,000). Fill it with the first 10,000 tweets.
- Stream: For every i-th tweet that arrives after that (where i > k):
- Roll: Generate a random integer j between 0 and n.
- Replace: If j < k, replace the element at index j with the new tweet. Otherwise, discard the new tweet.

ğ“ğ¡ğ ğğšğ²ğ¨ğŸğŸ: This simple logic mathematically guarantees that at any snapshot in time n, every single tweet seen so far has exactly a k/n probability of being in your buffer.

ğ“ğ¡ğ "ğ‡ğ¢ğ«ğğ" ğ€ğ§ğ¬ğ°ğğ«: "I will use Reservoir Sampling to maintain a fixed-size buffer of k items. This allows single-pass sampling over an infinite stream with O(1) space complexity, guaranteeing uniform probability without knowing N in advance."
