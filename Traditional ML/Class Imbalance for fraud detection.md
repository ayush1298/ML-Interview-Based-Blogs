You are in a Senior Machine Learning Interview at Google DeepMind. The interviewer sets a trap:

"We have a 1:1000 class imbalance for fraud detection. We applied ğ˜¤ğ˜­ğ˜¢ğ˜´ğ˜´_ğ˜¸ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´ to the ğ‚ğ«ğ¨ğ¬ğ¬-ğ„ğ§ğ­ğ«ğ¨ğ©ğ² loss, but the model is still missing the hard edge cases. What do we do?"

90% of candidates walk right into the wall.

Most candidates immediately suggest aggressive oversampling (ğ˜šğ˜”ğ˜–ğ˜›ğ˜Œ) or tuning the class weights even higher (e.g., 1:5000).

They think: "If the minority class is ignored, I just need to scream louder (higher weights) during backprop."

------
ğ“ğ¡ğ ğ‘ğğšğ¥ğ¢ğ­ğ²:
You aren't losing because the weights are wrong. You are losing because of ğ†ğ«ğšğğ¢ğğ§ğ­ ğƒğ«ğ¨ğ°ğ§ğ¢ğ§ğ .

Even with perfect class weights, your dataset likely contains 990,000 "easy" negatives (legitimate transactions that are obviously legit) and 1,000 "hard" positives.

In standard ğ–ğğ¢ğ ğ¡ğ­ğğ ğ‚ğ«ğ¨ğ¬ğ¬-ğ„ğ§ğ­ğ«ğ¨ğ©ğ² (ğ–ğ‚ğ„), the gradients from those 990,000 easy examples, even if individually small, sum up to dominate the update step.
 
The model spends all its capacity optimizing examples it has already learned, drowning out the signal from the difficult, subtle fraud cases.

------
The Solution: ğ“ğ¡ğ ğ„ğšğ¬ğ²-ğ„ğ±ğšğ¦ğ©ğ¥ğ ğ’ğ®ğ©ğ©ğ«ğğ¬ğ¬ğ¢ğ¨ğ§

You don't need to re-balance the counts. You need to re-balance the difficulty.
The solution is switching from ğ–ğğ¢ğ ğ¡ğ­ğğ ğ‚ğ«ğ¨ğ¬ğ¬-ğ„ğ§ğ­ğ«ğ¨ğ©ğ² to ğ…ğ¨ğœğšğ¥ ğ‹ğ¨ğ¬ğ¬.

Focal Loss adds a modulating factor (1 âˆ’ pâ‚œ)áµ to the standard loss equation. 

Here is what happens in production:
- ğ˜ğ˜§ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜ªğ˜´ ğ˜¶ğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜¦ (ğ˜ğ˜¢ğ˜³ğ˜¥ ğ˜Œğ˜¹ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦): The modulating factor stays near 1. The loss is unchanged. The model learns.
- ğ˜ğ˜§ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜ªğ˜´ ğ˜¤ğ˜°ğ˜¯ğ˜§ğ˜ªğ˜¥ğ˜¦ğ˜¯ğ˜µ (ğ˜Œğ˜¢ğ˜´ğ˜º ğ˜Œğ˜¹ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦): The factor drops to near 0. The loss contribution is effectively "shut off."

This forces the model to stop patting itself on the back for identifying the obvious negatives and focus 100% of its gradient descent budget on the edge cases.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"ğ–ğğ¢ğ ğ¡ğ­ğğ ğ‚ğ«ğ¨ğ¬ğ¬-ğ„ğ§ğ­ğ«ğ¨ğ©ğ² solves for moderate imbalance (1:10) by balancing counts. ğ…ğ¨ğœğšğ¥ ğ‹ğ¨ğ¬ğ¬ solves for extreme imbalance (1:1000+) by balancing hardness. In a fraud scenario, I would implement ğ…ğ¨ğœğšğ¥ ğ‹ğ¨ğ¬ğ¬ with Î³ = 2 to down-weight the easy negatives that are currently dominating the gradient."
