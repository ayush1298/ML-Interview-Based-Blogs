You're in a Machine Learning interview at Google. The VP hands you a fraud detection model with a 0.98 ROC AUC and asks:

"Is this model ready to ship?"

90% of candidates walk right into the trap.

The textbook answer is to look at the score and celebrate. They said:
"0.98 is phenomenal. The curve hugs the top-left corner perfectly. The separation between classes is distinct. Let's deploy."

------
ğ“ğ¡ğ ğ‘ğğšğ¥ğ¢ğ­ğ²: They aren't optimizing for a balanced dataset; they are optimizing for a massive imbalance (e.g., 1 fraud per 100,000 transactions).

The ROC Curve is chemically addicted to ğ“ğ«ğ®ğ ğğğ ğšğ­ğ¢ğ¯ğğ¬.

Because the ğ…ğšğ¥ğ¬ğ ğğ¨ğ¬ğ¢ğ­ğ¢ğ¯ğ ğ‘ğšğ­ğ (ğ…ğğ‘) uses the total number of negatives (legitimate transactions) in the denominator, a massive pool of legitimate traffic dilutes your errors.

You can have a model that misses 50% of fraud cases, but because you have 10 million legitimate transactions, the ROC line still looks perfect. It is lying to you.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
To pass the interview, you need to identify ğ“ğ¡ğ ğŒğšğ£ğ¨ğ«ğ¢ğ­ğ² ğŒğ¢ğ«ğšğ ğ.

You must switch metrics immediately to the ğ˜—ğ˜³ğ˜¦ğ˜¤ğ˜ªğ˜´ğ˜ªğ˜°ğ˜¯-ğ˜™ğ˜¦ğ˜¤ğ˜¢ğ˜­ğ˜­ (ğ˜—ğ˜™) ğ˜Šğ˜¶ğ˜³ğ˜·ğ˜¦.
- ROC includes True Negatives (the massive majority).
- PR Curve ignores True Negatives entirely.

It focuses exclusively on the minority class performance. When you switch the plot, that "perfect" 0.98 ROC often collapses into a jagged, embarrassing 0.12 PR AUC, revealing that your model is mostly hallucinating or missing the actual crime.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"ROC scores are optimistic proxies in high-imbalance domains. For fraud or rare disease detection, I ignore ğ˜™ğ˜–ğ˜Š and optimize for ğ˜ˆğ˜³ğ˜¦ğ˜¢ ğ˜œğ˜¯ğ˜¥ğ˜¦ğ˜³ ğ˜µğ˜©ğ˜¦ ğ˜—ğ˜³ğ˜¦ğ˜¤ğ˜ªğ˜´ğ˜ªğ˜°ğ˜¯-ğ˜™ğ˜¦ğ˜¤ğ˜¢ğ˜­ğ˜­ ğ˜Šğ˜¶ğ˜³ğ˜·ğ˜¦ (ğ˜ˆğ˜œğ˜—ğ˜™ğ˜Š) to ensure we aren't hiding poor recall behind a wall of easy negatives."

Also, can use the Matthews Correlation Coefficient (MCC) for an imbalanced dataset
