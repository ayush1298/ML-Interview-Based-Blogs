You're in a Senior AI Engineer interview at Google DeepMind and the interviewer asks:

"We just scaled our infrastructure to 4x our batch size (256 to 1024) to speed up training. We followed the ğ˜“ğ˜ªğ˜¯ğ˜¦ğ˜¢ğ˜³ ğ˜šğ˜¤ğ˜¢ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜™ğ˜¶ğ˜­ğ˜¦ and multiplied our ğ˜“ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜™ğ˜¢ğ˜µğ˜¦ by 4. But our test accuracy still degraded. What fundamental property of SGD did we accidentally kill?"

Most of candidates say: "We probably should have used the ğ˜šğ˜²ğ˜¶ğ˜¢ğ˜³ğ˜¦ ğ˜™ğ˜°ğ˜°ğ˜µ ğ˜´ğ˜¤ğ˜¢ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜³ğ˜¶ğ˜­ğ˜¦ instead." 
or 
"We didn't tune the ğ˜“ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜™ğ˜¢ğ˜µğ˜¦ enough."

This is the trap. It assumes the math is wrong, rather than the dynamics.

The reality is that they didn't just change the speed, they changed the ğˆğ¦ğ©ğ¥ğ¢ğœğ¢ğ­ ğ‘ğğ ğ®ğ¥ğšğ«ğ¢ğ³ğšğ­ğ¢ğ¨ğ§.

When they increase the batch size, they reduce ğ˜ğ˜³ğ˜¢ğ˜¥ğ˜ªğ˜¦ğ˜¯ğ˜µ ğ˜•ğ˜°ğ˜ªğ˜´ğ˜¦. And in Deep Learning, noise is often a feature, not a bug.

Here is the trade-off:
1ï¸âƒ£ ğ˜šğ˜®ğ˜¢ğ˜­ğ˜­ ğ˜‰ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜¦ğ˜´: Produce "noisy" gradient estimates. This noise acts like thermal motion, kicking your model out of ğ’ğ¡ğšğ«ğ© ğŒğ¢ğ§ğ¢ğ¦ğš (which generalize poorly) and pushing it toward ğ…ğ¥ğšğ­ ğŒğ¢ğ§ğ¢ğ¦ğš (which generalize well).

2ï¸âƒ£ ğ˜“ğ˜¢ğ˜³ğ˜¨ğ˜¦ ğ˜‰ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜¦ğ˜´: Average out that noise. The path becomes "too smooth." The model converges straight into the nearest sharp basin and gets stuck there.

By 4x-ing your batch size, you suppressed the stochasticity that helps the model explore. You made the optimization path too perfect, causing it to overfit the specific geometry of the training loss rather than finding a robust solution.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"Large batch training reduces the variance of the gradient estimate, effectively killing the noise needed to escape sharp basins. To fix this, you don't just scale the Learning Rate, you must implement a ğ‹ğ¢ğ§ğğšğ« ğ–ğšğ«ğ¦ğ®ğ© phase to stabilize the early training dynamics before the noise dampens."
