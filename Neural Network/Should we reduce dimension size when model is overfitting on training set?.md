You're in a Senior MLE interview at Google DeepMind. The interviewer sets a trap:

"Our new foundation model is overfitting severely on the training set. Should we cut the hidden dimension size from 4096 to 1024 to limit its capacity?"

90% of candidates walk right into it.

The candidates say: "Yes. Overfitting means the model has too much capacity, it's memorizing noise instead of learning patterns. We should reduce the number of parameters (neurons/layers) to force generalization."

It feels logical as a textbook answer. It's also the wrong architectural move.

The reality is that they aren't optimizing for parameter efficiency, they are optimizing for the loss landscape.

When you starve a network by reducing its size, you aren't just preventing overfitting, you are creating a harder optimization problem. Smaller networks have complex, non-convex loss landscapes filled with nasty local minima. They struggle to converge at all.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: The Senior Engineer knows the real rule of thumb: ğ˜•ğ˜¦ğ˜·ğ˜¦ğ˜³ ğ˜¶ğ˜´ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜¢ğ˜´ ğ˜¢ ğ˜³ğ˜¦ğ˜¨ğ˜¶ğ˜­ğ˜¢ğ˜³ğ˜ªğ˜»ğ˜¦ğ˜³.

Instead, you lean into ğ“ğ¡ğ ğğ¯ğğ«-ğğšğ«ğšğ¦ğğ­ğğ«ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğğšğ«ğšğğ¨ğ±.

You keep the massive architecture (or even make it bigger). You want the high capacity to ensure the model has the power to learn complex boundaries easily. Then, you aggressively constrain the weights using actual regularizers.
- Keep the 4096 hidden units.
- Crank up the regularization (ğ˜“ğ˜¢ğ˜®ğ˜£ğ˜¥ğ˜¢/ğ˜ğ˜¦ğ˜ªğ˜¨ğ˜©ğ˜µ ğ˜‹ğ˜¦ğ˜¤ğ˜¢ğ˜º) or ğ˜‹ğ˜³ğ˜°ğ˜±ğ˜°ğ˜¶ğ˜µ.

We trade the risk of "underfitting due to small size" for the manageable challenge of "tuning Î»."

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"I wouldn't touch the architecture size. It's easier to regularize a large model than to train a small one. Iâ€™d keep the capacity high to ensure learnability, then increase the regularization term Î» to penalize the weights until generalization improves."
