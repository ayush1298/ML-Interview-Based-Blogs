You are in a Senior Machine Learning interview at OpenAI. The interviewer sets a quiet trap:

"We implemented a custom Dropout layer from scratch. How do you handle it during inference?"

90% of candidates walk right into the trap.

Most candidates immediately answer: "Simple. You just turn off the random masking. We need deterministic results in production, so we use all the weights as they are."

It feels intuitive. It's also catastrophic.

If they answer this way, their modelâ€™s predictions in production will be garbage.

Why? ğŒğšğ ğ§ğ¢ğ­ğ®ğğ ğŒğ¢ğ¬ğ¦ğšğ­ğœğ¡.

During training, if they drop 50% of their neurons (p=0.5), the next layer learns to expect a signal sum based on only half the active inputs.

If they suddenly turn all the neurons on during inference without adjustment, the total input to the next layer doubles. Their activations explode, pushing their neurons into saturation (if using Tanh/Sigmoid) or blowing up their logits (if using ReLU), causing numerical instability.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: To fix this, you must solve ğ“ğ¡ğ ğ„ğ±ğ©ğğœğ­ğšğ­ğ¢ğ¨ğ§ ğ†ğšğ©.

You cannot just "turn it off." You have to preserve the expected magnitude of the signal. You have two architectural choices:
- ğ˜ğ˜¯ğ˜§ğ˜¦ğ˜³ğ˜¦ğ˜¯ğ˜¤ğ˜¦ ğ˜šğ˜¤ğ˜¢ğ˜­ğ˜ªğ˜¯ğ˜¨ (ğ˜›ğ˜©ğ˜¦ ğ˜–ğ˜­ğ˜¥ ğ˜ğ˜¢ğ˜º): At test time, multiply all outgoing weights by p. If you kept 50% of neurons during training, you scale outputs by 0.5 to match the training magnitude.
- ğ˜ğ˜¯ğ˜·ğ˜¦ğ˜³ğ˜µğ˜¦ğ˜¥ ğ˜‹ğ˜³ğ˜°ğ˜±ğ˜°ğ˜¶ğ˜µ (ğ˜›ğ˜©ğ˜¦ ğ˜—ğ˜³ğ˜°ğ˜¥ğ˜¶ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ğ˜¢ğ˜º): You scale the activations by 1/(1-p) during training. This artificially boosts the signal during the training pass so that it matches the "full" network magnitude.

Senior Engineers prefer Method #2 because it leaves the inference path clean, stateless, and unburdened by extra computation.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ
"I use ğˆğ§ğ¯ğğ«ğ­ğğ ğƒğ«ğ¨ğ©ğ¨ğ®ğ­. By scaling activations by 1/(1-p) during the training phase, I ensure the expected magnitude remains consistent. This allows me to simply remove the mask during inference without touching the weights or risking numerical instability."
