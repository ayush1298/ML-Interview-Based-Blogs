You're in a Senior Machine Learning interview at OpenAI. The interviewer hands you a marker and asks for a scratch implementation of Softmax Regression.

It feels like a "FizzBuzz" question. It isn't. It's a trap.

90% of candidates walk right into it by writing "clean," modular code.

They write the forward pass like this:
probs = softmax(logits)
loss = -log(probs)

It makes sense logically. You compute the probabilities (the hypothesis), and then you compute the error (the loss). It separates concerns. It looks like "good" software engineering.

-----
ğ“ğ¡ğ ğ“ğ®ğ«ğ§:
In production, you just killed the training run. Welcome to IEEE 754 floating-point hell.

If your model is confident and pushes a logit to a large negative number (say, -100), the softmax function will mathematically output a very small number 10^(-45).

But your GPU doesn't have infinite precision. It rounds that small number down to hard 0.0 (Underflow). The next line attempts log(0.0).

Result? -inf.

Backprop gradient? NaN.

The model explodes.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
You need to apply what I call ğ“ğ¡ğ ğ‹ğ¨ğ ğ’ğ®ğ¦ğ„ğ±ğ© ğ…ğ®ğ¬ğ¢ğ¨ğ§.
Senior Engineers know that you never, ever compute the probability explicitly before the loss if you can avoid it. You must fuse the layers.

Mathematically, the log of a softmax can be simplified:

log( e^{záµ¢} / âˆ‘ e^{zâ±¼} ) = záµ¢ âˆ’ log( âˆ‘ e^{zâ±¼} )

By calculating the loss directly from the raw logits (z), you bypass the division and the explicit probability calculation. This allows you to use the "ğ‹ğ¨ğ -ğ’ğ®ğ¦-ğ„ğ±ğ©" ğ­ğ«ğ¢ğœğ¤, which factors out the largest term to prevent overflow/underflow.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:
"I don't separate Softmax and Log. I use ğ˜¤ğ˜³ğ˜°ğ˜´ğ˜´_ğ˜¦ğ˜¯ğ˜µğ˜³ğ˜°ğ˜±ğ˜º_ğ˜¸ğ˜ªğ˜µğ˜©_ğ˜­ğ˜°ğ˜¨ğ˜ªğ˜µğ˜´. It fuses the operations to guarantee numerical convexity and prevents NaN explosions during mixed-precision training."
