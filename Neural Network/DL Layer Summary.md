### üß† **Deep Learning Layer Summary Table**

| **Layer Type**                       | **Input Shape**            | **Output Shape**                   | **Trainable Parameters**                                                                            | **Non-Trainable Parameters**        | **Notes / Explanation**                                                                  |
| ------------------------------------ | -------------------------- | ---------------------------------- | --------------------------------------------------------------------------------------------------- | ----------------------------------- | ---------------------------------------------------------------------------------------- |
| **Fully Connected (Dense)**          | `(N_in,)`                  | `(N_out,)`                         | `N_in √ó N_out + N_out` (weights + biases)                                                           | ‚Äì                                   | Every input neuron connects to every output neuron. Common in MLPs and classifier heads. |
| **Convolution (Conv2D)**             | `(H_in, W_in, C_in)`       | `(H_out, W_out, K)`                | `F √ó F √ó C_in √ó K + K` (weights + biases)                                                           | ‚Äì                                   | Each of the `K` filters learns `F√óF√óC_in` weights. Output has `K` channels.              |
| **Depthwise Conv2D**                 | `(H_in, W_in, C_in)`       | `(H_out, W_out, C_in)`             | `F √ó F √ó C_in + C_in`                                                                               | ‚Äì                                   | One filter per input channel. Often followed by pointwise conv (`1√ó1`) to mix channels.  |
| **Pointwise Conv (1√ó1 Conv)**        | `(H_in, W_in, C_in)`       | `(H_in, W_in, K)`                  | `1 √ó 1 √ó C_in √ó K + K`                                                                              | ‚Äì                                   | Used for channel mixing or dimensionality reduction.                                     |
| **Batch Normalization**              | `(any)`                    | `(same as input)`                  | `2 √ó C` (`Œ≥` and `Œ≤` for each channel)                                                              | `2 √ó C` (running mean and variance) | Normalizes activations to stabilize training.                                            |
| **Pooling (Max / Avg Pool)**         | `(H_in, W_in, C_in)`       | `(H_out, W_out, C_in)`             | ‚Äì                                                                                                   | ‚Äì                                   | Reduces spatial dimensions; no learnable parameters.                                     |
| **Flatten**                          | `(H, W, C)`                | `(H√óW√óC,)`                         | ‚Äì                                                                                                   | ‚Äì                                   | Converts 3D feature maps into a 1D vector before FC layers.                              |
| **Dropout**                          | `(any)`                    | `(same as input)`                  | ‚Äì                                                                                                   | ‚Äì                                   | Randomly drops units during training; no parameters.                                     |
| **Activation (ReLU, Sigmoid, etc.)** | `(any)`                    | `(same as input)`                  | ‚Äì                                                                                                   | ‚Äì                                   | Applies element-wise nonlinearity; purely functional.                                    |
| **Residual / Skip Connection**       | `(same input/output dims)` | `(same)`                           | ‚Äì                                                                                                   | ‚Äì                                   | Adds or concatenates activations from earlier layers; improves gradient flow.            |
| **Embedding Layer**                  | `(sequence_length,)`       | `(sequence_length, embedding_dim)` | `Vocab_size √ó embedding_dim`                                                                        | ‚Äì                                   | Maps discrete tokens to dense continuous vectors.                                        |
| **RNN / LSTM / GRU**                 | `(seq_len, input_dim)`     | `(seq_len, hidden_dim)`            | Complex: depends on gates; e.g. LSTM has `4 √ó (input_dim + hidden_dim) √ó hidden_dim + 4√óhidden_dim` | ‚Äì                                   | Used for sequence modeling. Parameters from gates (input, forget, output).               |
| **Softmax**                          | `(N,)`                     | `(N,)`                             | ‚Äì                                                                                                   | ‚Äì                                   | Converts logits into probabilities that sum to 1.                                        |

---

### ‚öôÔ∏è **Quick Reference Notes**

* **Trainable Parameters:** learned through backprop (weights, biases, BN Œ≥ & Œ≤, embedding weights).
* **Non-Trainable Parameters:** updated during training but not learned via gradients (e.g. running mean/var in BatchNorm).
* **Activations:** Output of each layer (shape = Output shape).
* **Activation size:** product of all dimensions in Output shape.

---
