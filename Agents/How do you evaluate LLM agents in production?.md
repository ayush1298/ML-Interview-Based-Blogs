You're in a ML Engineer interview at Microsoft, and the interviewer asks: "How do you evaluate LLM agents in production? What makes agent evaluation different from regular LLM evaluation?"

Here's how you can answer
A. Most candidates fail here because they think agents = chatbots

âŒ Wrong. Dead wrong.

Evaluating an LLM agent isn't about measuring "answer quality" or "response time." That's regular LLM evaluation. Agents are autonomous systems that plan, use tools, and make decisions across multiple steps.

âœ… Your evaluation framework needs to match that complexity.

A. ğ—˜ğ—»ğ—±-ğ˜ğ—¼-ğ—˜ğ—»ğ—± ğ˜ƒğ˜€ ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜-ğ—Ÿğ—²ğ˜ƒğ—²ğ—¹ ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—» - The difference that matter
âŒ Junior engineers only test end-to-end. âœ… Senior engineers test both layers:
End-to-End: Did the agent complete the task? Treats the system as a black box. Good for catching surface issues.

Component-Level: Which specific component failed? The retriever? The tool call? The reasoning step? Essential for debugging complex agents. LLM Agent Evaluation: Assessing Tool Use, Task ...

Here's the kicker: An agent might have a multi-retriever setup, and end-to-end testing will only tell you "retrieval needs improvement" â€” but won't tell you if Retriever A or Retriever B is the bottleneck. LLM Agent Evaluation: Assessing Tool Use, Task ...

Component-level testing finds the lowest-hanging fruit.

B. ğ—§ğ—¼ğ—¼ğ—¹-ğ—–ğ—®ğ—¹ğ—¹ğ—¶ğ—»ğ—´ ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®tion - The make-or-break metric for Level 2 agents
Your agent has 12 tools available. It needs to book a flight. Instead, it checks the weather, converts currency, THEN searches for flights. LLM Agent Evaluation: Assessing Tool Use, Task ...
Technically correct. Practically unusable.
You need TWO critical metrics:

1. ğ—§ğ—¼ğ—¼ğ—¹ ğ—–ğ—¼ğ—¿ğ—¿ğ—²ğ—°ğ˜ğ—»ğ—²ğ˜€ğ˜€ - Did the agent call the RIGHT tools? LLM Agent Evaluation: Assessing Tool Use, Task ...
This breaks into 3 levels of strictness:
âœ³ï¸ Tool Selection: Were the correct tools chosen?
âœ³ï¸ Input Parameters: Were the correct arguments passed?
âœ³ï¸ Output Accuracy: Did the tools return expected results?

Tool Correctness doesn't have to be binary. Order might not matter. Frequency might be flexible. A medical AI agent could query "patient symptom checker" after "medical history database" instead of before â€” as long as both tools are used correctly. LLM Agent Evaluation: Assessing Tool Use, Task ...

2. ğ—§ğ—¼ğ—¼ğ—¹ ğ—˜ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ—°ğ˜† - Did the agent take the shortest path?
Deterministic methods:
âœ³ï¸ Redundant Tool Usage: % of unnecessary tool calls
âœ³ï¸ Tool Frequency: Tools called more than required threshold
But here's the problem: Tool-calling behavior gets branched, nested, and convoluted fast. LLM Agent Evaluation: Assessing Tool Use, Task ...

Better approach? Use an LLM-as-judge to evaluate if the tool trajectory was the most efficient method given available tools.

Evaluation isn't a checkpoint. It's the foundation. LLM Agent Evaluation: Assessing Tool Use, Task ...
