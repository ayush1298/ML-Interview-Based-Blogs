You're in a GenAI Engineer interview at Morgan Stanley, and the interviewer asks:
"We're building an agentic trading assistant that needs to remember client preferences, track portfolio decisions, and adapt strategy over months. Should we use short-term or long-term memory? Justify your architecture."

Here's how you can answer:
A. Most candidates fumble here because they only know "agents need memory." Incomplete answer.
B. There are 4 critical factors every GenAI engineer should understand cold.

ğŸ­. ğ—§ğ—µğ—² ğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ—ªğ—¶ğ—»ğ—±ğ—¼ğ˜„ ğ—§ğ—¿ğ—®ğ—±ğ—²ğ—¼ğ—³ğ—³ - ğ—§ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—²
Short-term memory (STM) operates ENTIRELY within the context window:

Duration: Seconds to minutes (single conversation session)
Storage: In-prompt, ephemeral state management
Capacity: Limited by token budget (8K-200K tokens typical)

Long-term memory (LTM) persists ACROSS sessions and conversations:

Duration: Days to years (persistent across sessions)
Storage: External databases (vector stores, knowledge graphs)
Capacity: Virtually unlimited (millions of entities, relationships)

The brutal truth? STM is real-time coherence. LTM is actual intelligence.

ğŸ®. ğ—§ğ—µğ—² ğ—–ğ—¼ğ˜€ğ˜ ğ—™ğ—¼ğ—¼ğ˜ğ—½ğ—¿ğ—¶ğ—»ğ˜ - ğ—ªğ—µğ—²ğ—¿ğ—² ğŸµğŸ¬% ğ—¼ğ—³ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ˜€ ğ—´ğ—¼ ğ˜„ğ—¿ğ—¼ğ—»ğ—´
Most people think "just stuff everything in the context window."
Wrong move.
STM token costs compound with EVERY turn
LTM has fixed retrieval costs

But here's the catch - you STILL need both working together.
Real-world production costs? Hybrid STM+LTM cuts token usage 60-80% while improving quality.

3. ğ—§ğ—µğ—² ğ—¦ğ˜ğ—®ğ˜ğ—² ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜ ğ—§ğ—¿ğ—®ğ—±ğ—² - ğŸ±ğ˜… ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜…ğ—¶ğ˜ğ˜†, ğ—¯ğ˜‚ğ˜ ğ˜„ğ—µğ˜†?
STM: Zero infrastructure overhead
Pass conversation history in every API call

LTM: Production engineering required
 - Memory extraction: Which facts are worth storing?
 - Memory consolidation: Update vs. append strategies
 - Memory retrieval: Relevance scoring and ranking
 - Memory decay: When to forget outdated information

4. ğ—§ğ—µğ—² ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜ ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜† - ğ—§ğ—µğ—² ğ—°ğ—¼ğ˜€ğ˜ ğ—»ğ—¼ğ—¯ğ—¼ğ—±ğ˜† ğ˜ğ—®ğ—¹ğ—¸ğ˜€ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜
STM: Simple but brittle
LTM: Complex but scalable


ğ—ªğ—µğ—²ğ—» ğ—¦ğ—µğ—¼ğ—¿ğ˜-ğ—§ğ—²ğ—¿ğ—º ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ˜„ğ—¶ğ—»ğ˜€:
âœ… Single-session tasks (document Q&A, one-time code generation)
âœ… Simple chatbots with no personalization requirements
âœ… Prototyping and demos
âœ… No compliance requirements for data persistence

ğ—ªğ—µğ—²ğ—» ğ—Ÿğ—¼ğ—»ğ—´-ğ—§ğ—²ğ—¿ğ—º ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ˜„ğ—¶ğ—»ğ˜€:
âœ… Multi-session workflows (customer support, personal assistants)
âœ… Personalization is critical (user preferences, behavioral patterns)
âœ… Temporal reasoning required (tracking changes over time)
âœ… Complex relationship modeling (enterprise knowledge graphs)


The Final Answer for Your Interview:
"For a production trading assistant, I'd implement a hybrid memory architecture:
Short-term memory handles the current conversation flow.
Long-term memory captures client profiles, portfolio decisions, and strategy patterns. "
