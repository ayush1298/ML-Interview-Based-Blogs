You are in a Senior Computer Vision interview at OpenAI. The interviewer sets a classic trap:

"In VGGNet, we replace a single 7x7 convolution with a stack of three 3x3 convolutions. Why?"

90% of candidates walk right into the trap.

They grab the whiteboard marker and start doing arithmetic.

They say: "It's about efficiency and parameter reduction. A 7x7 filter has 49 weights (7^2). Three 3x3 filters have 27 weights (3x3^2). So, we get the same 7x7 receptive field with 45% fewer parameters. Itâ€™s a memory optimization."

The interviewer nods politely. They didn't get the job.

ğ˜›ğ˜©ğ˜¦ ğ˜³ğ˜¦ğ˜¢ğ˜­ğ˜ªğ˜µğ˜º ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜µğ˜©ğ˜¦ğ˜º ğ˜¢ğ˜³ğ˜¦ğ˜¯'ğ˜µ ğ˜°ğ˜±ğ˜µğ˜ªğ˜®ğ˜ªğ˜»ğ˜ªğ˜¯ğ˜¨ ğ˜§ğ˜°ğ˜³ ğ˜´ğ˜µğ˜°ğ˜³ğ˜¢ğ˜¨ğ˜¦. ğ˜›ğ˜©ğ˜¦ğ˜º ğ˜¢ğ˜³ğ˜¦ ğ˜°ğ˜±ğ˜µğ˜ªğ˜®ğ˜ªğ˜»ğ˜ªğ˜¯ğ˜¨ ğ˜§ğ˜°ğ˜³ ğ˜¦ğ˜¹ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜´ğ˜ªğ˜·ğ˜ªğ˜µğ˜º.

If the goal was purely parameter reduction, there are dozen ways to factorize matrices. The answer ignores the single most important component of Deep Learning: ğ“ğ¡ğ ğ€ğœğ­ğ¢ğ¯ğšğ­ğ¢ğ¨ğ§ ğ…ğ®ğ§ğœğ­ğ¢ğ¨ğ§.

A single 7x7 layer applies one linear transformation followed by one ReLU. It can only model a simple linear relationship within that 7x7 pixel patch.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: By stacking three layers, you aren't just covering space, you are injecting non-linearity. You are utilizing ğ“ğ¡ğ ğğ¨ğ§ğ¥ğ¢ğ§ğğšğ« ğˆğ§ğ£ğğœğ­ğ¢ğ¨ğ§ ğ„ğŸğŸğğœğ­.

By using a stack of three 3x3s, you intersperse three separate ReLU functions into the same effective receptive field.
* Layer 1 (3x3) + ReLU
* Layer 2 (3x3) + ReLU
* Layer 3 (3x3) + ReLU

Instead of a single linear decision boundary, you now have a highly complex, multi-stage discriminator looking at the exact same patch of pixels. You have made the network deeper and more discriminative while simultaneously making it lighter.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"The parameter reduction is a bonus, but the real optimization is discriminative power. Stacking smaller filters allows us to inject three non-linearities (ReLUs) instead of one, enabling the model to learn significantly more complex features within the same receptive field."
