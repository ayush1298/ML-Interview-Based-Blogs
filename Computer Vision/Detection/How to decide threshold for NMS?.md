ğ™ğ™ğ™š ğ™‰ğ™ˆğ™ ğ™ğ™ªğ™£ğ™ğ™£ğ™œ ğ™ƒğ™šğ™¡ğ™¡ ğ™ğ™§ğ™–ğ™¥ ğŸ”¥

You're in a Computer Vision interview at Tesla The interviewer asks:

"ğ˜–ğ˜¶ğ˜³ ğ˜°ğ˜£ğ˜«ğ˜¦ğ˜¤ğ˜µ ğ˜¥ğ˜¦ğ˜µğ˜¦ğ˜¤ğ˜µğ˜°ğ˜³ ğ˜§ğ˜ªğ˜¯ğ˜¥ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜¢ğ˜³ğ˜´, ğ˜£ğ˜¶ğ˜µ ğ˜ªğ˜µ ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µğ˜´ 5 ğ˜°ğ˜·ğ˜¦ğ˜³ğ˜­ğ˜¢ğ˜±ğ˜±ğ˜ªğ˜¯ğ˜¨ ğ˜£ğ˜°ğ˜¹ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜¦ğ˜·ğ˜¦ğ˜³ğ˜º ğ˜´ğ˜ªğ˜¯ğ˜¨ğ˜­ğ˜¦ ğ˜¤ğ˜¢ğ˜³. ğ˜ğ˜¦ ğ˜µğ˜¶ğ˜¯ğ˜¦ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜•ğ˜°ğ˜¯-ğ˜”ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® ğ˜šğ˜¶ğ˜±ğ˜±ğ˜³ğ˜¦ğ˜´ğ˜´ğ˜ªğ˜°ğ˜¯ (ğ˜•ğ˜”ğ˜š) ğ˜µğ˜©ğ˜³ğ˜¦ğ˜´ğ˜©ğ˜°ğ˜­ğ˜¥, ğ˜£ğ˜¶ğ˜µ ğ˜¯ğ˜°ğ˜¸ ğ˜ªğ˜µ ğ˜¥ğ˜¦ğ˜­ğ˜¦ğ˜µğ˜¦ğ˜´ ğ˜·ğ˜¢ğ˜­ğ˜ªğ˜¥ ğ˜¤ğ˜¢ğ˜³ğ˜´ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¢ğ˜³ğ˜¦ ğ˜±ğ˜¢ğ˜³ğ˜¬ğ˜¦ğ˜¥ ğ˜¤ğ˜­ğ˜°ğ˜´ğ˜¦ ğ˜µğ˜°ğ˜¨ğ˜¦ğ˜µğ˜©ğ˜¦ğ˜³. ğ˜ğ˜°ğ˜¸ ğ˜¥ğ˜° ğ˜¸ğ˜¦ ğ˜§ğ˜ªğ˜¹ ğ˜µğ˜©ğ˜¦ ğ˜µğ˜©ğ˜³ğ˜¦ğ˜´ğ˜©ğ˜°ğ˜­ğ˜¥?"

ğŸ•¸ï¸ 90% of candidates walk right into the trap.

They say: "ğ˜ ğ˜°ğ˜¶ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ ğ˜¢ 'ğ˜šğ˜°ğ˜§ğ˜µ-ğ˜•ğ˜”ğ˜š' ğ˜°ğ˜³ ğ˜¢ğ˜¯ ğ˜¢ğ˜¥ğ˜¢ğ˜±ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜µğ˜©ğ˜³ğ˜¦ğ˜´ğ˜©ğ˜°ğ˜­ğ˜¥ ğ˜£ğ˜¢ğ˜´ğ˜¦ğ˜¥ ğ˜°ğ˜¯ ğ˜¥ğ˜¦ğ˜¯ğ˜´ğ˜ªğ˜µğ˜º."

They just signed up for a lifetime of heuristic tuning.

The Reality: NMS is a Heuristic Hack.

NMS assumes that "overlapping boxes = duplicate detections." In dense scenes (crowds, parking lots), this assumption breaks. A car blocking another car ğ˜´ğ˜©ğ˜°ğ˜¶ğ˜­ğ˜¥ have an overlapping box. NMS mathematically cannot distinguish between "duplicate prediction" and "occluded object." You are fighting the limitations of the post-processing, not the model.

âœ… The Solution: Stop removing boxes. Start predicting Sets.

The senior solution is to move to ğ—˜ğ—»ğ—±-ğ˜ğ—¼-ğ—˜ğ—»ğ—± ğ—¢ğ—¯ğ—·ğ—²ğ—°ğ˜ ğ——ğ—²ğ˜ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» (ğ——ğ—˜ğ—§ğ—¥).

â€¢ Set Prediction: Instead of predicting thousands of candidate boxes and filtering them, the Transformer predicts a fixed set of N objects directly.
â€¢ Bipartite Matching: During training, it uses a Hungarian Loss to assign one predicted box to one ground-truth object uniquely.
â€¢ The model ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜´ to not output duplicates. It learns that "two boxes on the same car increases loss."
 

âœï¸ ğ—§ğ—µğ—² ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—§ğ—µğ—®ğ˜ ğ—šğ—²ğ˜ğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—›ğ—¶ğ—¿ğ—²ğ—±:
"ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ğ˜¯'ğ˜µ ğ˜µğ˜¶ğ˜¯ğ˜¦ ğ˜µğ˜©ğ˜¦ ğ˜•ğ˜”ğ˜š. ğ˜•ğ˜”ğ˜š ğ˜ªğ˜´ ğ˜¢ ğ˜£ğ˜°ğ˜µğ˜µğ˜­ğ˜¦ğ˜¯ğ˜¦ğ˜¤ğ˜¬ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜§ğ˜¢ğ˜ªğ˜­ğ˜´ ğ˜ªğ˜¯ ğ˜¥ğ˜¦ğ˜¯ğ˜´ğ˜¦ ğ˜°ğ˜¤ğ˜¤ğ˜­ğ˜¶ğ˜´ğ˜ªğ˜°ğ˜¯. ğ˜ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜´ğ˜¸ğ˜ªğ˜µğ˜¤ğ˜© ğ˜µğ˜° ğ˜¢ ğ˜›ğ˜³ğ˜¢ğ˜¯ğ˜´ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¦ğ˜³-ğ˜£ğ˜¢ğ˜´ğ˜¦ğ˜¥ ğ˜¢ğ˜³ğ˜¤ğ˜©ğ˜ªğ˜µğ˜¦ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦ ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜‹ğ˜Œğ˜›ğ˜™ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¶ğ˜´ğ˜¦ğ˜´ ğ˜£ğ˜ªğ˜±ğ˜¢ğ˜³ğ˜µğ˜ªğ˜µğ˜¦ ğ˜®ğ˜¢ğ˜µğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¨ ğ˜­ğ˜°ğ˜´ğ˜´. ğ˜›ğ˜©ğ˜ªğ˜´ ğ˜§ğ˜°ğ˜³ğ˜¤ğ˜¦ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ ğ˜µğ˜° ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ ğ˜°ğ˜¯ğ˜¦-ğ˜µğ˜°-ğ˜°ğ˜¯ğ˜¦ ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜¨ğ˜¯ğ˜®ğ˜¦ğ˜¯ğ˜µ, ğ˜¦ğ˜­ğ˜ªğ˜®ğ˜ªğ˜¯ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜¥ğ˜¶ğ˜±ğ˜­ğ˜ªğ˜¤ğ˜¢ğ˜µğ˜¦ ğ˜±ğ˜³ğ˜¦ğ˜¥ğ˜ªğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ ğ˜§ğ˜°ğ˜³ ğ˜•ğ˜”ğ˜š ğ˜¦ğ˜¯ğ˜µğ˜ªğ˜³ğ˜¦ğ˜­ğ˜º."
