You're in a Senior AI Interview at OpenAI. The interviewer sets a trap:

"Our RAG pipeline has perfect retrieval (Recall@5 > 0.95). The relevant chunks are in the context window. Yet, the model still hallucinates information that isn't in the text. Why?"

90% of candidates walk right into it.

They say: "We need better embeddings."
or "Maybe the chunk size is too small?"
or "Let's fine-tune the model on the data."

They try to fix the retrieval. But the interviewer just told them the retrieval is perfect.

The reality is they aren't fighting ğ˜¢ ğ˜¥ğ˜¢ğ˜µğ˜¢ ğ˜±ğ˜³ğ˜°ğ˜£ğ˜­ğ˜¦ğ˜®. They are fighting ğ˜¢ ğ˜—ğ˜³ğ˜ªğ˜°ğ˜³-ğ˜‰ğ˜ªğ˜¢ğ˜´ ğ˜±ğ˜³ğ˜°ğ˜£ğ˜­ğ˜¦ğ˜®.

LLMs are probabilistic, not deterministic. When they provide a context chunk, the model does not treat it as a database constraint. It treats it as a weak suggestion.

If the model's pre-trained weights (its "priors") conflict with their retrieved context, or if the context is slightly ambiguous, the model will prioritize its own internal knowledge (hallucination) over their provided text.

To the model, their RAG context is just noise in the prompt.

-----
ğ“ğ¡ğ ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§: You stop optimizing for Recall and start optimizing for Adherence. You introduce what I call The ğ‚ğ¨ğ§ğ¬ğ­ğ«ğšğ¢ğ§ğ­ ğ†ğšğ­ğ.

Instead of hoping the model uses the context, you architecturally force it to:
1ï¸âƒ£ ğ˜šğ˜°ğ˜¶ğ˜³ğ˜¤ğ˜¦-ğ˜ˆğ˜µğ˜µğ˜³ğ˜ªğ˜£ğ˜¶ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜‹ğ˜¦ğ˜¤ğ˜°ğ˜¥ğ˜ªğ˜¯ğ˜¨: You require the model to output a specific [Span ID] from the retrieved chunk for every claim. No ID = The generation is blocked.

2ï¸âƒ£ ğ˜›ğ˜°ğ˜¬ğ˜¦ğ˜¯-ğ˜“ğ˜¦ğ˜·ğ˜¦ğ˜­ ğ˜ğ˜¶ğ˜ªğ˜¥ğ˜¢ğ˜¯ğ˜¤ğ˜¦: You use grammar-constrained decoding (like JSON mode) to force the output to adhere to a strict schema that includes a verification_score field.

3ï¸âƒ£ ğ˜•ğ˜¦ğ˜¨ğ˜¢ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜—ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜°ğ˜¯ ğ˜—ğ˜³ğ˜ªğ˜°ğ˜³ğ˜´: You explicitly penalize the model for accessing internal knowledge bases for this specific query type.

ğ“ğ¡ğ ğ€ğ§ğ¬ğ°ğğ« ğ“ğ¡ğšğ­ ğ†ğğ­ğ¬ ğ˜ğ¨ğ® ğ‡ğ¢ğ«ğğ:

"Retrieval is only half of RAG. The other half is Alignment. I don't just fetch the truth, I implement decoding constraints that force the model to prefer the retrieved context over its pre-trained hallucinations."
