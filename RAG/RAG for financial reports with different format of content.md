You're in an ML Engineer interview for Jane street and interviewer ask:
"Our customers upload 10,000+ complex PDFs dailyâ€”financial reports with tables, charts, multi-column layouts. How do you build a RAG system that actually works?"

Here's what separates you from other ML engineers:

ğŸš¨ ğ—§ğ—µğ—² ğ—–ğ—¼ğ—¿ğ—² ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º: Why Traditional RAG Fails
Traditional pipeline:
PDF â†’ Text Extraction â†’ Chunk â†’ Embed â†’ Retrieve
This breaks catastrophically because:
ğŸ“Š Tables split across chunks = garbage
ğŸ–¼ï¸ Images contain critical information = lost
ğŸ“„ Layout destroyed = broken context

ğŸ—ï¸ ğ—§ğ—µğ—² ğ—£ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² (ğŸ± ğ—–ğ—¿ğ—¶ğ˜ğ—¶ğ—°ğ—®ğ—¹ ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜ğ˜€)
1ï¸âƒ£ ğ——ğ—¼ğ—°ğ˜‚ğ—ºğ—²ğ—»ğ˜ ğ—Ÿğ—®ğ˜†ğ—¼ğ˜‚ğ˜ ğ—”ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ—¶ğ˜€ - ğ—§ğ—µğ—² ğ—™ğ—¼ğ˜‚ğ—»ğ—±ğ—®ğ˜ğ—¶ğ—¼ğ—»
The brutal truth about PDF parsing:

PyPDF/pdfplumber works for <10% of real PDFs âŒ
They extract in PDF object order, NOT reading order
Result: Scrambled text, broken tables, missing context

Production solution: Layout Detection Models

Multimodal: Text + Layout + Image features
Detects: Text blocks, tables, figures, titles

ğŸ®ï¸âƒ£ ğ—§ğ—®ğ—¯ğ—¹ğ—²ğ˜€ - ğ—ªğ—µğ—²ğ—¿ğ—² ğŸµğŸ±% ğ—¼ğ—³ ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ˜€ ğ—™ğ—®ğ—¶ğ—¹
âŒ Wrong approach:
Text extraction: "Q1 Revenue 2023 2024 Product A 100M 150M"
â†’ LLMs can't reason over this garbage
âœ… Right approach: Table Structure Recognition

ğŸ¯ï¸âƒ£ ğ—œğ—ºğ—®ğ—´ğ—²ğ˜€ & ğ—–ğ—µğ—®ğ—¿ğ˜ğ˜€ - ğ—©ğ—¶ğ˜€ğ—¶ğ—¼ğ—»-ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€
The problem: Charts, diagrams, screenshots contain critical insights that text embeddings can't capture
Production VLM Pipeline:

âœ… Solution: Include Â±2 paragraphs of surrounding text for context

ğŸ°ï¸âƒ£ ğ—–ğ—¼ğ—¹ğ—£ğ—®ğ—¹ğ—¶ - ğ—§ğ—µğ—² ğŸ®ğŸ¬ğŸ®ğŸ° ğ—šğ—®ğ—ºğ—² ğ—–ğ—µğ—®ğ—»ğ—´ğ—²ğ—¿ ğŸš€
Traditional RAG: Text extraction â†’ Chunking â†’ Embedding
ColPali: Skip all that. Embed document images DIRECTLY.
How it works:
ğŸ“¸ Input: Raw page images (no text extraction!)
ğŸ§  Model: PaliGemma (Vision encoder + Language decoder)
ğŸ¯ Output: Multi-vector representation (~1024 vectors/page)

ğŸ±ï¸âƒ£ ğ—–ğ—µğ˜‚ğ—»ğ—¸ğ—¶ğ—»ğ—´ ğ—¦ğ˜ğ—¿ğ—®ğ˜ğ—²ğ—´ğ˜† - ğ—§ğ—µğ—² ğ— ğ—®ğ—¸ğ—²-ğ—¼ğ—¿-ğ—•ğ—¿ğ—²ğ—®ğ—¸ ğ——ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—»
Question that defines your seniority:
How do you chunk a 200-page PDF with tables, images, and nested sections?

Semantic chunking (the right way):
Instead of splitting every 512 tokens, use sentence embeddings to detect topic boundaries. When similarity drops below threshold â†’ new chunk.
Hierarchical chunking (enterprise-grade):
ğŸ¢ Document-level summary
ğŸ“‚ Section-level chunks
ğŸ“„ Atomic chunks with parent references

ğŸ’¬ ğ—§ğ—µğ—² ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ (ğŸ¯ğŸ¬ ğ˜€ğ—²ğ—°ğ—¼ğ—»ğ—±ğ˜€)
"I'd use a hybrid architecture with three components:

1ï¸âƒ£ LayoutLMv3 for document layout analysisâ€”detecting text, tables, figures with bounding boxes

2ï¸âƒ£ Multi-modal extraction: Table Transformer for structured tables with NL summaries, Qwen3 for image captions with context, semantic chunking for text preserving hierarchy

3ï¸âƒ£ Dual retrieval: Text embeddings for semantic search + ColPali for complex visual queries, with cross-encoder re-ranking
