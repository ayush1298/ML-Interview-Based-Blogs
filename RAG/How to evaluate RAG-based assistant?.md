Youâ€™re in an ML Engineer interview at Google DeepMind 

â€œWeâ€™re shipping a RAG-based assistant in 3 weeks.
We need an evaluation strategy we can trust.
How do you balance:
 â€¢ Manual spot checks
 â€¢ Automated evals (LLM-as-judge, metrics)
so we know itâ€™s safe to ship?â€

Donâ€™t answer: â€œWeâ€™ll use both manual and automated evals.â€

Thatâ€™s the what, not the why. The real tension is depth vs coverage with limited human time.

ğŒğšğ§ğ®ğšğ¥ ğ’ğ©ğ¨ğ­ ğ‚ğ¡ğğœğ¤ğ¬ (ğƒğğğ©, ğ›ğ®ğ­ ğ§ğšğ«ğ«ğ¨ğ°)

Humans read real Q&A and judge:
 â€¢ Did it solve the userâ€™s problem?
 â€¢ Was it grounded in the right evidence?

Manual review gives you nuance and new failure modes (subtle hallucinations, â€œtechnically right but uselessâ€ answers) and creates a shared definition of â€œgoodâ€.

But itâ€™s slow and doesnâ€™t scale. You canâ€™t run it on every build.

ğ€ğ®ğ­ğ¨ğ¦ğšğ­ğğ ğ„ğ¯ğšğ¥ğ¬ (ğ–ğ¢ğğ, ğ›ğ®ğ­ ğšğ©ğ©ğ«ğ¨ğ±ğ¢ğ¦ğšğ­ğ)

You turn that human judgment into always-on signals:
 â€¢ LLM-as-judge for correctness, grounding, completeness, safety
 â€¢ RAG metrics: retrieval hit rate, citation use, â€œI donâ€™t knowâ€ rate
 â€¢ Product metrics: thumbs, escalations, follow-up rate

Automated evals give you wide coverage, fast feedback on prompt / retrieval / model changes, and regression alerts in CI.

But theyâ€™re only as good as the rubric and gold set behind them. Left alone, they drift away from what users actually care about.

ğ“ğ¡ğ ğšğ§ğ¬ğ°ğğ« ğ­ğ¡ğšğ­ ğ ğğ­ğ¬ ğ²ğ¨ğ® ğ¡ğ¢ğ«ğğ:

Iâ€™d treat eval as a layered system, not a single score. Iâ€™d start with manual deep dives on realistic queries with domain experts to define a simple rubric for what â€˜goodâ€™ looks like (correct, grounded, safe, useful) and turn that into a small gold set. From there, Iâ€™d encode that rubric into automated evals â€” LLM-as-judge plus RAG- and product-level metrics â€” that run on every change and block serious regressions. Iâ€™d keep a steady trickle of human spot checks across high-, low-, and mid-scoring answers, and whenever human judgment and metrics disagree, we update the rubric and recalibrate the automated evals. Manual eval sets the standard; automation scales it across every build
