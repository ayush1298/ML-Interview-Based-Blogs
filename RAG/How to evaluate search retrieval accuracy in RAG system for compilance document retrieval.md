You're in a GenAI Engineer interview at Microsoft, and the interviewer asks:
"We're building a RAG system for compliance document retrieval. How do you evaluate search retrieval accuracy?"
Here's how you can answer:
A. Most candidates fumble here because they only know "Precision@K and Recall@K." Incomplete answer.
B. There are 5 critical dimensions every GenAI engineer should understand cold.

ğŸ­. ğ—§ğ—µğ—² ğ—•ğ—¶ğ—»ğ—®ğ—¿ğ˜† ğ˜ƒğ˜€ ğ—¥ğ—®ğ—»ğ—¸ğ—²ğ—± ğ—§ğ—¿ğ—®ğ—±ğ—²ğ—¼ğ—³ğ—³ - ğ—§ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—²
Order-unaware metrics treat all retrieved results equally. Order-aware metrics factor in ranking position.

Precision@K and Recall@K (order-unaware):
Precision@K = Relevant docs in top K / K
Recall@K = Relevant docs in top K / Total relevant docs
Retrieval at rank 1 scores the same as rank K

MRR, MAP, NDCG (order-aware):
MRR evaluates how well the system places relevant results at the top
MAP considers all relevant results across the entire ranking
NDCG weighs relevant items appearing earlier more heavily

ğŸ®. ğ—§ğ—µğ—² ğ—¥ğ—²ğ˜ğ—¿ğ—¶ğ—²ğ˜ƒğ—®ğ—¹-ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¦ğ—½ğ—¹ğ—¶ğ˜ - ğ—ªğ—µğ—²ğ—¿ğ—² ğŸµğŸ¬% ğ—¼ğ—³ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ˜€ ğ—´ğ—¼ ğ˜„ğ—¿ğ—¼ğ—»ğ—´
Most people think "RAG evaluation = end-to-end accuracy."
Wrong move.
RAG quality equals the product, not sum, of retriever and generator performance. If either fails, overall quality drops to zero.
Retrieval Stage:
Contextual Precision: Are docs ranked correctly?
Contextual Recall: Does retrieval contain ALL needed info?
Contextual Relevancy: How relevant is context to input?

ğŸ¯. ğ—§ğ—µğ—² ğ—šğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ—§ğ—¿ğ˜‚ğ˜ğ—µ ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º - ğ—§ğ—µğ—² ğ—µğ—¶ğ—±ğ—±ğ—²ğ—» ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¸ğ—¶ğ—¹ğ—¹ğ—²ğ—¿
 - Reference-Based Evaluation:
 Requires labeled dataset with correct answers
 - Perfect for offline testing
 Labor-intensive to create and maintain
 - Reference-Free Evaluation:
 Check faithfulness, relevance, tone without ground truth
 - Uses LLM-as-judge for quality scoring
 Essential for production monitoring

ğŸ°. ğ—§ğ—µğ—² ğ— ğ—²ğ˜ğ—¿ğ—¶ğ—° ğ—¦ğ—²ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—§ğ—¿ğ—®ğ—½ - ğŸ±ğ˜… ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜…ğ—¶ğ˜ğ˜†, ğ—¯ğ˜‚ğ˜ ğ˜„ğ—µğ˜†?
 - Precision@K: Minimizing irrelevant results
 - Recall@K: Completeness is critical
 - MRR: First result matters most
 - MAP: Multiple relevant results across ranking.
 - NDCG: Graded relevance. 

ğŸ±. ğ—§ğ—µğ—² ğ—£ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¥ğ—²ğ—®ğ—¹ğ—¶ğ˜ğ˜† - ğ—§ğ—µğ—² ğ—°ğ—¼ğ˜€ğ˜ ğ—»ğ—¼ğ—¯ğ—¼ğ—±ğ˜† ğ˜ğ—®ğ—¹ğ—¸ğ˜€ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜
 - Offline: Fixed datasets with ground truth. Run before deployment. 
 - Online Monitoring: Track answer relevancy, faithfulness scores, retrieval latency, error rates, user feedback. . 

ğ—ªğ—µğ—²ğ—» ğ—£ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—»@ğ— ğ˜„ğ—¶ğ—»ğ˜€: 
âœ… Single correct answer expected 
âœ… Minimizing false positives critical 
âœ… User reviews only top-1 result

ğ—ªğ—µğ—²ğ—» ğ—¡ğ——ğ—–ğ—š ğ˜„ğ—¶ğ—»ğ˜€: 
âœ… Multiple relevant results needed 
âœ… Ranking quality matters 
âœ… Graded relevance (0-5 scale) 

ğ—ªğ—µğ—²ğ—» ğ—™ğ—®ğ—¶ğ˜ğ—µğ—³ğ˜‚ğ—¹ğ—»ğ—²ğ˜€ğ˜€ ğ˜„ğ—¶ğ—»ğ˜€: 
âœ… Hallucination risk high 
âœ… Production deployment 
âœ… No ground truth available

